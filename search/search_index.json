{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Variational Inference of Polygenic Risk Scores (VIPRS)","text":"<p>This site contains documentation, tutorials, and examples for using the <code>viprs</code> package for the purposes of  inferring polygenic risk scores (PRS) from GWAS summary statistics. The <code>viprs</code> package is a python package that uses variational inference to estimate the posterior distribution of variant effect sizes conditional  on the GWAS summary statistics. The package is designed to be fast and accurate, and to provide a  variety of options for the user to customize the inference process.</p> <p>The details of the method and algorithms are described in detail in the following paper(s):</p> <p>Zabad, S., Gravel, S., &amp; Li, Y. (2023). Fast and accurate Bayesian polygenic risk modeling with variational inference.  The American Journal of Human Genetics, 110(5), 741\u2013761. https://doi.org/10.1016/j.ajhg.2023.03.009</p>"},{"location":"#helpful-links","title":"Helpful links","text":"<ul> <li>API Reference</li> <li>Installation</li> <li>Getting Started</li> <li>Command Line Scripts</li> <li>Download Reference LD matrices</li> <li>Project homepage on <code>GitHub</code></li> <li>Sister package <code>magenpy</code></li> </ul>"},{"location":"#software-contributions","title":"Software contributions","text":"<p>The latest version of the <code>viprs</code> package was developed in collaboration between research scientists  at McGill University and Intel Labs. </p> <ul> <li>Contributors from McGill University:<ul> <li>Shadi Zabad</li> <li>Yue Li</li> <li>Simon Gravel</li> </ul> </li> <li>Contributors from Intel Labs:<ul> <li>Chirayu Anant Haryan</li> <li>Sanchit Misra</li> </ul> </li> </ul>"},{"location":"#contact","title":"Contact","text":"<p>If you have any questions or issues, please feel free to open an issue  on the <code>GitHub</code> repository or contact us directly at:</p> <ul> <li>Shadi Zabad</li> <li>Yue Li</li> <li>Simon Gravel</li> </ul>"},{"location":"citation/","title":"Citation","text":"<p>If you use <code>viprs</code> in your research, please cite the following paper(s):</p> <pre><code>@article{ZABAD2023741,\n    title = {Fast and accurate Bayesian polygenic risk modeling with variational inference},\n    journal = {The American Journal of Human Genetics},\n    volume = {110},\n    number = {5},\n    pages = {741-761},\n    year = {2023},\n    issn = {0002-9297},\n    doi = {https://doi.org/10.1016/j.ajhg.2023.03.009},\n    url = {https://www.sciencedirect.com/science/article/pii/S0002929723000939},\n    author = {Shadi Zabad and Simon Gravel and Yue Li}\n}\n</code></pre>"},{"location":"download_ld/","title":"Download LD Reference","text":"<p>Linkage-Disequilibrium (LD) matrices, which record pairwise correlations between  genetic variants, are required as input to the <code>VIPRS</code> model. To facilitate running the model  on GWAS data from diverse ancestries, we computed LD matrices for 6 continental populations represented in  the UK Biobank. The six ancestry groups and their corresponding download links are listed below:</p> Code Ancestry group Sample size Download <code>EUR</code> European 362446 GitHub or Zenodo <code>CSA</code> Central/South Asian 8284 GitHub or Zenodo <code>AFR</code> African 6255 GitHub or Zenodo <code>EAS</code> East Asian 2700 GitHub or Zenodo <code>MID</code> Middle Eastern 1567 GitHub or Zenodo <code>AMR</code> Admixed American 987 GitHub or Zenodo <p>The sample sizes here are restricted to unrelated individuals in the UK Biobank. </p> <p>The matrices were computed using the <code>block</code> LD estimator, where we only record pairwise correlations between  variants in the same LD block. The LD blocks are defined by <code>LDetect</code>.  The matrices were computed using the sister package <code>magenpy</code> and were then  quantized to <code>int8</code> data type for enhanced compressibility. </p> <p>For European samples, we also provide LD matrices that record pairwise correlations for up to 18 million variants.  This matrix is available for download via Zenodo.</p> <p>For more details on QC criteria, data preparation, etc., please consult our manuscript:</p> <p>Zabad et al. (2025). Towards whole-genome inference of polygenic scores with fast and memory-efficient algorithms. BioRxiv.</p> <p>To access and use these matrices for downstream tasks, consult the codebase of <code>magenpy</code>, our  sister python package that implements specialized data structures for computing and processing large-scale LD matrices.</p>"},{"location":"download_ld/#bash-script-for-downloadingextracting-ld-matrices","title":"Bash Script for downloading/extracting LD matrices","text":"<p>Here is a bash script that can be used to download and extract the LD matrices for all 6 populations. The script uses the <code>GitHub</code> links provided above. Feel free to modify the script to suit your needs.</p> <pre><code>#!/bin/bash\noutput_dir=\"LD_matrices\"\npopulations=(\"EUR\" \"CSA\" \"AFR\" \"EAS\" \"MID\" \"AMR\")\nextract=true\n\nmkdir -p $output_dir\n\nfor pop in \"${populations[@]}\"\ndo\n    echo \"Downloading LD matrix for $pop\"\n    wget -O $output_dir/$pop.tar.gz \"https://github.com/shz9/viprs/releases/download/v0.1.2/$pop.tar.gz\"\n    if [ \"$extract\" = true ]; then\n        mkdir -p $output_dir/$pop\n        tar -xf $output_dir/$pop.tar.gz -C $output_dir/$pop\n    fi\ndone\n</code></pre>"},{"location":"getting_started/","title":"Getting Started","text":"<p><code>viprs</code> is a <code>python</code> package for fitting Bayesian Polygenic Risk Score (PRS) models to summary statistics  derived from Genome-wide Association Studies (GWASs). To showcase the interfaces and functionalities of the package  as well as the data structures that power it, we will start with a simple example. </p> <p>Note</p> <p>This example is designed to highlight the features of the package and the python API. If you'd like to  use the commandline interface, please refer to the Command Line Scripts documentation.</p> <p>Generally, summary statistics-based PRS methods require access to:</p> <ul> <li>GWAS summary statistics for the trait of interest </li> <li>Linkage-Disequilibrium (LD) matrices from an appropriately-matched reference panel (e.g.  from the 1KG dataset or UK Biobank). </li> </ul> <p>For the first item, we will use summary statistics for Standing Height (<code>EFO_0004339</code>) from the <code>fastGWA</code> catalogue.  For the second item, we will use genotype data on chromosome 22 for a subset of 378 European samples from the  1KG project. This small dataset is shipped with the python package <code>magenpy</code>.</p> <p>To start, let's import the required <code>python</code> packages:</p> <pre><code>import magenpy as mgp\nimport viprs as vp\n</code></pre> <p>Then, we will use <code>magenpy</code> to read the 1KG genotype dataset and automatically match it with the GWAS  summary statistics from <code>fastGWA</code>:</p> <pre><code># Load genotype and GWAS summary statistics data (chromosome 22):\ngdl = mgp.GWADataLoader(bed_files=mgp.tgp_eur_data_path(),  # Path of the genotype data\n                        sumstats_files=mgp.ukb_height_sumstats_path(),  # Path of the summary statistics\n                        sumstats_format=\"fastGWA\")  # Specify the format of the summary statistics\n</code></pre> <p>Once the genotype and summary statistics data are read by <code>magenpy</code>, we can go ahead and compute  the LD (or SNP-by-SNP correlation) matrix:</p> <pre><code># Compute LD using the shrinkage estimator (Wen and Stephens 2010):\ngdl.compute_ld(\"shrinkage\",\n               output_dir=\"temp\",\n               genetic_map_ne=11400, # effective population size (Ne)\n               genetic_map_sample_size=183,\n               threshold=1e-3)\n</code></pre> <p>Because of the small sample size of the reference panel, here we recommend using the <code>shrinkage</code> estimator  for LD from Wen and Stephens (2010). The shrinkage estimator results in compact and sparse LD matrices that are  more robust than the sample LD. The estimator requires access to information about the genetic map, such as  the position of each SNP in centi Morgan, the effective population size, and the sample size used to  estimate the genetic map.</p> <p>Given the LD information from the reference panel, we can next fit the VIPRS model to the summary statistics data:</p> <pre><code># Initialize VIPRS, passing it the GWADataLoader object\nv = vp.VIPRS(gdl)\n# Invoke the .fit() method to obtain posterior estimates\nv.fit()\n</code></pre> <p>Once the model converges, we can generate PRS estimates for height for the 1KG samples by simply  invoking the <code>.predict()</code> method:</p> <pre><code>v.predict()\n</code></pre> <p><pre><code>array([ 0.01944202,  0.00597704,  0.07329462, ..., 0.06666187,  0.05251297,  0.00359018])\n</code></pre> These are the polygenic scores for height for the European samples in the 1KG dataset! </p> <p>To examine posterior estimates for the model parameters, you can simply invoke the <code>.to_table()</code> method:</p> <pre><code>v.to_table()\n</code></pre> <pre><code>       CHR         SNP A1 A2       PIP          BETA      VAR_BETA\n 0       22    rs131538  A  G  0.006107 -5.955517e-06  1.874619e-08\n 1       22   rs9605903  C  T  0.005927  5.527188e-06  1.774252e-08\n 2       22   rs5746647  G  T  0.005015  1.194178e-07  1.120063e-08\n 3       22  rs16980739  T  C  0.008331 -1.335695e-05  3.717944e-08\n 4       22   rs9605923  A  T  0.006181  6.334971e-06  1.979157e-08\n ...    ...         ... .. ..       ...           ...           ...\n 15930   22   rs8137951  A  G  0.006367 -6.880591e-06  2.059650e-08\n 15931   22   rs2301584  A  G  0.179406 -7.234545e-04  2.597197e-06\n 15932   22   rs3810648  G  A  0.008000  1.222151e-05  3.399927e-08\n 15933   22   rs2285395  A  G  0.005356  3.004282e-06  1.349082e-08\n 15934   22  rs28729663  A  G  0.005350 -2.781053e-06  1.351239e-08\n\n [15935 rows x 7 columns]\n</code></pre> <p>Here, <code>PIP</code> is the Posterior Inclusion Probability under the variational density, while  <code>BETA</code> and <code>VAR_BETA</code> are the posterior mean and variance for the effect size, respectively.  For the purposes of prediction, we only need the <code>BETA</code> column. You can also examine the  inferred hyperparameters of the model by invoking the <code>.to_theta_table()</code> method:</p> <pre><code>v.to_theta_table()\n</code></pre> <pre><code>           Parameter     Value\n 0  Residual_variance  0.994231\n 1       Heritability  0.005736\n 2  Proportion_causal  0.015887\n 3         sigma_beta  0.000021\n</code></pre> <p>Note that here, the SNP heritability only considers the contribution of variants on  chromosome 22.</p>"},{"location":"installation/","title":"Installation","text":"<p>The <code>viprs</code> software is written in <code>C/C++</code> and <code>Cython/Python3</code> and is designed to be fast and accurate. The software is designed to be used in a variety of computing environments, including local workstations,  shared computing environments, and cloud-based computing environments. Because of the dependencies on <code>C/C++</code>, you need  to ensure that a <code>C/C++</code> Compiler (with appropriate flags) is present on your system.</p>"},{"location":"installation/#requirements","title":"Requirements","text":"<p>Building the <code>viprs</code> package requires the following dependencies:</p> <ul> <li><code>python</code> (&gt;=3.8)</li> <li><code>C/C++</code> Compilers</li> <li><code>cython</code></li> <li><code>numPy</code> </li> <li><code>pkg-config</code></li> <li><code>sciPy</code> (&gt;=1.5.4)</li> </ul> <p>To take full advantage of the parallel processing capabilities of the package, you will also need to make sure that  the following packages/libraries are available:</p> <ul> <li><code>OpenMP</code> </li> <li><code>BLAS</code></li> </ul>"},{"location":"installation/#setting-up-the-environment-with-conda","title":"Setting up the environment with <code>conda</code>","text":"<p>If you can use <code>Anaconda</code> or <code>miniconda</code> to manage your Python environment, we recommend using them to create  a new environment with the required dependencies as follows:</p> <pre><code>python_version=3.11  # Change python version here if needed\nconda create --name \"viprs_env\" -c anaconda -c conda-forge python=$python_version compilers pkg-config openblas -y\nconda activate viprs_env\n</code></pre> <p>Using <code>conda</code> to setup and manage your environment is especially recommended if you have trouble compiling  the <code>C/C++</code> extensions on your system.</p>"},{"location":"installation/#installation","title":"Installation","text":""},{"location":"installation/#using-pip","title":"Using <code>pip</code>","text":"<p>The package is available for easy installation via the Python Package Index (<code>pypi</code>) can  be installed using <code>pip</code>:</p> <pre><code>python -m pip install viprs&gt;=0.1\n</code></pre>"},{"location":"installation/#building-from-source","title":"Building from source","text":"<p>You may also build the package from source, by cloning the repository and  running the <code>make install</code> command:</p> <pre><code>git clone https://github.com/shz9/viprs.git\ncd viprs\nmake install\n</code></pre>"},{"location":"installation/#using-a-virtual-environment","title":"Using a virtual environment","text":"<p>If you wish to use <code>viprs</code> on a shared computing environment or cluster,  it is recommended that you install the package in a virtual environment. Here's a quick  example of how to install <code>viprs</code> on a SLURM-based cluster:</p> <pre><code>module load python/3.11\npython3 -m venv viprs_env  # Assumes venv is available\nsource viprs_env/bin/activate\npython -m pip install --upgrade pip\npython -m pip install viprs&gt;=0.1\n</code></pre>"},{"location":"installation/#using-docker-containers","title":"Using <code>Docker</code> containers","text":"<p>If you are using <code>Docker</code> containers, you can build a container with the <code>viprs</code> package  and all its dependencies by downloading the relevant <code>Dockerfile</code> from the  repository and building it  as follows:</p> <pre><code># Build the docker image:\ndocker build -f cli.Dockerfile -t viprs-cli .\n# Run the container in interactive mode:\ndocker run -it viprs-cli /bin/bash\n# Test that the package installed successfully:\nviprs_fit -h\n</code></pre> <p>We plan to publish pre-built <code>Docker</code> images on <code>DockerHub</code> in the future.</p>"},{"location":"api/overview/","title":"API Reference","text":""},{"location":"api/overview/#models","title":"Models","text":"<ul> <li>BayesPRSModel: A base class for all Bayesian PRS models.</li> <li>VIPRS: Implementation of VIPRS with the \"spike-and-slab\" prior.<ul> <li>Implementation of VIPRS with other priors:<ul> <li>VIPRSMix: VIPRS with a sparse Gaussian mixture prior.</li> </ul> </li> </ul> </li> <li>Hyperparameter Tuning: Models/modules for performing hyperparameter search with <code>VIPRS</code> models.<ul> <li>Hyperparameter grid: A utility class to help construct grids over model hyperparameters.</li> <li>HyperparameterSearch</li> <li>VIPRSGrid</li> <li>grid_utils: Utilities for performing model selection/averaging.</li> </ul> </li> <li>Baseline Models:<ul> <li>LDPredInf: Implementation of the LDPred-inf model.</li> </ul> </li> </ul>"},{"location":"api/overview/#model-evaluation","title":"Model Evaluation","text":"<ul> <li>Binary metrics: Evaluation metrics for binary (case-control) phenotypes.</li> <li>Continuous metrics: Evaluation metrics for continuous phenotypes.</li> <li>Pseudo metrics: Evaluation metrics based on GWAS summary statistics.</li> </ul>"},{"location":"api/overview/#utilities","title":"Utilities","text":"<ul> <li>Data utilities: Utilities for downloading and processing relevant data.</li> <li>Compute utilities: Utilities for computing various statistics / quantities over python data structures.</li> <li>Exceptions: Custom exceptions used in the package.</li> <li>OptimizeResult: A class to store the result of an optimization routine.</li> </ul>"},{"location":"api/overview/#plotting","title":"Plotting","text":"<ul> <li>Diagnostic plots: Functions for plotting various quantities / results from VIPRS or other PRS models.</li> </ul>"},{"location":"api/eval/binary_metrics/","title":"Binary metrics","text":""},{"location":"api/eval/binary_metrics/#viprs.eval.binary_metrics.avg_precision","title":"<code>avg_precision(true_val, pred_val)</code>","text":"<p>Compute the average precision between the PRS predictions and a binary.</p> <p>Parameters:</p> Name Type Description Default <code>true_val</code> <p>The response value or phenotype (a binary numpy vector with 0s and 1s)</p> required <code>pred_val</code> <p>The predicted value or PRS (a numpy vector)</p> required Source code in <code>viprs/eval/binary_metrics.py</code> <pre><code>def avg_precision(true_val, pred_val):\n    \"\"\"\n    Compute the average precision between the PRS predictions and a binary.\n\n    :param true_val: The response value or phenotype (a binary numpy vector with 0s and 1s)\n    :param pred_val: The predicted value or PRS (a numpy vector)\n    \"\"\"\n    from sklearn.metrics import average_precision_score\n    return average_precision_score(true_val, pred_val)\n</code></pre>"},{"location":"api/eval/binary_metrics/#viprs.eval.binary_metrics.cox_snell_r2","title":"<code>cox_snell_r2(true_val, pred_val, covariates=None)</code>","text":"<p>Compute the Cox-Snell pseudo-R^2 between the PRS predictions and a binary phenotype. If covariates are provided, we compute the incremental pseudo-R^2 by conditioning on the covariates.</p> <p>Parameters:</p> Name Type Description Default <code>true_val</code> <p>The response value or phenotype (a binary numpy vector with 0s and 1s)</p> required <code>pred_val</code> <p>The predicted value or PRS (a numpy vector)</p> required <code>covariates</code> <p>A pandas table of covariates where the rows are ordered the same way as the predictions and response.</p> <code>None</code> Source code in <code>viprs/eval/binary_metrics.py</code> <pre><code>def cox_snell_r2(true_val, pred_val, covariates=None):\n    \"\"\"\n    Compute the Cox-Snell pseudo-R^2 between the PRS predictions and a binary phenotype.\n    If covariates are provided, we compute the incremental pseudo-R^2 by conditioning\n    on the covariates.\n\n    :param true_val: The response value or phenotype (a binary numpy vector with 0s and 1s)\n    :param pred_val: The predicted value or PRS (a numpy vector)\n    :param covariates: A pandas table of covariates where the rows are ordered\n    the same way as the predictions and response.\n    \"\"\"\n\n    if covariates is None:\n        add_intercept = False\n        covariates = pd.DataFrame(np.ones((true_val.shape[0], 1)), columns=['const'])\n    else:\n        add_intercept = True\n\n    null_result = fit_linear_model(true_val, covariates,\n                                   family='binomial', add_intercept=add_intercept)\n    full_result = fit_linear_model(true_val, covariates.assign(pred_val=pred_val),\n                                   family='binomial', add_intercept=add_intercept)\n    n = true_val.shape[0]\n\n    return 1. - np.exp(-2 * (full_result.llf - null_result.llf) / n)\n</code></pre>"},{"location":"api/eval/binary_metrics/#viprs.eval.binary_metrics.f1","title":"<code>f1(true_val, pred_val)</code>","text":"<p>Compute the F1 score between the PRS predictions and a binary phenotype.</p> <p>Parameters:</p> Name Type Description Default <code>true_val</code> <p>The response value or phenotype (a binary numpy vector with 0s and 1s)</p> required <code>pred_val</code> <p>The predicted value or PRS (a numpy vector)</p> required Source code in <code>viprs/eval/binary_metrics.py</code> <pre><code>def f1(true_val, pred_val):\n    \"\"\"\n    Compute the F1 score between the PRS predictions and a binary phenotype.\n\n    :param true_val: The response value or phenotype (a binary numpy vector with 0s and 1s)\n    :param pred_val: The predicted value or PRS (a numpy vector)\n    \"\"\"\n    from sklearn.metrics import f1_score\n    return f1_score(true_val, pred_val)\n</code></pre>"},{"location":"api/eval/binary_metrics/#viprs.eval.binary_metrics.liability_logit_r2","title":"<code>liability_logit_r2(true_val, pred_val, covariates=None, return_all_r2=False)</code>","text":"<p>Compute the R^2 between the PRS predictions and a binary phenotype on the liability scale using the logit likelihood as outlined in Lee et al. (2012) Gene. Epi. https://pubmed.ncbi.nlm.nih.gov/22714935/</p> <p>The R^2 is defined as: R2_{logit} = Var(pred) / (Var(pred) + pi^2 / 3)</p> <p>Where Var(pred) is the variance of the predicted liability.</p> <p>If covariates are provided, we compute the incremental pseudo-R^2 by conditioning on the covariates.</p> <p>Parameters:</p> Name Type Description Default <code>true_val</code> <p>The response value or phenotype (a binary numpy vector with 0s and 1s)</p> required <code>pred_val</code> <p>The predicted value or PRS (a numpy vector)</p> required <code>covariates</code> <p>A pandas table of covariates where the rows are ordered the same way as the predictions and response.</p> <code>None</code> <code>return_all_r2</code> <p>If True, return the null, full and incremental R2 values.</p> <code>False</code> Source code in <code>viprs/eval/binary_metrics.py</code> <pre><code>def liability_logit_r2(true_val, pred_val, covariates=None, return_all_r2=False):\n    \"\"\"\n    Compute the R^2 between the PRS predictions and a binary phenotype on the liability\n    scale using the logit likelihood as outlined in Lee et al. (2012) Gene. Epi.\n    https://pubmed.ncbi.nlm.nih.gov/22714935/\n\n    The R^2 is defined as:\n    R2_{logit} = Var(pred) / (Var(pred) + pi^2 / 3)\n\n    Where Var(pred) is the variance of the predicted liability.\n\n    If covariates are provided, we compute the incremental pseudo-R^2 by conditioning\n    on the covariates.\n\n    :param true_val: The response value or phenotype (a binary numpy vector with 0s and 1s)\n    :param pred_val: The predicted value or PRS (a numpy vector)\n    :param covariates: A pandas table of covariates where the rows are ordered\n    the same way as the predictions and response.\n    :param return_all_r2: If True, return the null, full and incremental R2 values.\n    \"\"\"\n\n    if covariates is None:\n        add_intercept = False\n        covariates = pd.DataFrame(np.ones((true_val.shape[0], 1)), columns=['const'])\n    else:\n        add_intercept = True\n\n    null_result = fit_linear_model(true_val, covariates,\n                                   family='binomial', add_intercept=add_intercept)\n    full_result = fit_linear_model(true_val, covariates.assign(pred_val=pred_val),\n                                   family='binomial', add_intercept=add_intercept)\n\n    null_var = np.var(null_result.predict())\n    null_r2 = null_var / (null_var + (np.pi**2 / 3))\n\n    full_var = np.var(full_result.predict())\n    full_r2 = full_var / (full_var + (np.pi**2 / 3))\n\n    if return_all_r2:\n        return {\n            'Null_R2': null_r2,\n            'Full_R2': full_r2,\n            'Incremental_R2': full_r2 - null_r2\n        }\n    else:\n        return full_r2 - null_r2\n</code></pre>"},{"location":"api/eval/binary_metrics/#viprs.eval.binary_metrics.liability_probit_r2","title":"<code>liability_probit_r2(true_val, pred_val, covariates=None, return_all_r2=False)</code>","text":"<p>Compute the R^2 between the PRS predictions and a binary phenotype on the liability scale using the probit likelihood as outlined in Lee et al. (2012) Gene. Epi. https://pubmed.ncbi.nlm.nih.gov/22714935/</p> <p>The R^2 is defined as: R2_{probit} = Var(pred) / (Var(pred) + 1)</p> <p>Where Var(pred) is the variance of the predicted liability.</p> <p>If covariates are provided, we compute the incremental pseudo-R^2 by conditioning on the covariates.</p> <p>Parameters:</p> Name Type Description Default <code>true_val</code> <p>The response value or phenotype (a binary numpy vector with 0s and 1s)</p> required <code>pred_val</code> <p>The predicted value or PRS (a numpy vector)</p> required <code>covariates</code> <p>A pandas table of covariates where the rows are ordered the same way as the predictions and response.</p> <code>None</code> <code>return_all_r2</code> <p>If True, return the null, full and incremental R2 values.</p> <code>False</code> Source code in <code>viprs/eval/binary_metrics.py</code> <pre><code>def liability_probit_r2(true_val, pred_val, covariates=None, return_all_r2=False):\n    \"\"\"\n    Compute the R^2 between the PRS predictions and a binary phenotype on the liability\n    scale using the probit likelihood as outlined in Lee et al. (2012) Gene. Epi.\n    https://pubmed.ncbi.nlm.nih.gov/22714935/\n\n    The R^2 is defined as:\n    R2_{probit} = Var(pred) / (Var(pred) + 1)\n\n    Where Var(pred) is the variance of the predicted liability.\n\n    If covariates are provided, we compute the incremental pseudo-R^2 by conditioning\n    on the covariates.\n\n    :param true_val: The response value or phenotype (a binary numpy vector with 0s and 1s)\n    :param pred_val: The predicted value or PRS (a numpy vector)\n    :param covariates: A pandas table of covariates where the rows are ordered\n    the same way as the predictions and response.\n    :param return_all_r2: If True, return the null, full and incremental R2 values.\n    \"\"\"\n\n    if covariates is None:\n        add_intercept = False\n        covariates = pd.DataFrame(np.ones((true_val.shape[0], 1)), columns=['const'])\n    else:\n        add_intercept = True\n\n    null_result = fit_linear_model(true_val, covariates,\n                                   family='binomial', link='probit', add_intercept=add_intercept)\n    full_result = fit_linear_model(true_val, covariates.assign(pred_val=pred_val),\n                                   family='binomial', link='probit', add_intercept=add_intercept)\n\n    null_var = np.var(null_result.predict())\n    null_r2 = null_var / (null_var + 1.)\n\n    full_var = np.var(full_result.predict())\n    full_r2 = full_var / (full_var + 1.)\n\n    if return_all_r2:\n        return {\n            'Null_R2': null_r2,\n            'Full_R2': full_r2,\n            'Incremental_R2': full_r2 - null_r2\n        }\n    else:\n        return full_r2 - null_r2\n</code></pre>"},{"location":"api/eval/binary_metrics/#viprs.eval.binary_metrics.liability_r2","title":"<code>liability_r2(true_val, pred_val, covariates=None, return_all_r2=False)</code>","text":"<p>Compute the coefficient of determination (R^2) on the liability scale according to Lee et al. (2012) Gene. Epi. https://pubmed.ncbi.nlm.nih.gov/22714935/</p> <p>The R^2 liability is defined as: R_{liability}^2 = R2_{observed}K(K-1)/(z^2)</p> <p>where R_{observed}^2 is the R^2 on the observed scale and K is the sample prevalence and z is the \"height of the normal density at the quantile for K\".</p> <p>If covariates are provided, we compute the incremental pseudo-R^2 by conditioning on the covariates.</p> <p>Parameters:</p> Name Type Description Default <code>true_val</code> <p>The response value or phenotype (a binary numpy vector with 0s and 1s)</p> required <code>pred_val</code> <p>The predicted value or PRS (a numpy vector)</p> required <code>covariates</code> <p>A pandas table of covariates where the rows are ordered the same way as the predictions and response.</p> <code>None</code> <code>return_all_r2</code> <p>If True, return the null, full and incremental R2 values.</p> <code>False</code> Source code in <code>viprs/eval/binary_metrics.py</code> <pre><code>def liability_r2(true_val, pred_val, covariates=None, return_all_r2=False):\n    \"\"\"\n    Compute the coefficient of determination (R^2) on the liability scale\n    according to Lee et al. (2012) Gene. Epi.\n    https://pubmed.ncbi.nlm.nih.gov/22714935/\n\n    The R^2 liability is defined as:\n    R_{liability}^2 = R2_{observed}*K*(K-1)/(z^2)\n\n    where R_{observed}^2 is the R^2 on the observed scale and K is the sample prevalence\n    and z is the \"height of the normal density at the quantile for K\".\n\n    If covariates are provided, we compute the incremental pseudo-R^2 by conditioning\n    on the covariates.\n\n    :param true_val: The response value or phenotype (a binary numpy vector with 0s and 1s)\n    :param pred_val: The predicted value or PRS (a numpy vector)\n    :param covariates: A pandas table of covariates where the rows are ordered\n    the same way as the predictions and response.\n    :param return_all_r2: If True, return the null, full and incremental R2 values.\n    \"\"\"\n\n    # First, obtain the incremental R2 on the observed scale:\n    r2_obs = incremental_r2(true_val, pred_val, covariates, return_all_r2=return_all_r2)\n\n    # Second, compute the prevalence and the standard normal quantile of the prevalence:\n\n    from scipy.stats import norm\n\n    k = np.mean(true_val)\n    z2 = norm.pdf(norm.ppf(1.-k))**2\n    mult_factor = k*(1. - k) / z2\n\n    if return_all_r2:\n        return {\n            'Null_R2': r2_obs['Null_R2']*mult_factor,\n            'Full_R2': r2_obs['Full_R2']*mult_factor,\n            'Incremental_R2': r2_obs['Incremental_R2']*mult_factor\n        }\n    else:\n        return r2_obs * mult_factor\n</code></pre>"},{"location":"api/eval/binary_metrics/#viprs.eval.binary_metrics.mcfadden_r2","title":"<code>mcfadden_r2(true_val, pred_val, covariates=None)</code>","text":"<p>Compute the McFadden pseudo-R^2 between the PRS predictions and a phenotype. If covariates are provided, we compute the incremental pseudo-R^2 by conditioning on the covariates.</p> <p>Parameters:</p> Name Type Description Default <code>true_val</code> <p>The response value or phenotype (a binary numpy vector with 0s and 1s)</p> required <code>pred_val</code> <p>The predicted value or PRS (a numpy vector)</p> required <code>covariates</code> <p>A pandas table of covariates where the rows are ordered the same way as the predictions and response.</p> <code>None</code> Source code in <code>viprs/eval/binary_metrics.py</code> <pre><code>def mcfadden_r2(true_val, pred_val, covariates=None):\n    \"\"\"\n    Compute the McFadden pseudo-R^2 between the PRS predictions and a phenotype.\n    If covariates are provided, we compute the incremental pseudo-R^2 by conditioning\n    on the covariates.\n\n\n    :param true_val: The response value or phenotype (a binary numpy vector with 0s and 1s)\n    :param pred_val: The predicted value or PRS (a numpy vector)\n    :param covariates: A pandas table of covariates where the rows are ordered\n    the same way as the predictions and response.\n    \"\"\"\n\n    if covariates is None:\n        add_intercept = False\n        covariates = pd.DataFrame(np.ones((true_val.shape[0], 1)), columns=['const'])\n    else:\n        add_intercept = True\n\n    null_result = fit_linear_model(true_val, covariates,\n                                   family='binomial', add_intercept=add_intercept)\n    full_result = fit_linear_model(true_val, covariates.assign(pred_val=pred_val),\n                                   family='binomial', add_intercept=add_intercept)\n\n    return 1. - (full_result.llf / null_result.llf)\n</code></pre>"},{"location":"api/eval/binary_metrics/#viprs.eval.binary_metrics.nagelkerke_r2","title":"<code>nagelkerke_r2(true_val, pred_val, covariates=None)</code>","text":"<p>Compute the Nagelkerke pseudo-R^2 between the PRS predictions and a binary phenotype. If covariates are provided, we compute the incremental pseudo-R^2 by conditioning on the covariates.</p> <p>Parameters:</p> Name Type Description Default <code>true_val</code> <p>The response value or phenotype (a binary numpy vector with 0s and 1s)</p> required <code>pred_val</code> <p>The predicted value or PRS (a numpy vector)</p> required <code>covariates</code> <p>A pandas table of covariates where the rows are ordered the same way as the predictions and response.</p> <code>None</code> Source code in <code>viprs/eval/binary_metrics.py</code> <pre><code>def nagelkerke_r2(true_val, pred_val, covariates=None):\n    \"\"\"\n    Compute the Nagelkerke pseudo-R^2 between the PRS predictions and a binary phenotype.\n    If covariates are provided, we compute the incremental pseudo-R^2 by conditioning\n    on the covariates.\n\n    :param true_val: The response value or phenotype (a binary numpy vector with 0s and 1s)\n    :param pred_val: The predicted value or PRS (a numpy vector)\n    :param covariates: A pandas table of covariates where the rows are ordered\n    the same way as the predictions and response.\n    \"\"\"\n\n    if covariates is None:\n        add_intercept = False\n        covariates = pd.DataFrame(np.ones((true_val.shape[0], 1)), columns=['const'])\n    else:\n        add_intercept = True\n\n    null_result = fit_linear_model(true_val, covariates,\n                                   family='binomial', add_intercept=add_intercept)\n    full_result = fit_linear_model(true_val, covariates.assign(pred_val=pred_val),\n                                   family='binomial', add_intercept=add_intercept)\n    n = true_val.shape[0]\n\n    # First compute the Cox &amp; Snell R2:\n    cox_snell = 1. - np.exp(-2 * (full_result.llf - null_result.llf) / n)\n\n    # Then scale it by the maximum possible R2:\n    return cox_snell / (1. - np.exp(2 * null_result.llf / n))\n</code></pre>"},{"location":"api/eval/binary_metrics/#viprs.eval.binary_metrics.pr_auc","title":"<code>pr_auc(true_val, pred_val)</code>","text":"<p>Compute the area under the Precision-Recall curve for a model that maps from the PRS predictions to the binary phenotype.</p> <p>Parameters:</p> Name Type Description Default <code>true_val</code> <p>The response value or phenotype (a binary numpy vector with 0s and 1s)</p> required <code>pred_val</code> <p>The predicted value or PRS (a numpy vector)</p> required Source code in <code>viprs/eval/binary_metrics.py</code> <pre><code>def pr_auc(true_val, pred_val):\n    \"\"\"\n    Compute the area under the Precision-Recall curve for a model\n    that maps from the PRS predictions to the binary phenotype.\n\n    :param true_val: The response value or phenotype (a binary numpy vector with 0s and 1s)\n    :param pred_val: The predicted value or PRS (a numpy vector)\n    \"\"\"\n    from sklearn.metrics import precision_recall_curve, auc\n    precision, recall, thresholds = precision_recall_curve(true_val, pred_val)\n    return auc(recall, precision)\n</code></pre>"},{"location":"api/eval/binary_metrics/#viprs.eval.binary_metrics.roc_auc","title":"<code>roc_auc(true_val, pred_val)</code>","text":"<p>Compute the area under the ROC (AUROC) for a model  that maps from the PRS predictions to the binary phenotype.</p> <p>Parameters:</p> Name Type Description Default <code>true_val</code> <p>The response value or phenotype (a numpy binary vector with 0s and 1s)</p> required <code>pred_val</code> <p>The predicted value or PRS (a numpy vector)</p> required Source code in <code>viprs/eval/binary_metrics.py</code> <pre><code>def roc_auc(true_val, pred_val):\n    \"\"\"\n    Compute the area under the ROC (AUROC) for a model\n     that maps from the PRS predictions to the binary phenotype.\n\n    :param true_val: The response value or phenotype (a numpy binary vector with 0s and 1s)\n    :param pred_val: The predicted value or PRS (a numpy vector)\n    \"\"\"\n    from sklearn.metrics import roc_auc_score\n    return roc_auc_score(true_val, pred_val)\n</code></pre>"},{"location":"api/eval/continuous_metrics/","title":"Continuous metrics","text":""},{"location":"api/eval/continuous_metrics/#viprs.eval.continuous_metrics.incremental_r2","title":"<code>incremental_r2(true_val, pred_val, covariates=None, return_all_r2=False)</code>","text":"<p>Compute the incremental prediction R^2 (proportion of phenotypic variance explained by the PRS). This metric is computed by taking the R^2 of a model with covariates+PRS and subtracting from it the R^2 of a model with covariates alone covariates.</p> <p>Parameters:</p> Name Type Description Default <code>true_val</code> <p>The response value or phenotype (a numpy vector)</p> required <code>pred_val</code> <p>The predicted value or PRS (a numpy vector)</p> required <code>covariates</code> <p>A pandas table of covariates where the rows are ordered the same way as the predictions and response.</p> <code>None</code> <code>return_all_r2</code> <p>If True, return the R^2 values for the null and full models as well.</p> <code>False</code> <p>Returns:</p> Type Description <p>The incremental R^2 value</p> Source code in <code>viprs/eval/continuous_metrics.py</code> <pre><code>def incremental_r2(true_val, pred_val, covariates=None, return_all_r2=False):\n    \"\"\"\n    Compute the incremental prediction R^2 (proportion of phenotypic variance explained by the PRS).\n    This metric is computed by taking the R^2 of a model with covariates+PRS and subtracting from it\n    the R^2 of a model with covariates alone covariates.\n\n    :param true_val: The response value or phenotype (a numpy vector)\n    :param pred_val: The predicted value or PRS (a numpy vector)\n    :param covariates: A pandas table of covariates where the rows are ordered\n    the same way as the predictions and response.\n    :param return_all_r2: If True, return the R^2 values for the null and full models as well.\n\n    :return: The incremental R^2 value\n    \"\"\"\n\n    if covariates is None:\n        add_intercept = False\n        covariates = pd.DataFrame(np.ones((true_val.shape[0], 1)), columns=['const'])\n    else:\n        add_intercept = True\n\n    null_result = fit_linear_model(true_val, covariates, add_intercept=add_intercept)\n    full_result = fit_linear_model(true_val, covariates.assign(pred_val=pred_val),\n                                   add_intercept=add_intercept)\n\n    if return_all_r2:\n        return {\n            'Null_R2': null_result.rsquared,\n            'Full_R2': full_result.rsquared,\n            'Incremental_R2': full_result.rsquared - null_result.rsquared\n        }\n    else:\n        return full_result.rsquared - null_result.rsquared\n</code></pre>"},{"location":"api/eval/continuous_metrics/#viprs.eval.continuous_metrics.mse","title":"<code>mse(true_val, pred_val)</code>","text":"<p>Compute the mean squared error (MSE) between the predictions or PRS <code>pred_val</code> and the phenotype <code>true_val</code></p> <p>Parameters:</p> Name Type Description Default <code>true_val</code> <p>The response value or phenotype (a numpy vector)</p> required <code>pred_val</code> <p>The predicted value or PRS (a numpy vector)</p> required <p>Returns:</p> Type Description <p>The mean squared error</p> Source code in <code>viprs/eval/continuous_metrics.py</code> <pre><code>def mse(true_val, pred_val):\n    \"\"\"\n    Compute the mean squared error (MSE) between\n    the predictions or PRS `pred_val` and the phenotype `true_val`\n\n    :param true_val: The response value or phenotype (a numpy vector)\n    :param pred_val: The predicted value or PRS (a numpy vector)\n\n    :return: The mean squared error\n    \"\"\"\n\n    return np.mean((pred_val - true_val)**2)\n</code></pre>"},{"location":"api/eval/continuous_metrics/#viprs.eval.continuous_metrics.partial_correlation","title":"<code>partial_correlation(true_val, pred_val, covariates)</code>","text":"<p>Compute the partial correlation between the phenotype <code>true_val</code> and the PRS <code>pred_val</code> by conditioning on a set of covariates. This metric is computed by first residualizing the phenotype and the PRS on a set of covariates and then computing the correlation coefficient between the residuals.</p> <p>Parameters:</p> Name Type Description Default <code>true_val</code> <p>The response value or phenotype (a numpy vector)</p> required <code>pred_val</code> <p>The predicted value or PRS (a numpy vector)</p> required <code>covariates</code> <p>A pandas table of covariates where the rows are ordered the same way as the predictions and response.</p> required <p>Returns:</p> Type Description <p>The partial correlation coefficient</p> Source code in <code>viprs/eval/continuous_metrics.py</code> <pre><code>def partial_correlation(true_val, pred_val, covariates):\n    \"\"\"\n    Compute the partial correlation between the phenotype `true_val` and the PRS `pred_val`\n    by conditioning on a set of covariates. This metric is computed by first residualizing the\n    phenotype and the PRS on a set of covariates and then computing the correlation coefficient\n    between the residuals.\n\n    :param true_val: The response value or phenotype (a numpy vector)\n    :param pred_val: The predicted value or PRS (a numpy vector)\n    :param covariates: A pandas table of covariates where the rows are ordered\n    the same way as the predictions and response.\n\n    :return: The partial correlation coefficient\n    \"\"\"\n\n    true_response = fit_linear_model(true_val, covariates, add_intercept=True)\n    pred_response = fit_linear_model(pred_val, covariates, add_intercept=True)\n\n    return np.corrcoef(true_response.resid, pred_response.resid)[0, 1]\n</code></pre>"},{"location":"api/eval/continuous_metrics/#viprs.eval.continuous_metrics.pearson_r","title":"<code>pearson_r(true_val, pred_val)</code>","text":"<p>Compute the pearson correlation coefficient between the predictions or PRS <code>pred_val</code> and the phenotype <code>true_val</code></p> <p>Parameters:</p> Name Type Description Default <code>true_val</code> <p>The response value or phenotype (a numpy vector)</p> required <code>pred_val</code> <p>The predicted value or PRS (a numpy vector)</p> required <p>Returns:</p> Type Description <p>The pearson correlation coefficient</p> Source code in <code>viprs/eval/continuous_metrics.py</code> <pre><code>def pearson_r(true_val, pred_val):\n    \"\"\"\n    Compute the pearson correlation coefficient between\n    the predictions or PRS `pred_val` and the phenotype `true_val`\n\n    :param true_val: The response value or phenotype (a numpy vector)\n    :param pred_val: The predicted value or PRS (a numpy vector)\n\n    :return: The pearson correlation coefficient\n    \"\"\"\n    return np.corrcoef(true_val, pred_val)[0, 1]\n</code></pre>"},{"location":"api/eval/continuous_metrics/#viprs.eval.continuous_metrics.r2","title":"<code>r2(true_val, pred_val)</code>","text":"<p>Compute the R^2 (proportion of variance explained) between the predictions or PRS <code>pred_val</code> and the phenotype <code>true_val</code></p> <p>Parameters:</p> Name Type Description Default <code>true_val</code> <p>The response value or phenotype (a numpy vector)</p> required <code>pred_val</code> <p>The predicted value or PRS (a numpy vector)</p> required <p>Returns:</p> Type Description <p>The R^2 value</p> Source code in <code>viprs/eval/continuous_metrics.py</code> <pre><code>def r2(true_val, pred_val):\n    \"\"\"\n    Compute the R^2 (proportion of variance explained) between\n    the predictions or PRS `pred_val` and the phenotype `true_val`\n\n    :param true_val: The response value or phenotype (a numpy vector)\n    :param pred_val: The predicted value or PRS (a numpy vector)\n\n    :return: The R^2 value\n    \"\"\"\n    from scipy import stats\n\n    _, _, r_val, _, _ = stats.linregress(pred_val, true_val)\n    return r_val ** 2\n</code></pre>"},{"location":"api/eval/continuous_metrics/#viprs.eval.continuous_metrics.r2_residualized_target","title":"<code>r2_residualized_target(true_val, pred_val, covariates)</code>","text":"<p>Compute the R^2 (proportion of variance explained) between the predictions or PRS <code>pred_val</code> and the phenotype <code>true_val</code> after residualizing the phenotype on a set of covariates.</p> <p>Parameters:</p> Name Type Description Default <code>true_val</code> <p>The response value or phenotype (a numpy vector)</p> required <code>pred_val</code> <p>The predicted value or PRS (a numpy vector)</p> required <code>covariates</code> <p>A pandas table of covariates where the rows are ordered the same way as the predictions and response.</p> required <p>Returns:</p> Type Description <p>The residualized R^2 value</p> Source code in <code>viprs/eval/continuous_metrics.py</code> <pre><code>def r2_residualized_target(true_val, pred_val, covariates):\n    \"\"\"\n    Compute the R^2 (proportion of variance explained) between\n    the predictions or PRS `pred_val` and the phenotype `true_val`\n    after residualizing the phenotype on a set of covariates.\n\n    :param true_val: The response value or phenotype (a numpy vector)\n    :param pred_val: The predicted value or PRS (a numpy vector)\n    :param covariates: A pandas table of covariates where the rows are ordered\n    the same way as the predictions and response.\n\n    :return: The residualized R^2 value\n    \"\"\"\n\n    resid_true_val = fit_linear_model(true_val, covariates, add_intercept=True)\n\n    return r2(resid_true_val.resid, pred_val)\n</code></pre>"},{"location":"api/eval/continuous_metrics/#viprs.eval.continuous_metrics.spearman_r","title":"<code>spearman_r(true_val, pred_val)</code>","text":"<p>Compute the spearman correlation between the predictions or PRS <code>pred_val</code> and the phenotype <code>true_val</code></p> <p>Parameters:</p> Name Type Description Default <code>true_val</code> <p>The response value or phenotype (a numpy vector)</p> required <code>pred_val</code> <p>The predicted value or PRS (a numpy vector)</p> required <p>Returns:</p> Type Description <p>The spearman correlation</p> Source code in <code>viprs/eval/continuous_metrics.py</code> <pre><code>def spearman_r(true_val, pred_val):\n    \"\"\"\n    Compute the spearman correlation between the predictions or PRS `pred_val` and the phenotype `true_val`\n\n    :param true_val: The response value or phenotype (a numpy vector)\n    :param pred_val: The predicted value or PRS (a numpy vector)\n    :return: The spearman correlation\n    \"\"\"\n\n    from scipy import stats\n    return stats.spearmanr(true_val, pred_val).statistic\n</code></pre>"},{"location":"api/eval/pseudo_metrics/","title":"Pseudo metrics","text":""},{"location":"api/eval/pseudo_metrics/#viprs.eval.pseudo_metrics.pseudo_pearson_r","title":"<code>pseudo_pearson_r(test_gdl, prs_beta_table)</code>","text":"<p>Perform pseudo-validation of the inferred effect sizes by comparing them to standardized marginal betas from an independent validation set. Here, we follow the pseudo-validation procedures outlined in Mak et al. (2017) and Yang and Zhou (2020), where the correlation between the PRS and the phenotype in an independent validation cohort can be approximated with:</p> <p>Corr(PRS, y) ~= r'b / sqrt(b'Sb)</p> <p>Where <code>r</code> is the standardized marginal beta from a validation set, <code>b</code> is the posterior mean for the effect size of each variant and <code>S</code> is the LD matrix.</p> <p>Parameters:</p> Name Type Description Default <code>test_gdl</code> <p>An instance of <code>GWADataLoader</code> with the summary statistics table initialized.</p> required <code>prs_beta_table</code> <p>A pandas DataFrame with the PRS effect sizes. Must contain the columns: CHR, SNP, A1, A2, BETA.</p> required Source code in <code>viprs/eval/pseudo_metrics.py</code> <pre><code>def pseudo_pearson_r(test_gdl, prs_beta_table):\n    \"\"\"\n    Perform pseudo-validation of the inferred effect sizes by comparing them to\n    standardized marginal betas from an independent validation set. Here, we follow the pseudo-validation\n    procedures outlined in Mak et al. (2017) and Yang and Zhou (2020), where\n    the correlation between the PRS and the phenotype in an independent validation\n    cohort can be approximated with:\n\n    Corr(PRS, y) ~= r'b / sqrt(b'Sb)\n\n    Where `r` is the standardized marginal beta from a validation set,\n    `b` is the posterior mean for the effect size of each variant and `S` is the LD matrix.\n\n    :param test_gdl: An instance of `GWADataLoader` with the summary statistics table initialized.\n    :param prs_beta_table: A pandas DataFrame with the PRS effect sizes. Must contain\n    the columns: CHR, SNP, A1, A2, BETA.\n    \"\"\"\n\n    std_beta, prs_beta, q = _match_variant_stats(test_gdl, prs_beta_table)\n\n    rb = np.sum((prs_beta.T * std_beta).T, axis=0)\n    bsb = np.sum(prs_beta * q, axis=0)\n\n    return rb / np.sqrt(bsb)\n</code></pre>"},{"location":"api/eval/pseudo_metrics/#viprs.eval.pseudo_metrics.pseudo_r2","title":"<code>pseudo_r2(test_gdl, prs_beta_table)</code>","text":"<p>Compute the R-Squared metric (proportion of variance explained) for a given PRS using standardized marginal betas from an independent test set. Here, we follow the pseudo-validation procedures outlined in Mak et al. (2017) and Yang and Zhou (2020), where the proportion of phenotypic variance explained by the PRS in an independent validation cohort can be approximated with:</p> <p>R2(PRS, y) ~= 2*r'b - b'Sb</p> <p>Where <code>r</code> is the standardized marginal beta from a validation/test set, <code>b</code> is the posterior mean for the effect size of each variant and <code>S</code> is the LD matrix.</p> <p>Parameters:</p> Name Type Description Default <code>test_gdl</code> <p>An instance of <code>GWADataLoader</code> with the summary statistics table initialized.</p> required <code>prs_beta_table</code> <p>A pandas DataFrame with the PRS effect sizes. Must contain the columns: CHR, SNP, A1, A2, BETA.</p> required Source code in <code>viprs/eval/pseudo_metrics.py</code> <pre><code>def pseudo_r2(test_gdl, prs_beta_table):\n    \"\"\"\n    Compute the R-Squared metric (proportion of variance explained) for a given\n    PRS using standardized marginal betas from an independent test set.\n    Here, we follow the pseudo-validation procedures outlined in Mak et al. (2017) and\n    Yang and Zhou (2020), where the proportion of phenotypic variance explained by the PRS\n    in an independent validation cohort can be approximated with:\n\n    R2(PRS, y) ~= 2*r'b - b'Sb\n\n    Where `r` is the standardized marginal beta from a validation/test set,\n    `b` is the posterior mean for the effect size of each variant and `S` is the LD matrix.\n\n    :param test_gdl: An instance of `GWADataLoader` with the summary statistics table initialized.\n    :param prs_beta_table: A pandas DataFrame with the PRS effect sizes. Must contain\n    the columns: CHR, SNP, A1, A2, BETA.\n    \"\"\"\n\n    # std_beta, prs_beta, q = _match_variant_stats(test_gdl, prs_beta_table)\n\n    # rb = np.sum((prs_beta.T * std_beta).T, axis=0)\n    # bsb = np.sum(prs_beta*q, axis=0)\n\n    # return 2*rb - bsb\n\n    # NOTE: The above procedure can be biased/problematic when the LD matrix is highly\n    # sparsified. For now, we will use the squared Pearson correlation as a proxy for the R^2 metric:\n\n    return pseudo_pearson_r(test_gdl, prs_beta_table)**2\n</code></pre>"},{"location":"api/model/BayesPRSModel/","title":"BayesPRSModel","text":""},{"location":"api/model/BayesPRSModel/#viprs.model.BayesPRSModel.BayesPRSModel","title":"<code>BayesPRSModel</code>","text":"<p>A base class for Bayesian PRS models. This class defines the basic structure and methods that are common to most Bayesian PRS models. Specifically, this class provides methods and interfaces for initialization, harmonization, prediction, and fitting of Bayesian PRS models.</p> <p>The class is generic is designed to be inherited and extended by specific Bayesian PRS models, such as <code>LDPred</code> and <code>VIPRS</code>.</p> <p>Attributes:</p> Name Type Description <code>gdl</code> <p>A GWADataLoader object containing harmonized GWAS summary statistics and Linkage-Disequilibrium (LD) matrices.</p> <code>float_precision</code> <p>The precision of the floating point variables. Options are: 'float32' or 'float64'.</p> <code>shapes</code> <p>A dictionary where keys are chromosomes and values are the shapes of the variant arrays (e.g. the number of variants per chromosome).</p> <code>n_per_snp</code> <p>A dictionary where keys are chromosomes and values are the sample sizes per variant.</p> <code>std_beta</code> <p>A dictionary of the standardized marginal effect sizes from GWAS.</p> <code>validation_std_beta</code> <p>A dictionary of the standardized marginal effect sizes from an independent validation set.</p> <code>_sample_size</code> <p>The maximum per-SNP sample size.</p> <code>pip</code> <p>The posterior inclusion probability.</p> <code>post_mean_beta</code> <p>The posterior mean for the effect sizes.</p> <code>post_var_beta</code> <p>The posterior variance for the effect sizes.</p> Source code in <code>viprs/model/BayesPRSModel.py</code> <pre><code>class BayesPRSModel:\n    \"\"\"\n    A base class for Bayesian PRS models. This class defines the basic structure and methods\n    that are common to most Bayesian PRS models. Specifically, this class provides methods and interfaces\n    for initialization, harmonization, prediction, and fitting of Bayesian PRS models.\n\n    The class is generic is designed to be inherited and extended by\n    specific Bayesian PRS models, such as `LDPred` and `VIPRS`.\n\n    :ivar gdl: A GWADataLoader object containing harmonized GWAS summary statistics and\n    Linkage-Disequilibrium (LD) matrices.\n    :ivar float_precision: The precision of the floating point variables. Options are: 'float32' or 'float64'.\n    :ivar shapes: A dictionary where keys are chromosomes and values are the shapes of the variant arrays\n    (e.g. the number of variants per chromosome).\n    :ivar n_per_snp: A dictionary where keys are chromosomes and values are the sample sizes per variant.\n    :ivar std_beta: A dictionary of the standardized marginal effect sizes from GWAS.\n    :ivar validation_std_beta: A dictionary of the standardized marginal effect sizes\n    from an independent validation set.\n    :ivar _sample_size: The maximum per-SNP sample size.\n    :ivar pip: The posterior inclusion probability.\n    :ivar post_mean_beta: The posterior mean for the effect sizes.\n    :ivar post_var_beta: The posterior variance for the effect sizes.\n    \"\"\"\n\n    def __init__(self,\n                 gdl: GWADataLoader,\n                 float_precision='float32'):\n        \"\"\"\n        Initialize the Bayesian PRS model.\n        :param gdl: An instance of `GWADataLoader`. Must contain either GWAS summary statistics\n        or genotype data.\n        :param float_precision: The precision for the floating point numbers.\n        \"\"\"\n\n        # ------------------- Sanity checks -------------------\n\n        assert isinstance(gdl, GWADataLoader), \"The `gdl` object must be an instance of GWASDataLoader.\"\n\n        assert gdl.genotype is not None or (gdl.ld is not None and gdl.sumstats_table is not None), (\n            \"The GWADataLoader object must contain either genotype data or summary statistics and LD matrices.\"\n        )\n\n        # -----------------------------------------------------\n\n        self.gdl = gdl\n        self.float_precision = float_precision\n        self.float_eps = np.finfo(self.float_precision).eps\n        self.shapes = self.gdl.shapes.copy()\n\n        # Placeholder for sample size per SNP:\n        self.n_per_snp = None\n        # Placeholder for standardized marginal betas:\n        self.std_beta = None\n\n        # Placeholder for standardized marginal betas from an independent validation set:\n        self.validation_std_beta = None\n\n        if gdl.sumstats_table is not None:\n            # Initialize the input data arrays:\n            self.initialize_input_data_arrays()\n\n            # Determine the overall sample size:\n            self._sample_size = dict_max(self.n_per_snp)\n\n        # Placeholder for inferred model parameters:\n        self.pip = None  # Posterior inclusion probability\n        self.post_mean_beta = None  # The posterior mean for the effect sizes\n        self.post_var_beta = None  # The posterior variance for the effect sizes\n\n    @property\n    def chromosomes(self):\n        \"\"\"\n        :return: The list of chromosomes that are included in the BayesPRSModel\n        \"\"\"\n        return sorted(list(self.shapes.keys()))\n\n    @property\n    def m(self) -&gt; int:\n        \"\"\"\n\n        !!! seealso \"See Also\"\n            * [n_snps][viprs.model.BayesPRSModel.BayesPRSModel.n_snps]\n\n        :return: The number of variants in the model.\n        \"\"\"\n        return self.gdl.m\n\n    @property\n    def n(self) -&gt; int:\n        \"\"\"\n        :return: The number of samples in the model. If not available, average the per-SNP\n        sample sizes.\n        \"\"\"\n        return self._sample_size\n\n    @property\n    def n_snps(self) -&gt; int:\n        \"\"\"\n        !!! seealso \"See Also\"\n            * [m][viprs.model.BayesPRSModel.BayesPRSModel.m]\n\n        :return: The number of SNPs in the model.\n        \"\"\"\n        return self.m\n\n    def initialize_input_data_arrays(self):\n        \"\"\"\n        Initialize the input data arrays for the Bayesian PRS model.\n        The input data for summary statistics-based PRS models typically include the following:\n            * The sample size per variant (n_per_snp)\n            * The standardized marginal betas (std_beta)\n            * LD matrices (LD)\n\n        This convenience method initializes the first two inputs, primarily the sample size per variant\n        and the standardized marginal betas.\n        \"\"\"\n\n        logger.debug(\"&gt; Initializing the input data arrays (marginal statistics).\")\n\n        try:\n            self.n_per_snp = {c: ss.n_per_snp\n                              for c, ss in self.gdl.sumstats_table.items()}\n            self.std_beta = {c: ss.get_snp_pseudo_corr().astype(self.float_precision)\n                             for c, ss in self.gdl.sumstats_table.items()}\n        except AttributeError:\n            # If not provided, use the overall sample size:\n            self.n_per_snp = {c: np.repeat(self.gdl.n, c_size)\n                              for c, c_size in self.shapes.items()}\n\n        self.validation_std_beta = None\n\n    def set_validation_sumstats(self):\n        \"\"\"\n        Set the validation summary statistics.\n        TODO: Allow the user to set the validation sumstats as a property of the model.\n        \"\"\"\n        raise NotImplementedError\n\n    def split_gwas_sumstats(self,\n                            prop_train=0.8,\n                            seed=None,\n                            **kwargs):\n        \"\"\"\n        Split the GWAS summary statistics into training and validation sets, using the\n        PUMAS procedure outlined in Zhao et al. (2021).\n\n        :param prop_train: The proportion of samples to include in the training set.\n        :param seed: The random seed for reproducibility.\n        :param kwargs: Additional keyword arguments to pass to the `sumstats_train_test_split` function.\n        \"\"\"\n\n        from magenpy.utils.model_utils import sumstats_train_test_split\n\n        logger.debug(\"&gt; Splitting the GWAS summary statistics into training and validation sets. \"\n                     f\"Training proportion: {prop_train}\")\n\n        split_sumstats = sumstats_train_test_split(self.gdl,\n                                                   prop_train=prop_train,\n                                                   seed=seed,\n                                                   **kwargs)\n\n        self.std_beta = {\n            c: split_sumstats[c]['train_beta'].astype(self.float_precision)\n            for c in self.chromosomes\n        }\n\n        self.n_per_snp = {\n            c: self.n_per_snp[c]*prop_train\n            for c in self.chromosomes\n        }\n\n        self.validation_std_beta = {\n            c: split_sumstats[c]['test_beta'].astype(self.float_precision)\n            for c in self.chromosomes\n        }\n\n    def fit(self, *args, **kwargs):\n        \"\"\"\n        A genetic method to fit the Bayesian PRS model. This method should be implemented by the\n        specific Bayesian PRS model.\n        :raises NotImplementedError: If the method is not implemented in the child class.\n        \"\"\"\n        raise NotImplementedError\n\n    def get_proportion_causal(self):\n        \"\"\"\n        A generic method to get an estimate of the proportion of causal variants.\n        :raises NotImplementedError: If the method is not implemented in the child class.\n        \"\"\"\n        raise NotImplementedError\n\n    def get_heritability(self):\n        \"\"\"\n        A generic method to get an estimate of the heritability, or proportion of variance explained by SNPs.\n        :raises NotImplementedError: If the method is not implemented in the child class.\n        \"\"\"\n        raise NotImplementedError\n\n    def get_pip(self):\n        \"\"\"\n        :return: The posterior inclusion probability for each variant in the model.\n        \"\"\"\n        return self.pip\n\n    def get_posterior_mean_beta(self):\n        \"\"\"\n        :return: The posterior mean of the effect sizes (BETA) for each variant in the model.\n        \"\"\"\n        return self.post_mean_beta\n\n    def get_posterior_variance_beta(self):\n        \"\"\"\n        :return: The posterior variance of the effect sizes (BETA) for each variant in the model.\n        \"\"\"\n        return self.post_var_beta\n\n    def predict(self, test_gdl=None):\n        \"\"\"\n        Given the inferred effect sizes, predict the phenotype for the training samples in\n        the GWADataLoader object or new test samples. If `test_gdl` is not provided, genotypes\n        from training samples will be used (if available).\n\n        :param test_gdl: A GWADataLoader object containing genotype data for new test samples.\n        :raises ValueError: If the posterior means for BETA are not set. AssertionError if the GWADataLoader object\n        does not contain genotype data.\n        \"\"\"\n\n        if self.post_mean_beta is None:\n            raise ValueError(\"The posterior means for BETA are not set. Call `.fit()` first.\")\n\n        if test_gdl is None:\n            assert self.gdl.genotype is not None, \"The GWADataLoader object must contain genotype data.\"\n            test_gdl = self.gdl\n            post_mean_beta = self.post_mean_beta\n        else:\n            _, post_mean_beta, _ = self.harmonize_data(gdl=test_gdl)\n\n        return test_gdl.predict(post_mean_beta)\n\n    def harmonize_data(self, gdl=None, parameter_table=None):\n        \"\"\"\n        Harmonize the inferred effect sizes with a new GWADataLoader object. This method is useful\n        when the user wants to predict on new samples or when the effect sizes are inferred from a\n        different set of samples. The method aligns the effect sizes with the SNP table in the\n        GWADataLoader object.\n\n        :param gdl: An instance of `GWADataLoader` object.\n        :param parameter_table: A `pandas` DataFrame of variant effect sizes.\n\n        :return: A tuple of the harmonized posterior inclusion probability, posterior mean for the effect sizes,\n        and posterior variance for the effect sizes.\n\n        \"\"\"\n\n        if gdl is None and parameter_table is None:\n            return\n\n        if gdl is None:\n            gdl = self.gdl\n\n        if parameter_table is None:\n            parameter_table = self.to_table(per_chromosome=True)\n        else:\n            parameter_table = {c: parameter_table.loc[parameter_table['CHR'] == c, ]\n                               for c in parameter_table['CHR'].unique()}\n\n        snp_tables = gdl.to_snp_table(col_subset=['SNP', 'A1', 'A2'],\n                                      per_chromosome=True)\n\n        pip = {}\n        post_mean_beta = {}\n        post_var_beta = {}\n\n        common_chroms = sorted(list(set(snp_tables.keys()).intersection(set(parameter_table.keys()))))\n\n        from magenpy.utils.model_utils import merge_snp_tables\n\n        for c in common_chroms:\n\n            try:\n                post_mean_cols = expand_column_names('BETA', self.post_mean_beta[c].shape)\n                pip_cols = expand_column_names('PIP', self.post_mean_beta[c].shape)\n                post_var_cols = expand_column_names('VAR_BETA', self.post_mean_beta[c].shape)\n\n            except (TypeError, KeyError):\n                pip_cols = [col for col in parameter_table[c].columns if 'PIP' in col]\n                post_var_cols = [col for col in parameter_table[c].columns if 'VAR_BETA' in col]\n                post_mean_cols = [col for col in parameter_table[c].columns\n                                  if 'BETA' in col and col not in post_var_cols]\n\n            # Merge the effect table with the GDL SNP table:\n            c_df = merge_snp_tables(snp_tables[c], parameter_table[c], how='left',\n                                    signed_statistics=post_mean_cols)\n\n            if len(c_df) &lt; len(snp_tables[c]):\n                raise ValueError(\"The parameter table could not aligned with the reference SNP table. This may due to \"\n                                 \"conflicts/errors in use of reference vs. alternative alleles.\")\n\n            # Obtain the values for the posterior mean:\n            c_df[post_mean_cols] = c_df[post_mean_cols].fillna(0.)\n            post_mean_beta[c] = c_df[post_mean_cols].values\n\n            # Obtain the values for the posterior inclusion probability:\n            if len(set(pip_cols).intersection(set(c_df.columns))) &gt; 0:\n                c_df[pip_cols] = c_df[pip_cols].fillna(0.)\n                pip[c] = c_df[pip_cols].values\n\n            # Obtain the values for the posterior variance:\n            if len(set(post_var_cols).intersection(set(c_df.columns))) &gt; 0:\n                c_df[post_var_cols] = c_df[post_var_cols].fillna(0.)\n                post_var_beta[c] = c_df[post_var_cols].values\n\n        if len(pip) &lt; 1:\n            pip = None\n\n        if len(post_var_beta) &lt; 1:\n            post_var_beta = None\n\n        return pip, post_mean_beta, post_var_beta\n\n    def to_table(self, col_subset=('CHR', 'SNP', 'POS', 'A1', 'A2'), per_chromosome=False):\n        \"\"\"\n        Output the posterior estimates for the effect sizes to a pandas dataframe.\n        :param col_subset: The subset of columns to include in the tables (in addition to the effect sizes).\n        :param per_chromosome: If True, return a separate table for each chromosome.\n\n        :return: A pandas Dataframe with the posterior estimates for the effect sizes.\n        \"\"\"\n\n        if self.post_mean_beta is None:\n            raise Exception(\"The posterior means for BETA are not set. Call `.fit()` first.\")\n\n        tables = self.gdl.to_snp_table(col_subset=col_subset, per_chromosome=True)\n\n        for c in self.chromosomes:\n\n            cols_to_add = []\n\n            mean_beta_df = pd.DataFrame(self.post_mean_beta[c],\n                                        columns=expand_column_names('BETA', self.post_mean_beta[c].shape),\n                                        index=tables[c].index)\n            cols_to_add.append(mean_beta_df)\n\n            if self.pip is not None:\n                pip_df = pd.DataFrame(self.pip[c],\n                                      columns=expand_column_names('PIP', self.pip[c].shape),\n                                      index=tables[c].index)\n                cols_to_add.append(pip_df)\n\n            if self.post_var_beta is not None:\n                var_beta_df = pd.DataFrame(self.post_var_beta[c],\n                                           columns=expand_column_names('VAR_BETA', self.post_var_beta[c].shape),\n                                           index=tables[c].index)\n                cols_to_add.append(var_beta_df)\n\n            tables[c] = pd.concat([tables[c]] + cols_to_add, axis=1)\n\n        if per_chromosome:\n            return tables\n        else:\n            return pd.concat([tables[c] for c in self.chromosomes])\n\n    def pseudo_validate(self, test_gdl=None):\n        \"\"\"\n        Evaluate the prediction accuracy of the inferred PRS using external GWAS summary statistics.\n\n        :param test_gdl: A `GWADataLoader` object with the external GWAS summary statistics and LD matrix information.\n\n        :return: The pseudo-R^2 metric.\n        \"\"\"\n\n        from ..eval.pseudo_metrics import pseudo_r2, _streamlined_pseudo_r2\n        from ..utils.compute_utils import dict_concat\n\n        assert self.post_mean_beta is not None, \"The posterior means for BETA are not set. Call `.fit()` first.\"\n        assert self.validation_std_beta is not None or test_gdl is not None, (\n            \"Must provide a GWADataLoader object with validation sumstats or initialize the standardized \"\n            \"betas inside the model.\"\n        )\n\n        if test_gdl is not None:\n            return pseudo_r2(test_gdl, self.to_table(per_chromosome=False))\n        else:\n\n            # Check if q is an attribute of the model:\n            if hasattr(self, 'q'):\n                ldw_prs = {c: self.q[c] + self.post_mean_beta[c] for c in self.shapes}\n            else:\n                # Compute LD-weighted PRS weights first:\n                ldw_prs = {}\n                for c in self.shapes:\n                    ldw_prs[c] = self.gdl.ld[c].dot(self.post_mean_beta[c])\n\n            return _streamlined_pseudo_r2(\n                dict_concat(self.validation_std_beta),\n                dict_concat(self.post_mean_beta),\n                dict_concat(ldw_prs)\n            )\n\n    def set_model_parameters(self, parameter_table):\n        \"\"\"\n        Parses a pandas dataframe with model parameters and assigns them \n        to the corresponding class attributes. \n\n        For example: \n            * Columns with `BETA`, will be assigned to `self.post_mean_beta`.\n            * Columns with `PIP` will be assigned to `self.pip`.\n            * Columns with `VAR_BETA`, will be assigned to `self.post_var_beta`.\n\n        :param parameter_table: A pandas table or dataframe.\n        \"\"\"\n\n        self.pip, self.post_mean_beta, self.post_var_beta = self.harmonize_data(parameter_table=parameter_table)\n\n    def read_inferred_parameters(self, f_names, sep=r\"\\s+\"):\n        \"\"\"\n        Read a file with the inferred parameters.\n        :param f_names: A path (or list of paths) to the file with the effect sizes.\n        :param sep: The delimiter for the file(s).\n        \"\"\"\n\n        if isinstance(f_names, str):\n            f_names = [f_names]\n\n        param_table = []\n\n        for f_name in f_names:\n            param_table.append(pd.read_csv(f_name, sep=sep))\n\n        if len(param_table) &gt; 0:\n            param_table = pd.concat(param_table)\n            self.set_model_parameters(param_table)\n        else:\n            raise FileNotFoundError\n\n    def write_inferred_parameters(self, f_name, per_chromosome=False, sep=\"\\t\"):\n        \"\"\"\n        A convenience method to write the inferred posterior for the effect sizes to file.\n\n        TODO:\n            * Support outputting scoring files compatible with PGS catalog format:\n            https://www.pgscatalog.org/downloads/#dl_scoring_files\n\n        :param f_name: The filename (or directory) where to write the effect sizes\n        :param per_chromosome: If True, write a file for each chromosome separately.\n        :param sep: The delimiter for the file (tab by default).\n        \"\"\"\n\n        tables = self.to_table(per_chromosome=per_chromosome)\n\n        if '.fit' not in f_name:\n            ext = '.fit'\n        else:\n            ext = ''\n\n        if per_chromosome:\n            for c, tab in tables.items():\n                try:\n                    tab.to_csv(osp.join(f_name, f'chr_{c}.fit'), sep=sep, index=False)\n                except Exception as e:\n                    raise e\n        else:\n            try:\n                tables.to_csv(f_name + ext, sep=sep, index=False)\n            except Exception as e:\n                raise e\n</code></pre>"},{"location":"api/model/BayesPRSModel/#viprs.model.BayesPRSModel.BayesPRSModel.chromosomes","title":"<code>chromosomes</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The list of chromosomes that are included in the BayesPRSModel</p>"},{"location":"api/model/BayesPRSModel/#viprs.model.BayesPRSModel.BayesPRSModel.m","title":"<code>m</code>  <code>property</code>","text":"<p>See Also</p> <ul> <li>n_snps</li> </ul> <p>Returns:</p> Type Description <code>int</code> <p>The number of variants in the model.</p>"},{"location":"api/model/BayesPRSModel/#viprs.model.BayesPRSModel.BayesPRSModel.n","title":"<code>n</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>int</code> <p>The number of samples in the model. If not available, average the per-SNP sample sizes.</p>"},{"location":"api/model/BayesPRSModel/#viprs.model.BayesPRSModel.BayesPRSModel.n_snps","title":"<code>n_snps</code>  <code>property</code>","text":"<p>See Also</p> <ul> <li>m</li> </ul> <p>Returns:</p> Type Description <code>int</code> <p>The number of SNPs in the model.</p>"},{"location":"api/model/BayesPRSModel/#viprs.model.BayesPRSModel.BayesPRSModel.__init__","title":"<code>__init__(gdl, float_precision='float32')</code>","text":"<p>Initialize the Bayesian PRS model.</p> <p>Parameters:</p> Name Type Description Default <code>gdl</code> <code>GWADataLoader</code> <p>An instance of <code>GWADataLoader</code>. Must contain either GWAS summary statistics or genotype data.</p> required <code>float_precision</code> <p>The precision for the floating point numbers.</p> <code>'float32'</code> Source code in <code>viprs/model/BayesPRSModel.py</code> <pre><code>def __init__(self,\n             gdl: GWADataLoader,\n             float_precision='float32'):\n    \"\"\"\n    Initialize the Bayesian PRS model.\n    :param gdl: An instance of `GWADataLoader`. Must contain either GWAS summary statistics\n    or genotype data.\n    :param float_precision: The precision for the floating point numbers.\n    \"\"\"\n\n    # ------------------- Sanity checks -------------------\n\n    assert isinstance(gdl, GWADataLoader), \"The `gdl` object must be an instance of GWASDataLoader.\"\n\n    assert gdl.genotype is not None or (gdl.ld is not None and gdl.sumstats_table is not None), (\n        \"The GWADataLoader object must contain either genotype data or summary statistics and LD matrices.\"\n    )\n\n    # -----------------------------------------------------\n\n    self.gdl = gdl\n    self.float_precision = float_precision\n    self.float_eps = np.finfo(self.float_precision).eps\n    self.shapes = self.gdl.shapes.copy()\n\n    # Placeholder for sample size per SNP:\n    self.n_per_snp = None\n    # Placeholder for standardized marginal betas:\n    self.std_beta = None\n\n    # Placeholder for standardized marginal betas from an independent validation set:\n    self.validation_std_beta = None\n\n    if gdl.sumstats_table is not None:\n        # Initialize the input data arrays:\n        self.initialize_input_data_arrays()\n\n        # Determine the overall sample size:\n        self._sample_size = dict_max(self.n_per_snp)\n\n    # Placeholder for inferred model parameters:\n    self.pip = None  # Posterior inclusion probability\n    self.post_mean_beta = None  # The posterior mean for the effect sizes\n    self.post_var_beta = None  # The posterior variance for the effect sizes\n</code></pre>"},{"location":"api/model/BayesPRSModel/#viprs.model.BayesPRSModel.BayesPRSModel.fit","title":"<code>fit(*args, **kwargs)</code>","text":"<p>A genetic method to fit the Bayesian PRS model. This method should be implemented by the specific Bayesian PRS model.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the method is not implemented in the child class.</p> Source code in <code>viprs/model/BayesPRSModel.py</code> <pre><code>def fit(self, *args, **kwargs):\n    \"\"\"\n    A genetic method to fit the Bayesian PRS model. This method should be implemented by the\n    specific Bayesian PRS model.\n    :raises NotImplementedError: If the method is not implemented in the child class.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/model/BayesPRSModel/#viprs.model.BayesPRSModel.BayesPRSModel.get_heritability","title":"<code>get_heritability()</code>","text":"<p>A generic method to get an estimate of the heritability, or proportion of variance explained by SNPs.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the method is not implemented in the child class.</p> Source code in <code>viprs/model/BayesPRSModel.py</code> <pre><code>def get_heritability(self):\n    \"\"\"\n    A generic method to get an estimate of the heritability, or proportion of variance explained by SNPs.\n    :raises NotImplementedError: If the method is not implemented in the child class.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/model/BayesPRSModel/#viprs.model.BayesPRSModel.BayesPRSModel.get_pip","title":"<code>get_pip()</code>","text":"<p>Returns:</p> Type Description <p>The posterior inclusion probability for each variant in the model.</p> Source code in <code>viprs/model/BayesPRSModel.py</code> <pre><code>def get_pip(self):\n    \"\"\"\n    :return: The posterior inclusion probability for each variant in the model.\n    \"\"\"\n    return self.pip\n</code></pre>"},{"location":"api/model/BayesPRSModel/#viprs.model.BayesPRSModel.BayesPRSModel.get_posterior_mean_beta","title":"<code>get_posterior_mean_beta()</code>","text":"<p>Returns:</p> Type Description <p>The posterior mean of the effect sizes (BETA) for each variant in the model.</p> Source code in <code>viprs/model/BayesPRSModel.py</code> <pre><code>def get_posterior_mean_beta(self):\n    \"\"\"\n    :return: The posterior mean of the effect sizes (BETA) for each variant in the model.\n    \"\"\"\n    return self.post_mean_beta\n</code></pre>"},{"location":"api/model/BayesPRSModel/#viprs.model.BayesPRSModel.BayesPRSModel.get_posterior_variance_beta","title":"<code>get_posterior_variance_beta()</code>","text":"<p>Returns:</p> Type Description <p>The posterior variance of the effect sizes (BETA) for each variant in the model.</p> Source code in <code>viprs/model/BayesPRSModel.py</code> <pre><code>def get_posterior_variance_beta(self):\n    \"\"\"\n    :return: The posterior variance of the effect sizes (BETA) for each variant in the model.\n    \"\"\"\n    return self.post_var_beta\n</code></pre>"},{"location":"api/model/BayesPRSModel/#viprs.model.BayesPRSModel.BayesPRSModel.get_proportion_causal","title":"<code>get_proportion_causal()</code>","text":"<p>A generic method to get an estimate of the proportion of causal variants.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the method is not implemented in the child class.</p> Source code in <code>viprs/model/BayesPRSModel.py</code> <pre><code>def get_proportion_causal(self):\n    \"\"\"\n    A generic method to get an estimate of the proportion of causal variants.\n    :raises NotImplementedError: If the method is not implemented in the child class.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/model/BayesPRSModel/#viprs.model.BayesPRSModel.BayesPRSModel.harmonize_data","title":"<code>harmonize_data(gdl=None, parameter_table=None)</code>","text":"<p>Harmonize the inferred effect sizes with a new GWADataLoader object. This method is useful when the user wants to predict on new samples or when the effect sizes are inferred from a different set of samples. The method aligns the effect sizes with the SNP table in the GWADataLoader object.</p> <p>Parameters:</p> Name Type Description Default <code>gdl</code> <p>An instance of <code>GWADataLoader</code> object.</p> <code>None</code> <code>parameter_table</code> <p>A <code>pandas</code> DataFrame of variant effect sizes.</p> <code>None</code> <p>Returns:</p> Type Description <p>A tuple of the harmonized posterior inclusion probability, posterior mean for the effect sizes, and posterior variance for the effect sizes.</p> Source code in <code>viprs/model/BayesPRSModel.py</code> <pre><code>def harmonize_data(self, gdl=None, parameter_table=None):\n    \"\"\"\n    Harmonize the inferred effect sizes with a new GWADataLoader object. This method is useful\n    when the user wants to predict on new samples or when the effect sizes are inferred from a\n    different set of samples. The method aligns the effect sizes with the SNP table in the\n    GWADataLoader object.\n\n    :param gdl: An instance of `GWADataLoader` object.\n    :param parameter_table: A `pandas` DataFrame of variant effect sizes.\n\n    :return: A tuple of the harmonized posterior inclusion probability, posterior mean for the effect sizes,\n    and posterior variance for the effect sizes.\n\n    \"\"\"\n\n    if gdl is None and parameter_table is None:\n        return\n\n    if gdl is None:\n        gdl = self.gdl\n\n    if parameter_table is None:\n        parameter_table = self.to_table(per_chromosome=True)\n    else:\n        parameter_table = {c: parameter_table.loc[parameter_table['CHR'] == c, ]\n                           for c in parameter_table['CHR'].unique()}\n\n    snp_tables = gdl.to_snp_table(col_subset=['SNP', 'A1', 'A2'],\n                                  per_chromosome=True)\n\n    pip = {}\n    post_mean_beta = {}\n    post_var_beta = {}\n\n    common_chroms = sorted(list(set(snp_tables.keys()).intersection(set(parameter_table.keys()))))\n\n    from magenpy.utils.model_utils import merge_snp_tables\n\n    for c in common_chroms:\n\n        try:\n            post_mean_cols = expand_column_names('BETA', self.post_mean_beta[c].shape)\n            pip_cols = expand_column_names('PIP', self.post_mean_beta[c].shape)\n            post_var_cols = expand_column_names('VAR_BETA', self.post_mean_beta[c].shape)\n\n        except (TypeError, KeyError):\n            pip_cols = [col for col in parameter_table[c].columns if 'PIP' in col]\n            post_var_cols = [col for col in parameter_table[c].columns if 'VAR_BETA' in col]\n            post_mean_cols = [col for col in parameter_table[c].columns\n                              if 'BETA' in col and col not in post_var_cols]\n\n        # Merge the effect table with the GDL SNP table:\n        c_df = merge_snp_tables(snp_tables[c], parameter_table[c], how='left',\n                                signed_statistics=post_mean_cols)\n\n        if len(c_df) &lt; len(snp_tables[c]):\n            raise ValueError(\"The parameter table could not aligned with the reference SNP table. This may due to \"\n                             \"conflicts/errors in use of reference vs. alternative alleles.\")\n\n        # Obtain the values for the posterior mean:\n        c_df[post_mean_cols] = c_df[post_mean_cols].fillna(0.)\n        post_mean_beta[c] = c_df[post_mean_cols].values\n\n        # Obtain the values for the posterior inclusion probability:\n        if len(set(pip_cols).intersection(set(c_df.columns))) &gt; 0:\n            c_df[pip_cols] = c_df[pip_cols].fillna(0.)\n            pip[c] = c_df[pip_cols].values\n\n        # Obtain the values for the posterior variance:\n        if len(set(post_var_cols).intersection(set(c_df.columns))) &gt; 0:\n            c_df[post_var_cols] = c_df[post_var_cols].fillna(0.)\n            post_var_beta[c] = c_df[post_var_cols].values\n\n    if len(pip) &lt; 1:\n        pip = None\n\n    if len(post_var_beta) &lt; 1:\n        post_var_beta = None\n\n    return pip, post_mean_beta, post_var_beta\n</code></pre>"},{"location":"api/model/BayesPRSModel/#viprs.model.BayesPRSModel.BayesPRSModel.initialize_input_data_arrays","title":"<code>initialize_input_data_arrays()</code>","text":"<p>Initialize the input data arrays for the Bayesian PRS model. The input data for summary statistics-based PRS models typically include the following:     * The sample size per variant (n_per_snp)     * The standardized marginal betas (std_beta)     * LD matrices (LD)</p> <p>This convenience method initializes the first two inputs, primarily the sample size per variant and the standardized marginal betas.</p> Source code in <code>viprs/model/BayesPRSModel.py</code> <pre><code>def initialize_input_data_arrays(self):\n    \"\"\"\n    Initialize the input data arrays for the Bayesian PRS model.\n    The input data for summary statistics-based PRS models typically include the following:\n        * The sample size per variant (n_per_snp)\n        * The standardized marginal betas (std_beta)\n        * LD matrices (LD)\n\n    This convenience method initializes the first two inputs, primarily the sample size per variant\n    and the standardized marginal betas.\n    \"\"\"\n\n    logger.debug(\"&gt; Initializing the input data arrays (marginal statistics).\")\n\n    try:\n        self.n_per_snp = {c: ss.n_per_snp\n                          for c, ss in self.gdl.sumstats_table.items()}\n        self.std_beta = {c: ss.get_snp_pseudo_corr().astype(self.float_precision)\n                         for c, ss in self.gdl.sumstats_table.items()}\n    except AttributeError:\n        # If not provided, use the overall sample size:\n        self.n_per_snp = {c: np.repeat(self.gdl.n, c_size)\n                          for c, c_size in self.shapes.items()}\n\n    self.validation_std_beta = None\n</code></pre>"},{"location":"api/model/BayesPRSModel/#viprs.model.BayesPRSModel.BayesPRSModel.predict","title":"<code>predict(test_gdl=None)</code>","text":"<p>Given the inferred effect sizes, predict the phenotype for the training samples in the GWADataLoader object or new test samples. If <code>test_gdl</code> is not provided, genotypes from training samples will be used (if available).</p> <p>Parameters:</p> Name Type Description Default <code>test_gdl</code> <p>A GWADataLoader object containing genotype data for new test samples.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the posterior means for BETA are not set. AssertionError if the GWADataLoader object does not contain genotype data.</p> Source code in <code>viprs/model/BayesPRSModel.py</code> <pre><code>def predict(self, test_gdl=None):\n    \"\"\"\n    Given the inferred effect sizes, predict the phenotype for the training samples in\n    the GWADataLoader object or new test samples. If `test_gdl` is not provided, genotypes\n    from training samples will be used (if available).\n\n    :param test_gdl: A GWADataLoader object containing genotype data for new test samples.\n    :raises ValueError: If the posterior means for BETA are not set. AssertionError if the GWADataLoader object\n    does not contain genotype data.\n    \"\"\"\n\n    if self.post_mean_beta is None:\n        raise ValueError(\"The posterior means for BETA are not set. Call `.fit()` first.\")\n\n    if test_gdl is None:\n        assert self.gdl.genotype is not None, \"The GWADataLoader object must contain genotype data.\"\n        test_gdl = self.gdl\n        post_mean_beta = self.post_mean_beta\n    else:\n        _, post_mean_beta, _ = self.harmonize_data(gdl=test_gdl)\n\n    return test_gdl.predict(post_mean_beta)\n</code></pre>"},{"location":"api/model/BayesPRSModel/#viprs.model.BayesPRSModel.BayesPRSModel.pseudo_validate","title":"<code>pseudo_validate(test_gdl=None)</code>","text":"<p>Evaluate the prediction accuracy of the inferred PRS using external GWAS summary statistics.</p> <p>Parameters:</p> Name Type Description Default <code>test_gdl</code> <p>A <code>GWADataLoader</code> object with the external GWAS summary statistics and LD matrix information.</p> <code>None</code> <p>Returns:</p> Type Description <p>The pseudo-R^2 metric.</p> Source code in <code>viprs/model/BayesPRSModel.py</code> <pre><code>def pseudo_validate(self, test_gdl=None):\n    \"\"\"\n    Evaluate the prediction accuracy of the inferred PRS using external GWAS summary statistics.\n\n    :param test_gdl: A `GWADataLoader` object with the external GWAS summary statistics and LD matrix information.\n\n    :return: The pseudo-R^2 metric.\n    \"\"\"\n\n    from ..eval.pseudo_metrics import pseudo_r2, _streamlined_pseudo_r2\n    from ..utils.compute_utils import dict_concat\n\n    assert self.post_mean_beta is not None, \"The posterior means for BETA are not set. Call `.fit()` first.\"\n    assert self.validation_std_beta is not None or test_gdl is not None, (\n        \"Must provide a GWADataLoader object with validation sumstats or initialize the standardized \"\n        \"betas inside the model.\"\n    )\n\n    if test_gdl is not None:\n        return pseudo_r2(test_gdl, self.to_table(per_chromosome=False))\n    else:\n\n        # Check if q is an attribute of the model:\n        if hasattr(self, 'q'):\n            ldw_prs = {c: self.q[c] + self.post_mean_beta[c] for c in self.shapes}\n        else:\n            # Compute LD-weighted PRS weights first:\n            ldw_prs = {}\n            for c in self.shapes:\n                ldw_prs[c] = self.gdl.ld[c].dot(self.post_mean_beta[c])\n\n        return _streamlined_pseudo_r2(\n            dict_concat(self.validation_std_beta),\n            dict_concat(self.post_mean_beta),\n            dict_concat(ldw_prs)\n        )\n</code></pre>"},{"location":"api/model/BayesPRSModel/#viprs.model.BayesPRSModel.BayesPRSModel.read_inferred_parameters","title":"<code>read_inferred_parameters(f_names, sep='\\\\s+')</code>","text":"<p>Read a file with the inferred parameters.</p> <p>Parameters:</p> Name Type Description Default <code>f_names</code> <p>A path (or list of paths) to the file with the effect sizes.</p> required <code>sep</code> <p>The delimiter for the file(s).</p> <code>'\\\\s+'</code> Source code in <code>viprs/model/BayesPRSModel.py</code> <pre><code>def read_inferred_parameters(self, f_names, sep=r\"\\s+\"):\n    \"\"\"\n    Read a file with the inferred parameters.\n    :param f_names: A path (or list of paths) to the file with the effect sizes.\n    :param sep: The delimiter for the file(s).\n    \"\"\"\n\n    if isinstance(f_names, str):\n        f_names = [f_names]\n\n    param_table = []\n\n    for f_name in f_names:\n        param_table.append(pd.read_csv(f_name, sep=sep))\n\n    if len(param_table) &gt; 0:\n        param_table = pd.concat(param_table)\n        self.set_model_parameters(param_table)\n    else:\n        raise FileNotFoundError\n</code></pre>"},{"location":"api/model/BayesPRSModel/#viprs.model.BayesPRSModel.BayesPRSModel.set_model_parameters","title":"<code>set_model_parameters(parameter_table)</code>","text":"<p>Parses a pandas dataframe with model parameters and assigns them  to the corresponding class attributes. </p> <p>For example:      * Columns with <code>BETA</code>, will be assigned to <code>self.post_mean_beta</code>.     * Columns with <code>PIP</code> will be assigned to <code>self.pip</code>.     * Columns with <code>VAR_BETA</code>, will be assigned to <code>self.post_var_beta</code>.</p> <p>Parameters:</p> Name Type Description Default <code>parameter_table</code> <p>A pandas table or dataframe.</p> required Source code in <code>viprs/model/BayesPRSModel.py</code> <pre><code>def set_model_parameters(self, parameter_table):\n    \"\"\"\n    Parses a pandas dataframe with model parameters and assigns them \n    to the corresponding class attributes. \n\n    For example: \n        * Columns with `BETA`, will be assigned to `self.post_mean_beta`.\n        * Columns with `PIP` will be assigned to `self.pip`.\n        * Columns with `VAR_BETA`, will be assigned to `self.post_var_beta`.\n\n    :param parameter_table: A pandas table or dataframe.\n    \"\"\"\n\n    self.pip, self.post_mean_beta, self.post_var_beta = self.harmonize_data(parameter_table=parameter_table)\n</code></pre>"},{"location":"api/model/BayesPRSModel/#viprs.model.BayesPRSModel.BayesPRSModel.set_validation_sumstats","title":"<code>set_validation_sumstats()</code>","text":"<p>Set the validation summary statistics. TODO: Allow the user to set the validation sumstats as a property of the model.</p> Source code in <code>viprs/model/BayesPRSModel.py</code> <pre><code>def set_validation_sumstats(self):\n    \"\"\"\n    Set the validation summary statistics.\n    TODO: Allow the user to set the validation sumstats as a property of the model.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/model/BayesPRSModel/#viprs.model.BayesPRSModel.BayesPRSModel.split_gwas_sumstats","title":"<code>split_gwas_sumstats(prop_train=0.8, seed=None, **kwargs)</code>","text":"<p>Split the GWAS summary statistics into training and validation sets, using the PUMAS procedure outlined in Zhao et al. (2021).</p> <p>Parameters:</p> Name Type Description Default <code>prop_train</code> <p>The proportion of samples to include in the training set.</p> <code>0.8</code> <code>seed</code> <p>The random seed for reproducibility.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments to pass to the <code>sumstats_train_test_split</code> function.</p> <code>{}</code> Source code in <code>viprs/model/BayesPRSModel.py</code> <pre><code>def split_gwas_sumstats(self,\n                        prop_train=0.8,\n                        seed=None,\n                        **kwargs):\n    \"\"\"\n    Split the GWAS summary statistics into training and validation sets, using the\n    PUMAS procedure outlined in Zhao et al. (2021).\n\n    :param prop_train: The proportion of samples to include in the training set.\n    :param seed: The random seed for reproducibility.\n    :param kwargs: Additional keyword arguments to pass to the `sumstats_train_test_split` function.\n    \"\"\"\n\n    from magenpy.utils.model_utils import sumstats_train_test_split\n\n    logger.debug(\"&gt; Splitting the GWAS summary statistics into training and validation sets. \"\n                 f\"Training proportion: {prop_train}\")\n\n    split_sumstats = sumstats_train_test_split(self.gdl,\n                                               prop_train=prop_train,\n                                               seed=seed,\n                                               **kwargs)\n\n    self.std_beta = {\n        c: split_sumstats[c]['train_beta'].astype(self.float_precision)\n        for c in self.chromosomes\n    }\n\n    self.n_per_snp = {\n        c: self.n_per_snp[c]*prop_train\n        for c in self.chromosomes\n    }\n\n    self.validation_std_beta = {\n        c: split_sumstats[c]['test_beta'].astype(self.float_precision)\n        for c in self.chromosomes\n    }\n</code></pre>"},{"location":"api/model/BayesPRSModel/#viprs.model.BayesPRSModel.BayesPRSModel.to_table","title":"<code>to_table(col_subset=('CHR', 'SNP', 'POS', 'A1', 'A2'), per_chromosome=False)</code>","text":"<p>Output the posterior estimates for the effect sizes to a pandas dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>col_subset</code> <p>The subset of columns to include in the tables (in addition to the effect sizes).</p> <code>('CHR', 'SNP', 'POS', 'A1', 'A2')</code> <code>per_chromosome</code> <p>If True, return a separate table for each chromosome.</p> <code>False</code> <p>Returns:</p> Type Description <p>A pandas Dataframe with the posterior estimates for the effect sizes.</p> Source code in <code>viprs/model/BayesPRSModel.py</code> <pre><code>def to_table(self, col_subset=('CHR', 'SNP', 'POS', 'A1', 'A2'), per_chromosome=False):\n    \"\"\"\n    Output the posterior estimates for the effect sizes to a pandas dataframe.\n    :param col_subset: The subset of columns to include in the tables (in addition to the effect sizes).\n    :param per_chromosome: If True, return a separate table for each chromosome.\n\n    :return: A pandas Dataframe with the posterior estimates for the effect sizes.\n    \"\"\"\n\n    if self.post_mean_beta is None:\n        raise Exception(\"The posterior means for BETA are not set. Call `.fit()` first.\")\n\n    tables = self.gdl.to_snp_table(col_subset=col_subset, per_chromosome=True)\n\n    for c in self.chromosomes:\n\n        cols_to_add = []\n\n        mean_beta_df = pd.DataFrame(self.post_mean_beta[c],\n                                    columns=expand_column_names('BETA', self.post_mean_beta[c].shape),\n                                    index=tables[c].index)\n        cols_to_add.append(mean_beta_df)\n\n        if self.pip is not None:\n            pip_df = pd.DataFrame(self.pip[c],\n                                  columns=expand_column_names('PIP', self.pip[c].shape),\n                                  index=tables[c].index)\n            cols_to_add.append(pip_df)\n\n        if self.post_var_beta is not None:\n            var_beta_df = pd.DataFrame(self.post_var_beta[c],\n                                       columns=expand_column_names('VAR_BETA', self.post_var_beta[c].shape),\n                                       index=tables[c].index)\n            cols_to_add.append(var_beta_df)\n\n        tables[c] = pd.concat([tables[c]] + cols_to_add, axis=1)\n\n    if per_chromosome:\n        return tables\n    else:\n        return pd.concat([tables[c] for c in self.chromosomes])\n</code></pre>"},{"location":"api/model/BayesPRSModel/#viprs.model.BayesPRSModel.BayesPRSModel.write_inferred_parameters","title":"<code>write_inferred_parameters(f_name, per_chromosome=False, sep='\\t')</code>","text":"<p>A convenience method to write the inferred posterior for the effect sizes to file.</p> <p>TODO:     * Support outputting scoring files compatible with PGS catalog format:     https://www.pgscatalog.org/downloads/#dl_scoring_files</p> <p>Parameters:</p> Name Type Description Default <code>f_name</code> <p>The filename (or directory) where to write the effect sizes</p> required <code>per_chromosome</code> <p>If True, write a file for each chromosome separately.</p> <code>False</code> <code>sep</code> <p>The delimiter for the file (tab by default).</p> <code>'\\t'</code> Source code in <code>viprs/model/BayesPRSModel.py</code> <pre><code>def write_inferred_parameters(self, f_name, per_chromosome=False, sep=\"\\t\"):\n    \"\"\"\n    A convenience method to write the inferred posterior for the effect sizes to file.\n\n    TODO:\n        * Support outputting scoring files compatible with PGS catalog format:\n        https://www.pgscatalog.org/downloads/#dl_scoring_files\n\n    :param f_name: The filename (or directory) where to write the effect sizes\n    :param per_chromosome: If True, write a file for each chromosome separately.\n    :param sep: The delimiter for the file (tab by default).\n    \"\"\"\n\n    tables = self.to_table(per_chromosome=per_chromosome)\n\n    if '.fit' not in f_name:\n        ext = '.fit'\n    else:\n        ext = ''\n\n    if per_chromosome:\n        for c, tab in tables.items():\n            try:\n                tab.to_csv(osp.join(f_name, f'chr_{c}.fit'), sep=sep, index=False)\n            except Exception as e:\n                raise e\n    else:\n        try:\n            tables.to_csv(f_name + ext, sep=sep, index=False)\n        except Exception as e:\n            raise e\n</code></pre>"},{"location":"api/model/LDPredInf/","title":"LDPredInf","text":""},{"location":"api/model/LDPredInf/#viprs.model.LDPredInf.LDPredInf","title":"<code>LDPredInf</code>","text":"<p>               Bases: <code>BayesPRSModel</code></p> <p>A wrapper class implementing the LDPred-inf model. The LDPred-inf model is a Bayesian model that uses summary statistics from GWAS to estimate the posterior mean effect sizes of the SNPs. It is equivalent to performing ridge regression, with the penalty proportional to the inverse of the per-SNP heritability.</p> <p>Refer to the following references for details about the LDPred-inf model: * Vilhj\u00e1lmsson et al. AJHG. 2015 * Priv\u00e9 et al. Bioinformatics. 2020</p> <p>Attributes:</p> Name Type Description <code>gdl</code> <p>An instance of <code>GWADataLoader</code></p> <code>h2</code> <p>The heritability for the trait (can also be chromosome-specific)</p> Source code in <code>viprs/model/LDPredInf.py</code> <pre><code>class LDPredInf(BayesPRSModel):\n    \"\"\"\n    A wrapper class implementing the LDPred-inf model.\n    The LDPred-inf model is a Bayesian model that uses summary statistics\n    from GWAS to estimate the posterior mean effect sizes of the SNPs. It is equivalent\n    to performing ridge regression, with the penalty proportional to the inverse of\n    the per-SNP heritability.\n\n    Refer to the following references for details about the LDPred-inf model:\n    * Vilhj\u00e1lmsson et al. AJHG. 2015\n    * Priv\u00e9 et al. Bioinformatics. 2020\n\n    :ivar gdl: An instance of `GWADataLoader`\n    :ivar h2: The heritability for the trait (can also be chromosome-specific)\n\n    \"\"\"\n\n    def __init__(self,\n                 gdl,\n                 h2=None):\n        \"\"\"\n        Initialize the LDPred-inf model.\n        :param gdl: An instance of GWADataLoader\n        :param h2: The heritability for the trait (can also be chromosome-specific)\n        \"\"\"\n        super().__init__(gdl)\n\n        if h2 is None:\n            from magenpy.stats.h2.ldsc import simple_ldsc\n            self.h2 = simple_ldsc(self.gdl)\n        else:\n            self.h2 = h2\n\n    def get_heritability(self):\n        \"\"\"\n        :return: The heritability estimate for the trait of interest.\n        \"\"\"\n        return self.h2\n\n    def fit(self, solver='minres', **solver_kwargs):\n        \"\"\"\n        Fit the summary statistics-based ridge regression,\n        following the specifications of the LDPred-inf model.\n\n        !!! warning\n            Not tested yet.\n\n        Here, we use `lsqr` or `minres` solvers to solve the system of equations:\n\n        (D + lam*I)BETA = BETA_HAT\n\n        where D is the LD matrix, BETA is ridge regression\n        estimate that we wish to obtain and BETA_HAT is the\n        marginal effect sizes estimated from GWAS.\n\n        In this case, lam = M / N*h2, where M is the number of SNPs,\n        N is the number of samples and h2 is the heritability\n        of the trait.\n\n        https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.lsqr.html\n        https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.minres.html\n\n        :param solver: The solver for the system of linear equations. Options: `minres` or `lsqr`\n        :param solver_kwargs: keyword arguments for the solver.\n        \"\"\"\n\n        assert solver in ('lsqr', 'minres')\n\n        import numpy as np\n        from scipy.sparse.linalg import lsqr, minres\n        from scipy.sparse import identity, block_diag\n\n        if solver == 'lsqr':\n            solve = lsqr\n        else:\n            solve = minres\n\n        # Lambda, the regularization parameter for the\n        # ridge regression estimator. For LDPred-inf model,\n        # we set this to 'M / N*h2', where M is the number of SNPs,\n        # N is the number of samples and h2 is the heritability\n        # of the trait.\n        lam = self.n_snps / (self.n * self.h2)\n\n        chroms = self.gdl.chromosomes\n\n        # Extract the LD matrices for all the chromosomes represented and\n        # concatenate them into one block diagonal matrix:\n        ld_mats = []\n        for c in chroms:\n            self.gdl.ld[c].load(dtype=np.float32)\n            ld_mats.append(self.gdl.ld[c].csr_matrix)\n\n        ld = block_diag(ld_mats, format='csr')\n\n        # Extract the marginal GWAS effect sizes:\n        marginal_beta = np.concatenate([self.gdl.sumstats_table[c].marginal_beta\n                                        for c in chroms])\n\n        # Estimate the BETAs under the ridge penalty:\n        res = solve(ld + lam * identity(ld.shape[0]), marginal_beta, **solver_kwargs)\n\n        # Extract the estimates and populate them in `post_mean_beta`\n        start = 0\n        self.post_mean_beta = {}\n\n        for c in chroms:\n            self.post_mean_beta[c] = res[0][start:start + self.shapes[c]]\n            start += self.shapes[c]\n\n        return self\n</code></pre>"},{"location":"api/model/LDPredInf/#viprs.model.LDPredInf.LDPredInf.__init__","title":"<code>__init__(gdl, h2=None)</code>","text":"<p>Initialize the LDPred-inf model.</p> <p>Parameters:</p> Name Type Description Default <code>gdl</code> <p>An instance of GWADataLoader</p> required <code>h2</code> <p>The heritability for the trait (can also be chromosome-specific)</p> <code>None</code> Source code in <code>viprs/model/LDPredInf.py</code> <pre><code>def __init__(self,\n             gdl,\n             h2=None):\n    \"\"\"\n    Initialize the LDPred-inf model.\n    :param gdl: An instance of GWADataLoader\n    :param h2: The heritability for the trait (can also be chromosome-specific)\n    \"\"\"\n    super().__init__(gdl)\n\n    if h2 is None:\n        from magenpy.stats.h2.ldsc import simple_ldsc\n        self.h2 = simple_ldsc(self.gdl)\n    else:\n        self.h2 = h2\n</code></pre>"},{"location":"api/model/LDPredInf/#viprs.model.LDPredInf.LDPredInf.fit","title":"<code>fit(solver='minres', **solver_kwargs)</code>","text":"<p>Fit the summary statistics-based ridge regression, following the specifications of the LDPred-inf model.</p> <p>Warning</p> <p>Not tested yet.</p> <p>Here, we use <code>lsqr</code> or <code>minres</code> solvers to solve the system of equations:</p> <p>(D + lam*I)BETA = BETA_HAT</p> <p>where D is the LD matrix, BETA is ridge regression estimate that we wish to obtain and BETA_HAT is the marginal effect sizes estimated from GWAS.</p> <p>In this case, lam = M / N*h2, where M is the number of SNPs, N is the number of samples and h2 is the heritability of the trait.</p> <p>https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.lsqr.html https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.minres.html</p> <p>Parameters:</p> Name Type Description Default <code>solver</code> <p>The solver for the system of linear equations. Options: <code>minres</code> or <code>lsqr</code></p> <code>'minres'</code> <code>solver_kwargs</code> <p>keyword arguments for the solver.</p> <code>{}</code> Source code in <code>viprs/model/LDPredInf.py</code> <pre><code>def fit(self, solver='minres', **solver_kwargs):\n    \"\"\"\n    Fit the summary statistics-based ridge regression,\n    following the specifications of the LDPred-inf model.\n\n    !!! warning\n        Not tested yet.\n\n    Here, we use `lsqr` or `minres` solvers to solve the system of equations:\n\n    (D + lam*I)BETA = BETA_HAT\n\n    where D is the LD matrix, BETA is ridge regression\n    estimate that we wish to obtain and BETA_HAT is the\n    marginal effect sizes estimated from GWAS.\n\n    In this case, lam = M / N*h2, where M is the number of SNPs,\n    N is the number of samples and h2 is the heritability\n    of the trait.\n\n    https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.lsqr.html\n    https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.minres.html\n\n    :param solver: The solver for the system of linear equations. Options: `minres` or `lsqr`\n    :param solver_kwargs: keyword arguments for the solver.\n    \"\"\"\n\n    assert solver in ('lsqr', 'minres')\n\n    import numpy as np\n    from scipy.sparse.linalg import lsqr, minres\n    from scipy.sparse import identity, block_diag\n\n    if solver == 'lsqr':\n        solve = lsqr\n    else:\n        solve = minres\n\n    # Lambda, the regularization parameter for the\n    # ridge regression estimator. For LDPred-inf model,\n    # we set this to 'M / N*h2', where M is the number of SNPs,\n    # N is the number of samples and h2 is the heritability\n    # of the trait.\n    lam = self.n_snps / (self.n * self.h2)\n\n    chroms = self.gdl.chromosomes\n\n    # Extract the LD matrices for all the chromosomes represented and\n    # concatenate them into one block diagonal matrix:\n    ld_mats = []\n    for c in chroms:\n        self.gdl.ld[c].load(dtype=np.float32)\n        ld_mats.append(self.gdl.ld[c].csr_matrix)\n\n    ld = block_diag(ld_mats, format='csr')\n\n    # Extract the marginal GWAS effect sizes:\n    marginal_beta = np.concatenate([self.gdl.sumstats_table[c].marginal_beta\n                                    for c in chroms])\n\n    # Estimate the BETAs under the ridge penalty:\n    res = solve(ld + lam * identity(ld.shape[0]), marginal_beta, **solver_kwargs)\n\n    # Extract the estimates and populate them in `post_mean_beta`\n    start = 0\n    self.post_mean_beta = {}\n\n    for c in chroms:\n        self.post_mean_beta[c] = res[0][start:start + self.shapes[c]]\n        start += self.shapes[c]\n\n    return self\n</code></pre>"},{"location":"api/model/LDPredInf/#viprs.model.LDPredInf.LDPredInf.get_heritability","title":"<code>get_heritability()</code>","text":"<p>Returns:</p> Type Description <p>The heritability estimate for the trait of interest.</p> Source code in <code>viprs/model/LDPredInf.py</code> <pre><code>def get_heritability(self):\n    \"\"\"\n    :return: The heritability estimate for the trait of interest.\n    \"\"\"\n    return self.h2\n</code></pre>"},{"location":"api/model/VIPRS/","title":"VIPRS","text":""},{"location":"api/model/VIPRS/#viprs.model.VIPRS.VIPRS","title":"<code>VIPRS</code>","text":"<p>               Bases: <code>BayesPRSModel</code></p> <p>The base class for performing Variational Inference of Polygenic Risk Scores (VIPRS).</p> <p>This class implements the Variational EM algorithm for estimating the posterior distribution of the effect sizes using GWAS summary statistics. The model assumes a spike-and-slab mixture prior on the effect size distribution, with the spike component representing the null effects and the slab component representing the non-null effects.</p> <p>Details for the algorithm can be found in the Supplementary Material of the following paper:</p> <p>Zabad S, Gravel S, Li Y. Fast and accurate Bayesian polygenic risk modeling with variational inference. Am J Hum Genet. 2023 May 4;110(5):741-761. doi: 10.1016/j.ajhg.2023.03.009. Epub 2023 Apr 7. PMID: 37030289; PMCID: PMC10183379.</p> <p>Attributes:</p> Name Type Description <code>gdl</code> <p>An instance of GWADataLoader containing harmonized GWAS summary statistics and LD matrices.</p> <code>var_gamma</code> <p>A dictionary of the variational gamma parameter, denoting the probability that the variant comes from the slab component.</p> <code>var_mu</code> <p>A dictionary of the variational mu parameter, denoting the mean of the effect size for each variant.</p> <code>var_tau</code> <p>A dictionary of the variational tau parameter, denoting the precision of the effect size for each variant.</p> <code>eta</code> <p>A dictionary of the posterior mean of the effect size, E[B] = gamma*mu.</p> <code>zeta</code> <p>A dictionary of the expectation of B^2 under the posterior, E[B^2] = gamma*(mu^2 + 1./tau).</p> <code>eta_diff</code> <p>A dictionary of the difference between the etas in two consecutive iterations.</p> <code>q</code> <p>A dictionary of the q-factor, which keeps track of the multiplication of eta with the LD matrix.</p> <code>sigma_epsilon</code> <p>The global residual variance parameter.</p> <code>tau_beta</code> <p>The prior precision (inverse variance) for the effect size.</p> <code>pi</code> <p>The proportion of causal variants.</p> <code>_sigma_g</code> <p>A pseudo-estimate of the additive genotypic variance.</p> <code>lambda_min</code> <p>The minimum eigenvalue for the LD matrix (or an approximation of it that will serve as a regularizer).</p> <code>ld_data</code> <p>A dictionary of the <code>data</code> arrays of the sparse LD matrices.</p> <code>ld_indptr</code> <p>A dictionary of the <code>indptr</code> arrays of the sparse LD matrices.</p> <code>ld_left_bound</code> <p>A dictionary of the left boundaries of the LD matrices.</p> <code>std_beta</code> <p>A dictionary of the standardized marginal effect sizes from GWAS.</p> <code>n_per_snp</code> <p>A dictionary of the sample size per SNP from the GWAS study.</p> <code>threads</code> <p>The number of threads to use when fitting the model.</p> <code>fix_params</code> <p>A dictionary of hyperparameters with their fixed values.</p> <code>float_precision</code> <p>The precision of the floating point variables. Options are: 'float32' or 'float64'.</p> <code>order</code> <p>The order of the arrays in memory. Options are: 'C' or 'F'.</p> <code>low_memory</code> <p>A boolean flag to indicate whether to use low memory mode.</p> <code>dequantize_on_the_fly</code> <p>A boolean flag to indicate whether to dequantize the LD matrix on the fly.</p> <code>optim_result</code> <p>An instance of OptimizeResult tracking the progress of the optimization algorithm.</p> <code>history</code> <p>A dictionary to store the history of the optimization procedure (e.g. the objective as a function of iteration number).</p> <code>tracked_params</code> <p>A list of hyperparameters to track throughout the optimization procedure. Useful for debugging/model checking.</p> Source code in <code>viprs/model/VIPRS.py</code> <pre><code>class VIPRS(BayesPRSModel):\n    \"\"\"\n    The base class for performing Variational Inference of Polygenic Risk Scores (VIPRS).\n\n    This class implements the Variational EM algorithm for estimating the posterior distribution\n    of the effect sizes using GWAS summary statistics. The model assumes a spike-and-slab mixture\n    prior on the effect size distribution, with the spike component representing the null effects\n    and the slab component representing the non-null effects.\n\n    Details for the algorithm can be found in the Supplementary Material of the following paper:\n\n    &gt; Zabad S, Gravel S, Li Y. Fast and accurate Bayesian polygenic risk modeling with variational inference.\n    Am J Hum Genet. 2023 May 4;110(5):741-761. doi: 10.1016/j.ajhg.2023.03.009.\n    Epub 2023 Apr 7. PMID: 37030289; PMCID: PMC10183379.\n\n    :ivar gdl: An instance of GWADataLoader containing harmonized GWAS summary statistics and LD matrices.\n    :ivar var_gamma: A dictionary of the variational gamma parameter, denoting the probability that the\n    variant comes from the slab component.\n    :ivar var_mu: A dictionary of the variational mu parameter, denoting the mean of the\n    effect size for each variant.\n    :ivar var_tau: A dictionary of the variational tau parameter, denoting the precision of\n    the effect size for each variant.\n    :ivar eta: A dictionary of the posterior mean of the effect size, E[B] = gamma*mu.\n    :ivar zeta: A dictionary of the expectation of B^2 under the posterior, E[B^2] = gamma*(mu^2 + 1./tau).\n    :ivar eta_diff: A dictionary of the difference between the etas in two consecutive iterations.\n    :ivar q: A dictionary of the q-factor, which keeps track of the multiplication of eta with the LD matrix.\n    :ivar sigma_epsilon: The global residual variance parameter.\n    :ivar tau_beta: The prior precision (inverse variance) for the effect size.\n    :ivar pi: The proportion of causal variants.\n    :ivar _sigma_g: A pseudo-estimate of the additive genotypic variance.\n    :ivar lambda_min: The minimum eigenvalue for the LD matrix (or an approximation of it that will serve\n    as a regularizer).\n    :ivar ld_data: A dictionary of the `data` arrays of the sparse LD matrices.\n    :ivar ld_indptr: A dictionary of the `indptr` arrays of the sparse LD matrices.\n    :ivar ld_left_bound: A dictionary of the left boundaries of the LD matrices.\n    :ivar std_beta: A dictionary of the standardized marginal effect sizes from GWAS.\n    :ivar n_per_snp: A dictionary of the sample size per SNP from the GWAS study.\n    :ivar threads: The number of threads to use when fitting the model.\n    :ivar fix_params: A dictionary of hyperparameters with their fixed values.\n    :ivar float_precision: The precision of the floating point variables. Options are: 'float32' or 'float64'.\n    :ivar order: The order of the arrays in memory. Options are: 'C' or 'F'.\n    :ivar low_memory: A boolean flag to indicate whether to use low memory mode.\n    :ivar dequantize_on_the_fly: A boolean flag to indicate whether to dequantize the LD matrix on the fly.\n    :ivar optim_result: An instance of OptimizeResult tracking the progress of the optimization algorithm.\n    :ivar history: A dictionary to store the history of the optimization procedure (e.g. the objective as a function\n    of iteration number).\n    :ivar tracked_params: A list of hyperparameters to track throughout the optimization procedure. Useful for\n    debugging/model checking.\n\n    \"\"\"\n\n    def __init__(self,\n                 gdl,\n                 fix_params=None,\n                 tracked_params=None,\n                 lambda_min=None,\n                 float_precision='float32',\n                 order='F',\n                 low_memory=True,\n                 dequantize_on_the_fly=False,\n                 threads=1):\n\n        \"\"\"\n\n        Initialize the VIPRS model.\n\n        .. note::\n            The initialization of the model involves loading the LD matrix to memory.\n\n        :param gdl: An instance of GWADataLoader containing harmonized GWAS summary statistics and LD matrices.\n        :param fix_params: A dictionary of hyperparameters with their fixed values.\n        :param tracked_params: A list of hyperparameters/quantities to track throughout the optimization\n        procedure. Useful for debugging/model checking. Currently, we allow the user to track the following:\n\n            * The proportion of causal variants (`pi`).\n            * The heritability ('heritability').\n            * The residual variance (`sigma_epsilon`).\n            * The prior precision for the effect size (`tau_beta`).\n            * The additive genotypic variance (`sigma_g`).\n            * The maximum difference in the posterior mean between iterations (`max_eta_diff`).\n            * User may also provide arbitrary functions that take the `VIPRS` object as input and\n            compute any quantity of interest from it.\n\n        :param lambda_min: The minimum eigenvalue for the LD matrix (or an approximation of it that will serve\n        as a regularizer). If set to 'infer', the minimum eigenvalue will be computed or retrieved from the LD matrix.\n        :param float_precision: The precision of the floating point variables. Options are: 'float32' or 'float64'.\n        :param order: The order of the arrays in memory. Options are: 'C' or 'F'.\n        :param low_memory: A boolean flag to indicate whether to use low memory mode.\n        :param dequantize_on_the_fly: A boolean flag to indicate whether to dequantize the LD matrix on the fly.\n        :param threads: The number of threads to use when fitting the model.\n        \"\"\"\n\n        super().__init__(gdl, float_precision=float_precision)\n\n        # ------------------- Initialize the model -------------------\n\n        # Variational parameters:\n        self.var_gamma = {}\n        self.var_mu = {}\n        self.var_tau = {}\n\n        # Cache this quantity:\n        self._log_var_tau = {}\n\n        # Properties of proposed variational distribution:\n        self.eta = {}  # The posterior mean, E[B] = \\gamma*\\mu_beta\n        self.zeta = {}  # The expectation of B^2 under the posterior, E[B^2] = \\gamma*(\\mu_beta^2 + 1./\\tau_beta)\n\n        # The difference between the etas in two consecutive iterations (can be used for checking convergence,\n        # or implementing optimized updates in the E-Step).\n        self.eta_diff = {}\n\n        # q-factor (keeps track of LD-related terms)\n        self.q = {}\n\n        # ---------- Model hyperparameters ----------\n\n        self.sigma_epsilon = None\n        self.tau_beta = None\n        self.pi = None\n        self._sigma_g = None  # A proxy for the additive genotypic variance\n        self.lambda_min = None\n\n        # ---------- Inputs to the model: ----------\n\n        # NOTE: Here, we typecast the inputs to the model to the specified float precision.\n        # This also needs to be done in the initialization methods.\n\n        # LD-related quantities:\n\n        self.ld_data = {}\n        self.ld_indptr = {}\n        self.ld_left_bound = {}\n\n        logger.debug(\"&gt; Loading LD matrices to memory\")\n\n        for c, ld_mat in self.gdl.get_ld_matrices().items():\n\n            # Determine how to load the LD data:\n            if dequantize_on_the_fly and np.issubdtype(ld_mat.stored_dtype, np.integer):\n                dtype = ld_mat.stored_dtype\n            else:\n\n                if dequantize_on_the_fly:\n                    logger.debug(\"Dequantization on the fly is only supported for \"\n                                 \"integer data types. Ignoring this flag.\")\n\n                dtype = float_precision\n                dequantize_on_the_fly = False\n\n            ld_lop = ld_mat.load(return_symmetric=not low_memory, dtype=dtype)\n\n            # Load the LD data:\n            self.ld_data[c] = ld_lop.ld_data\n            self.ld_indptr[c] = ld_lop.ld_indptr\n            self.ld_left_bound[c] = ld_lop.leftmost_idx\n\n            # Obtain / infer lambda_min:\n            # TODO: Handle cases where we do inference over multiple chromosomes.\n            # In this case, `lambda_min` should ideally be a dictionary.\n            if lambda_min is None:\n                self.lambda_min = 0.\n            elif is_numeric(lambda_min):\n\n                self.lambda_min = lambda_min\n\n                if not np.isscalar(self.lambda_min):\n                    assert self.lambda_min.shape == self.ld_indptr[c].shape[0] - 1, \\\n                        \"Vector-valued lambda_min must have the same shape as the LD matrix.\"\n            else:\n\n                # If lambda min is set to `infer`, we try to retrieve information about the\n                # spectral properties of the LD matrix from the LDMatrix object.\n                # If this is not available, we set the minimum eigenvalue to 0.\n                self.lambda_min = ld_mat.get_lambda_min(min_max_ratio=1e-3)\n\n        # ---------- General properties: ----------\n\n        self.threads = threads\n        self.fix_params = fix_params or {}\n\n        self.order = order\n        self.low_memory = low_memory\n\n        self.dequantize_on_the_fly = dequantize_on_the_fly\n\n        if self.dequantize_on_the_fly:\n            info = np.iinfo(self.ld_data[self.chromosomes[0]].dtype)\n            self.dequantize_scale = 1. / info.max\n        else:\n            self.dequantize_scale = 1.\n\n        self.optim_result = OptimizeResult()\n        self.history = {}\n        self.tracked_params = tracked_params or []\n\n    def initialize(self, theta_0=None, param_0=None):\n        \"\"\"\n        A convenience method to initialize all the objects associated with the model.\n        :param theta_0: A dictionary of initial values for the hyperparameters theta\n        :param param_0: A dictionary of initial values for the variational parameters\n        \"\"\"\n\n        logger.debug(\"&gt; Initializing model parameters\")\n\n        self.initialize_theta(theta_0)\n        self.initialize_variational_parameters(param_0)\n        self.init_optim_meta()\n\n    def init_optim_meta(self):\n        \"\"\"\n        Initialize the various quantities/objects to keep track of the optimization process.\n         This method initializes the \"history\" object (which keeps track of the objective + other\n         hyperparameters requested by the user), in addition to the OptimizeResult objects.\n        \"\"\"\n\n        self.history = {\n            'ELBO': [],\n        }\n\n        for tt in self.tracked_params:\n            if isinstance(tt, str):\n                self.history[tt] = []\n            elif callable(tt):\n                self.history[tt.__name__] = []\n\n        self.optim_result.reset()\n\n    def initialize_theta(self, theta_0=None):\n        \"\"\"\n        Initialize the global hyperparameters of the model.\n        :param theta_0: A dictionary of initial values for the hyperparameters theta\n        \"\"\"\n\n        if theta_0 is not None and self.fix_params is not None:\n            theta_0.update(self.fix_params)\n        elif self.fix_params is not None:\n            theta_0 = self.fix_params\n        elif theta_0 is None:\n            theta_0 = {}\n\n        # ----------------------------------------------\n        # (1) If 'pi' is not set, initialize from a uniform\n        if 'pi' not in theta_0:\n\n            min_pi = max(10./self.n_snps, 1e-5)\n            max_pi = min(0.2, 1e4/self.n_snps)\n\n            self.pi = np.random.uniform(low=min_pi, high=max_pi)\n        else:\n            self.pi = theta_0['pi']\n\n        # ----------------------------------------------\n        # (2) Initialize sigma_epsilon and tau_beta\n        # Assuming that the genotype and phenotype are normalized,\n        # these two quantities are conceptually linked.\n        # The initialization routine here assumes that:\n        # Var(y) = h2 + sigma_epsilon\n        # Where, by assumption, Var(y) = 1,\n        # And h2 ~= pi*M/tau_beta\n\n        if 'sigma_epsilon' not in theta_0:\n            if 'tau_beta' not in theta_0:\n\n                # If neither tau_beta nor sigma_epsilon are given,\n                # then initialize using the SNP heritability estimate\n\n                try:\n                    from magenpy.stats.h2.ldsc import simple_ldsc\n                    naive_h2g = np.clip(simple_ldsc(self.gdl), a_min=.01, a_max=.99)\n                except Exception as e:\n                    logger.debug(e)\n                    naive_h2g = np.random.uniform(low=.01, high=.1)\n\n                self.sigma_epsilon = 1. - naive_h2g\n                self.tau_beta = self.pi * self.n_snps / max(naive_h2g, 0.01)\n            else:\n\n                # If tau_beta is given, use it to initialize sigma_epsilon\n\n                self.tau_beta = theta_0['tau_beta']\n                self.sigma_epsilon = np.clip(1. - (self.pi * self.n_snps / self.tau_beta),\n                                             a_min=1e-4,\n                                             a_max=1. - 1e-4)\n        else:\n\n            # If sigma_epsilon is given, use it in the initialization\n\n            self.sigma_epsilon = theta_0['sigma_epsilon']\n\n            if 'tau_beta' in theta_0:\n                self.tau_beta = theta_0['tau_beta']\n            else:\n                self.tau_beta = (self.pi * self.n_snps) / np.maximum(0.01, 1. - self.sigma_epsilon)\n\n        # Cast all the hyperparameters to conform to the precision set by the user:\n        self.sigma_epsilon = np.dtype(self.float_precision).type(self.sigma_epsilon)\n        self.pi = np.dtype(self.float_precision).type(self.pi)\n        self.lambda_min = np.dtype(self.float_precision).type(self.lambda_min)\n        self._sigma_g = np.dtype(self.float_precision).type(0.)\n\n    def initialize_variational_parameters(self, param_0=None):\n        \"\"\"\n        Initialize the variational parameters of the model.\n        :param param_0: A dictionary of initial values for the variational parameters\n        \"\"\"\n\n        param_0 = param_0 or {}\n\n        self.var_mu = {}\n        self.var_tau = {}\n        self.var_gamma = {}\n\n        for c, shapes in self.shapes.items():\n\n            # Initialize the variational parameters according to the derived update equations,\n            # ignoring correlations between SNPs.\n            if 'tau' in param_0:\n                self.var_tau[c] = param_0['tau'][c]\n            else:\n                self.var_tau[c] = (self.n_per_snp[c] / self.sigma_epsilon) + self.tau_beta\n\n            self.var_tau[c] = self.var_tau[c]\n\n            if 'mu' in param_0:\n                self.var_mu[c] = param_0['mu'][c].astype(self.float_precision, order=self.order)\n            else:\n                self.var_mu[c] = np.zeros(shapes, dtype=self.float_precision, order=self.order)\n\n            if 'gamma' in param_0:\n                self.var_gamma[c] = param_0['gamma'][c].astype(self.float_precision, order=self.order)\n            else:\n                pi = self.get_pi(c)\n                if isinstance(self.pi, dict):\n                    self.var_gamma[c] = pi.astype(self.float_precision, order=self.order)\n                else:\n                    self.var_gamma[c] = pi*np.ones(shapes, dtype=self.float_precision, order=self.order)\n\n        self.eta = self.compute_eta()\n        self.zeta = self.compute_zeta()\n        self.eta_diff = {c: np.zeros_like(eta, dtype=self.float_precision) for c, eta in self.eta.items()}\n        self.q = {c: np.zeros_like(eta, dtype=self.float_precision) for c, eta in self.eta.items()}\n        self._log_var_tau = {c: np.log(self.var_tau[c]) for c in self.var_tau}\n\n    def set_fixed_params(self, fix_params):\n        \"\"\"\n        Set the fixed hyperparameters of the model.\n        :param fix_params: A dictionary of hyperparameters with their fixed values.\n        \"\"\"\n\n        assert isinstance(fix_params, dict), \"The fixed parameters must be provided as a dictionary.\"\n\n        self.fix_params.update(fix_params)\n\n        for key, val in fix_params.items():\n            if key == 'sigma_epsilon':\n                self.sigma_epsilon = np.dtype(self.float_precision).type(val)\n            elif key == 'tau_beta':\n                self.tau_beta = np.dtype(self.float_precision).type(val)\n            elif key == 'pi':\n                self.pi = np.dtype(self.float_precision).type(val)\n            elif key == 'lambda_min':\n                self.lambda_min = np.dtype(self.float_precision).type(val)\n\n    def e_step(self):\n        \"\"\"\n        Run the E-Step of the Variational EM algorithm.\n        Here, we update the variational parameters for each variant using coordinate\n        ascent optimization techniques. The update equations are outlined in\n        the Supplementary Material of the following paper:\n\n        &gt; Zabad S, Gravel S, Li Y. Fast and accurate Bayesian polygenic risk modeling with variational inference.\n        Am J Hum Genet. 2023 May 4;110(5):741-761. doi: 10.1016/j.ajhg.2023.03.009.\n        Epub 2023 Apr 7. PMID: 37030289; PMCID: PMC10183379.\n        \"\"\"\n\n        for c, c_size in self.shapes.items():\n\n            # Get the priors:\n            tau_beta = self.get_tau_beta(c)\n            pi = self.get_pi(c)\n\n            # Updates for tau variational parameters:\n            self.var_tau[c] = (self.n_per_snp[c]*(1. + self.lambda_min) / self.sigma_epsilon) + tau_beta\n            np.log(self.var_tau[c], out=self._log_var_tau[c])\n\n            # Compute some quantities that are needed for the per-SNP updates:\n            mu_mult = (self.n_per_snp[c]/(self.var_tau[c]*self.sigma_epsilon)).astype(self.float_precision)\n            u_logs = (np.log(pi) - np.log(1. - pi) + .5*(np.log(tau_beta) -\n                                                         self._log_var_tau[c])).astype(self.float_precision)\n\n            cpp_e_step(self.ld_left_bound[c],\n                       self.ld_indptr[c],\n                       self.ld_data[c],\n                       self.std_beta[c],\n                       self.var_gamma[c],\n                       self.var_mu[c],\n                       self.eta[c],\n                       self.q[c],\n                       self.eta_diff[c],\n                       u_logs,\n                       np.sqrt(0.5*self.var_tau[c]).astype(self.float_precision),\n                       mu_mult,\n                       self.dequantize_scale,\n                       self.threads,\n                       self.low_memory)\n\n        self.zeta = self.compute_zeta()\n\n    def update_pi(self):\n        \"\"\"\n        Update the prior probability of a variant being causal, or the proportion of causal variants, `pi`.\n        \"\"\"\n\n        if 'pi' not in self.fix_params:\n\n            # Get the average of the gammas:\n            self.pi = dict_mean(self.var_gamma, axis=0)\n\n    def update_tau_beta(self):\n        \"\"\"\n        Update the prior precision (inverse variance) for the effect size, `tau_beta`.\n        \"\"\"\n\n        if 'tau_beta' not in self.fix_params:\n\n            # tau_beta estimate:\n            self.tau_beta = (self.pi * self.n_snps / dict_sum(self.zeta, axis=0))\n\n    def _update_sigma_g(self):\n        \"\"\"\n        Update the expectation of the additive genotypic variance, `sigma_g`, under the variational distribution.\n        This quantity is equivalent to E_q[B'RB], where B is the vector of effect sizes and R is the LD matrix.\n        This quantity is used in the update of the residual variance, `sigma_epsilon` and\n        in computing the pseudo-heritability.\n        \"\"\"\n\n        self._sigma_g = np.sum([\n            np.sum((1. + self.lambda_min)*self.zeta[c] + np.multiply(self.q[c], self.eta[c]), axis=0)\n            for c in self.shapes.keys()\n        ], axis=0)\n\n    def update_sigma_epsilon(self):\n        \"\"\"\n        Update the global residual variance parameter, `sigma_epsilon`.\n        \"\"\"\n\n        if 'sigma_epsilon' not in self.fix_params:\n\n            sig_eps = 0.\n\n            for c, _ in self.shapes.items():\n                sig_eps -= 2.*self.std_beta[c].dot(self.eta[c])\n\n            self.sigma_epsilon = 1. + sig_eps + self._sigma_g\n\n    def m_step(self):\n        \"\"\"\n        Run the M-Step of the Variational EM algorithm.\n        Here, we update the hyperparameters of the model, by simply calling\n        the update functions for each hyperparameter separately.\n\n        \"\"\"\n\n        self.update_pi()\n        self.update_tau_beta()\n        self._update_sigma_g()\n        self.update_sigma_epsilon()\n\n    def objective(self):\n        \"\"\"\n        The optimization objective for the variational inference problem. The objective\n        for the VIPRS method is the Evidence Lower-Bound (ELBO) in this case.\n\n        !!! seealso \"See Also\"\n            * [elbo][viprs.model.VIPRS.VIPRS.elbo]\n\n        \"\"\"\n        return self.elbo()\n\n    def elbo(self, sum_axis=None):\n        \"\"\"\n        Compute the variational objective, the Evidence Lower-BOund (ELBO),\n        from GWAS summary statistics and the reference LD data. This implementation assumes\n        that the product of the LD matrix with the current estimate of the effect sizes\n        is already computed and stored in the `q` dictionary. If this is not the case,\n        we recommend computing q first and then calling this method.\n\n        :param sum_axis: The axis along which to sum the ELBO. If None, the ELBO is returned as a scalar.\n        :return: The ELBO of the model.\n        \"\"\"\n\n        double_resolution = np.finfo(np.float64).resolution\n\n        # Concatenate the dictionary items for easy computation:\n        var_gamma = np.clip(dict_concat(self.var_gamma).astype(np.float64),\n                            a_min=double_resolution,\n                            a_max=1. - double_resolution)\n        # The gamma for the null component\n        null_gamma = np.clip(1. - dict_concat(self.compute_pip()).astype(np.float64),\n                             a_min=double_resolution,\n                             a_max=1. - double_resolution)\n        log_var_tau = dict_concat(self._log_var_tau)\n\n        if isinstance(self.pi, dict):\n            pi = dict_concat(self.pi)\n            null_pi = dict_concat(self.get_null_pi())\n        else:\n            pi = self.pi\n            null_pi = self.get_null_pi()\n\n        if isinstance(self.tau_beta, dict):\n            tau_beta = dict_concat(self.tau_beta).astype(np.float64)\n        else:\n            tau_beta = self.tau_beta\n\n        zeta = dict_concat(self.zeta).astype(np.float64)\n\n        # Initialize the ELBO:\n        elbo = 0.\n\n        # -----------------------------------------------\n        # (1) Compute the log of the joint density:\n\n        #\n        # (1.1) The following terms are an expansion of ||Y - X\\beta||^2\n        #\n        # -N/2log(2pi*sigma_epsilon)\n        elbo -= np.log(2 * np.pi * self.sigma_epsilon)\n\n        # -Y'Y/(2*sigma_epsilon), where we assume Y'Y = N\n        # + (1./sigma_epsilon)*\\beta*(XY), where we assume XY = N\\hat{\\beta}\n        if 'sigma_epsilon' not in self.fix_params:\n            # If sigma_epsilon was updated in the M-Step, then this expression would\n            # simply evaluate to 1. and there's no point in re-computing it again:\n            elbo -= 1.\n        else:\n\n            eta = dict_concat(self.eta).astype(np.float64)\n            std_beta = dict_concat(self.std_beta).astype(np.float64)\n\n            elbo -= (1. / self.sigma_epsilon) * (1. - 2.*std_beta.dot(eta) + self._sigma_g)\n\n        elbo *= 0.5*self.n\n\n        elbo -= np.multiply(var_gamma, np.log(var_gamma) - np.log(pi)).sum(axis=sum_axis)\n        elbo -= np.multiply(null_gamma, np.log(null_gamma) - np.log(null_pi)).sum(axis=sum_axis)\n\n        elbo += .5 * np.multiply(var_gamma, 1. - log_var_tau + np.log(tau_beta)).sum(axis=sum_axis)\n\n        if np.isscalar(tau_beta) or len(zeta.shape) &gt; 1:\n            elbo -= .5*(tau_beta*zeta).sum(axis=sum_axis)\n        else:\n            var_mu = dict_concat(self.var_mu)\n            var_tau = dict_concat(self.var_tau)\n\n            elbo -= .5*(np.multiply(var_gamma, tau_beta) * (var_mu**2 + 1./var_tau)).sum(axis=sum_axis)\n\n        try:\n            if len(elbo) == 1:\n                return elbo[0]\n            else:\n                return elbo\n        except TypeError:\n            return elbo\n\n    def entropy(self, sum_axis=None):\n        \"\"\"\n        Compute the entropy of the variational distribution given the current parameter values.\n\n        :param sum_axis: The axis along which to sum the ELBO. If None, the ELBO is returned as a scalar.\n        :return: The entropy of the variational distribution.\n        \"\"\"\n\n        double_resolution = np.finfo(np.float64).resolution\n\n        # Concatenate the dictionary items for easy computation:\n        var_gamma = np.clip(dict_concat(self.var_gamma),\n                            a_min=double_resolution,\n                            a_max=1. - double_resolution)\n        # The gamma for the null component\n        null_gamma = np.clip(1. - dict_concat(self.compute_pip()),\n                             a_min=double_resolution,\n                             a_max=1. - double_resolution)\n\n        log_var_tau = dict_concat(self._log_var_tau)\n\n        entropy = 0.\n\n        # Bernoulli entropy terms:\n        entropy -= np.multiply(var_gamma, np.log(var_gamma)).sum(axis=sum_axis)\n        entropy -= np.multiply(null_gamma, np.log(null_gamma)).sum(axis=sum_axis)\n        # Gaussian entropy terms:\n        entropy -= .5 * np.multiply(var_gamma, log_var_tau).sum(axis=sum_axis)\n\n        return .5 * self.n_snps * (np.log(2. * np.pi) + 1.) + entropy\n\n    def loglikelihood(self):\n        \"\"\"\n        Compute the expectation of the loglikelihood of the data given the current model parameter values.\n        The expectation is taken with respect to the variational distribution.\n\n        :return: The loglikelihood of the data.\n        \"\"\"\n\n        eta = dict_concat(self.eta)\n        std_beta = dict_concat(self.std_beta)\n\n        return -0.5*self.n*(\n                np.log(2.*np.pi*self.sigma_epsilon) +\n                (1./self.sigma_epsilon)*(1. - 2.*std_beta.dot(eta) + self._sigma_g)\n        )\n\n    def log_prior(self, sum_axis=None):\n        \"\"\"\n        Compute the expectation of the log prior of the model parameters given the current hyperparameter values.\n        The expectation is taken with respect to the variational distribution.\n\n        :param sum_axis: The axis along which to sum the log prior.\n        :return: The expectation of the log prior according to the variational density.\n        \"\"\"\n\n        double_resolution = np.finfo(np.float64).resolution\n\n        var_gamma = np.clip(dict_concat(self.var_gamma),\n                            a_min=double_resolution,\n                            a_max=1. - double_resolution)\n        # The gamma for the null component\n        null_gamma = np.clip(1. - dict_concat(self.compute_pip()),\n                             a_min=double_resolution,\n                             a_max=1. - double_resolution)\n\n        if isinstance(self.pi, dict):\n            pi = dict_concat(self.pi)\n            null_pi = dict_concat(self.get_null_pi())\n        else:\n            pi = self.pi\n            null_pi = self.get_null_pi()\n\n        if isinstance(self.tau_beta, dict):\n            tau_beta = dict_concat(self.tau_beta)\n        else:\n            tau_beta = self.tau_beta\n\n        zeta = dict_concat(self.zeta)\n\n        log_prior = 0.\n\n        log_prior += .5*(np.multiply(var_gamma, np.log(tau_beta))).sum(axis=sum_axis)\n        log_prior += np.multiply(var_gamma, np.log(pi)).sum(axis=sum_axis)\n        log_prior += np.multiply(null_gamma, np.log(null_pi)).sum(axis=sum_axis)\n\n        if np.isscalar(tau_beta) or len(zeta.shape) &gt; 1:\n            log_prior -= (.5*tau_beta * zeta).sum(axis=sum_axis)\n        else:\n            var_mu = dict_concat(self.var_mu)\n            var_tau = dict_concat(self.var_tau)\n\n            log_prior -= .5 * (np.multiply(var_gamma, tau_beta) * (var_mu ** 2 + 1. / var_tau)).sum(axis=sum_axis)\n\n        return log_prior - .5*self.n_snps*np.log(2.*np.pi)\n\n    def complete_loglikelihood(self):\n        \"\"\"\n        Compute the complete loglikelihood of the data given the current model parameter values.\n        The complete loglikelihood is the sum of the loglikelihood and the log prior.\n\n        :return: The complete loglikelihood of the data.\n        \"\"\"\n\n        return self.loglikelihood() + self.log_prior()\n\n    def mse(self, sum_axis=None):\n        \"\"\"\n        Compute a summary statistics-based estimate of the mean squared error on the training set.\n\n        :param sum_axis: The axis along which to sum the MSE.\n        If None, the MSE is returned as a scalar.\n        :return: The mean squared error.\n        \"\"\"\n\n        eta = dict_concat(self.eta)\n        std_beta = dict_concat(self.std_beta)\n        zeta = dict_concat(self.zeta)\n\n        return 1. - 2.*std_beta.dot(eta) + (\n                self._sigma_g - zeta.sum(axis=sum_axis) + (eta**2).sum(axis=sum_axis)\n        )\n\n    def get_sigma_epsilon(self):\n        \"\"\"\n        :return: The value of the residual variance, `sigma_epsilon`.\n        \"\"\"\n        return self.sigma_epsilon\n\n    def get_tau_beta(self, chrom=None):\n        \"\"\"\n        :param chrom: Get the value of `tau_beta` for a given chromosome.\n\n        :return: The value of the prior precision on the effect size(s), `tau_beta`\n        \"\"\"\n        if chrom is None:\n            return self.tau_beta\n        else:\n            if isinstance(self.tau_beta, dict):\n                return self.tau_beta[chrom]\n            else:\n                return self.tau_beta\n\n    def get_pi(self, chrom=None):\n        \"\"\"\n        :param chrom: Get the value of `pi` for a given chromosome.\n\n        :return: The value of the prior probability of a variant being causal, `pi`.\n        \"\"\"\n\n        if chrom is None:\n            return self.pi\n        else:\n            if isinstance(self.pi, dict):\n                return self.pi[chrom]\n            else:\n                return self.pi\n\n    def get_null_pi(self, chrom=None):\n        \"\"\"\n        :param chrom: If provided, get the mixing proportion for the null component on a given chromosome.\n\n        :return: The value of the prior probability of a variant being null, `1 - pi`.\n        \"\"\"\n\n        pi = self.get_pi(chrom=chrom)\n\n        if isinstance(pi, dict):\n            return {c: 1. - c_pi for c, c_pi in pi.items()}\n        else:\n            return 1. - pi\n\n    def get_proportion_causal(self):\n        \"\"\"\n        :return: The proportion of causal variants in the model.\n        \"\"\"\n        if isinstance(self.pi, dict):\n            return dict_mean(self.pi, axis=0)\n        else:\n            return self.pi\n\n    def get_average_effect_size_variance(self):\n        \"\"\"\n        :return: The average per-SNP variance for the prior mixture components\n        \"\"\"\n        if isinstance(self.pi, dict):\n            pi = dict_concat(self.pi, axis=0)\n        else:\n            pi = self.pi\n\n        if isinstance(self.tau_beta, dict):\n            tau_beta = dict_concat(self.tau_beta, axis=0)\n        else:\n            tau_beta = self.tau_beta\n\n        return np.sum(pi / tau_beta, axis=0)\n\n    def get_heritability(self):\n        \"\"\"\n        :return: An estimate of the SNP heritability, or proportion of variance explained by SNPs.\n        \"\"\"\n\n        return self._sigma_g / (self._sigma_g + self.sigma_epsilon)\n\n    def to_theta_table(self):\n        \"\"\"\n        :return: A `pandas` DataFrame containing information about the estimated hyperparameters of the model.\n        \"\"\"\n\n        theta_table = [\n            {'Parameter': 'ELBO', 'Value': self.elbo()},\n            {'Parameter': 'Residual_variance', 'Value': self.sigma_epsilon},\n            {'Parameter': 'Heritability', 'Value': self.get_heritability()},\n            {'Parameter': 'Proportion_causal', 'Value': self.get_proportion_causal()},\n            {'Parameter': 'Average_effect_variance', 'Value': self.get_average_effect_size_variance()},\n        ]\n\n        if np.isscalar(self.lambda_min):\n            theta_table += [\n                {'Parameter': 'Lambda_min', 'Value': self.lambda_min}\n            ]\n\n        if isinstance(self.tau_beta, dict):\n            taus = dict_mean(self.tau_beta, axis=0)\n        else:\n            taus = self.tau_beta\n\n        try:\n            taus = list(taus)\n            for i in range(len(taus)):\n                theta_table.append({'Parameter': f'tau_beta_{i+1}', 'Value': taus[i]})\n        except TypeError:\n            theta_table.append({'Parameter': 'tau_beta', 'Value': taus})\n\n        return pd.DataFrame(theta_table)\n\n    def to_history_table(self):\n        \"\"\"\n        :return: A `pandas` DataFrame containing the history of tracked parameters as a function of\n        the number of iterations.\n        \"\"\"\n        return pd.DataFrame(self.history)\n\n    def write_inferred_theta(self, f_name, sep=\"\\t\"):\n        \"\"\"\n        A convenience method to write the inferred (and fixed) hyperparameters of the model to file.\n        :param f_name: The file name\n        :param sep: The separator for the hyperparameter file.\n        \"\"\"\n\n        # Write the table to file:\n        try:\n            self.to_theta_table().to_csv(f_name, sep=sep, index=False)\n        except Exception as e:\n            raise e\n\n    def update_theta_history(self):\n        \"\"\"\n        A convenience method to update the history of the hyperparameters/objectives/other summary statistics\n        of the model, if the user requested that they should be tracked.\n        \"\"\"\n\n        self.history['ELBO'].append(self.elbo())\n\n        for tt in self.tracked_params:\n            if tt == 'pi':\n                self.history['pi'].append(self.get_proportion_causal())\n            elif tt == 'pis':\n                self.history['pis'].append(self.pi)\n            if tt == 'heritability':\n                self.history['heritability'].append(self.get_heritability())\n            if tt == 'sigma_epsilon':\n                self.history['sigma_epsilon'].append(self.sigma_epsilon)\n            elif tt == 'tau_beta':\n                self.history['tau_beta'].append(self.tau_beta)\n            elif tt == 'sigma_g':\n                self.history['sigma_g'].append(self._sigma_g)\n            elif tt == 'entropy':\n                self.history['entropy'].append(self.entropy())\n            elif tt == 'loglikelihood':\n                self.history['loglikelihood'].append(self.loglikelihood())\n            elif tt == 'log_prior':\n                self.history['log_prior'].append(self.log_prior())\n            elif tt == 'mse':\n                self.history['mse'].append(self.mse())\n            elif tt == 'max_eta_diff':\n                self.history['max_eta_diff'].append(np.max([\n                    np.max(np.abs(diff)) for diff in self.eta_diff.values()\n                ]))\n            elif callable(tt):\n                self.history[tt.__name__].append(tt(self))\n\n    def compute_pip(self):\n        \"\"\"\n        :return: The posterior inclusion probability (PIP) of\n        each variant under the variational posterior.\n        \"\"\"\n        return self.var_gamma.copy()\n\n    def compute_eta(self):\n        \"\"\"\n        :return: The mean for the effect size under the variational posterior.\n        \"\"\"\n        return {c: v*self.var_mu[c] for c, v in self.var_gamma.items()}\n\n    def compute_zeta(self):\n        \"\"\"\n\n        .. note:: Due to the small magnitude of the variational parameters, we only store\n        zeta using double precision.\n\n        :return: The expectation of the squared effect size under the variational posterior.\n        \"\"\"\n        return {c: np.multiply(v, self.var_mu[c].astype(np.float64)**2 + 1./self.var_tau[c].astype(np.float64))\n                for c, v in self.var_gamma.items()}\n\n    def update_posterior_moments(self):\n        \"\"\"\n        A convenience method to update the dictionaries containing the posterior moments,\n        including the PIP and posterior mean and variance for the effect size.\n        \"\"\"\n\n        self.pip = self.compute_pip()\n        self.post_mean_beta = {c: eta.copy() for c, eta in self.eta.items()}\n        self.post_var_beta = {c: zeta - self.eta[c]**2 for c, zeta in self.zeta.items()}\n\n    def fit(self,\n            max_iter=1000,\n            theta_0=None,\n            param_0=None,\n            continued=False,\n            disable_pbar=False,\n            min_iter=3,\n            f_abs_tol=1e-6,\n            x_abs_tol=1e-6,\n            patience=10,\n            **kwargs):\n        \"\"\"\n        A convenience method to fit the model using the Variational EM algorithm.\n\n        :param max_iter: Maximum number of iterations. \n        :param theta_0: A dictionary of values to initialize the hyperparameters\n        :param param_0: A dictionary of values to initialize the variational parameters\n        :param continued: If true, continue the model fitting for more iterations from current parameters\n        instead of starting over.\n        :param disable_pbar: If True, disable the progress bar.\n        :param min_iter: The minimum number of iterations to run before checking for convergence.\n        :param f_abs_tol: The absolute tolerance threshold for the objective (ELBO).\n        :param x_abs_tol: The absolute tolerance threshold for the variational parameters.\n        :param patience: The maximum number of consecutive iterations with no improvement in the ELBO or change\n        in model parameters.\n        :param kwargs: Additional keyword arguments to pass to the optimization routine.\n\n        :return: The VIPRS object with the fitted model parameters.\n        \"\"\"\n\n        if not continued:\n            self.initialize(theta_0, param_0)\n            start_idx = 1\n            self.update_theta_history()\n        else:\n            start_idx = len(self.history['ELBO']) + 1\n            # Update OptimizeResult object to enable continuation of the optimization:\n            self.optim_result.update(self.elbo(), increment=False)\n\n        logger.info(\"&gt; Performing model fit...\")\n        if self.threads &gt; 1:\n            logger.info(f\"&gt; Using up to {self.threads} threads.\")\n\n        # If the model is fit over a single chromosome, append this information to the\n        # tqdm progress bar:\n        if len(self.shapes) == 1:\n            desc = f\"Chromosome {self.chromosomes[0]} ({self.n_snps} variants)\"\n        else:\n            desc = None\n\n        if continued:\n            prev_elbo = self.elbo()\n        else:\n            prev_elbo = -np.inf\n\n        # The following is used to track LD-weighted effect sizes.\n        # This is useful for tracking oscillations in ultra high-dimensions due to high LD.\n        prev_sigma_g = self._sigma_g\n        sigma_g_icc = IterationConditionCounter()\n        divergence_icc = IterationConditionCounter()\n\n        # -------------------------- Main optimization loop (EM Algorithm) --------------------------\n\n        with (logging_redirect_tqdm(loggers=[logger])):\n\n            # Progress bar:\n            pbar = tqdm(range(start_idx, start_idx + max_iter),\n                        disable=disable_pbar,\n                        desc=desc)\n\n            for i in pbar:\n\n                if self.optim_result.stop_iteration:\n                    pbar.set_postfix({'Final ELBO': f\"{self.optim_result.objective:.4f}\"})\n                    pbar.n = i - 1\n                    pbar.total = i - 1\n                    pbar.refresh()\n                    pbar.close()\n                    break\n\n                # Perform parameter updates (E-Step + M-Step):\n                self.e_step()\n                self.m_step()\n\n                # Update the tracked parameters (including objectives):\n                self.update_theta_history()\n\n                # Compute maximum absolute difference in effect sizes:\n                max_eta_diff = max([np.max(np.abs(diff)) for diff in self.eta_diff.values()])\n\n                # Update the current ELBO:\n                curr_elbo = self.history['ELBO'][-1]\n\n                # Update the sigma_g condition counter:\n                sigma_g_icc.update(\n                    (i &gt; min_iter) and\n                    np.isclose(self._sigma_g, prev_sigma_g, atol=x_abs_tol, rtol=0.) and\n                    max_eta_diff &lt; x_abs_tol * 10,\n                    i\n                )\n\n                # Update the ELBO drop condition counter:\n                # TODO: Find other ways to determine if the optimization is diverging.\n                divergence_icc.update(\n                    (curr_elbo &lt; prev_elbo) and not\n                    np.isclose(curr_elbo, prev_elbo, atol=1e3*f_abs_tol, rtol=1e-4),\n                    i\n                )\n\n                # Update the progress bar:\n                pbar.set_postfix({'ELBO': f\"{curr_elbo:.4f}\"})\n\n                # --------------------------------------------------------------------------------------\n                # Sanity checking / convergence criteria:\n\n                # Check if the objective / model parameters behave in unexpected/pathological ways:\n                if self.mse() &lt; 0.:\n\n                    if 'sigma_epsilon' not in self.fix_params:\n\n                        logger.info(f\"Iteration {i} | MSE is negative; Restarting optimization \"\n                                    f\"and fixing residual variance hyperparameter (sigma_epsilon).\")\n\n                        self.initialize_theta(theta_0)\n                        self.initialize_variational_parameters(param_0)\n\n                        # Set the residual variance to a fixed value for now:\n                        self.fix_params['sigma_epsilon'] = self.sigma_epsilon = .95\n\n                        continue\n\n                    else:\n                        self.optim_result.update(curr_elbo,\n                                                 stop_iteration=True,\n                                                 success=False,\n                                                 message=f'The MSE is negative ({self.mse():.6f}).')\n\n                elif not np.isfinite(curr_elbo):\n                    self.optim_result.update(curr_elbo,\n                                             stop_iteration=True,\n                                             success=False,\n                                             message='Objective (ELBO) is undefined.')\n                elif self.sigma_epsilon &lt; 0.:\n                    self.optim_result.update(curr_elbo,\n                                             stop_iteration=True,\n                                             success=False,\n                                             message='Residual variance estimate is negative.')\n                elif self.threads &gt; 1 and self.optim_result.oscillation_counter &gt; 5:\n\n                    logger.info(f\"Iteration {i} | Reducing the number of \"\n                                f\"threads for better parameter synchronization.\")\n                    self.threads -= 1\n                    self.optim_result._reset_oscillation_counter()\n\n                elif self.get_heritability() &gt; 1. or self.get_heritability() &lt; 0.:\n\n                    self.optim_result.update(curr_elbo,\n                                             stop_iteration=True,\n                                             success=False,\n                                             message='Estimated heritability is out of bounds.')\n\n                # Check for convergence in the objective + parameters:\n                elif (i &gt; min_iter) and np.isclose(prev_elbo, curr_elbo, atol=f_abs_tol, rtol=0.):\n                    self.optim_result.update(curr_elbo,\n                                             stop_iteration=True,\n                                             success=True,\n                                             message='Objective (ELBO) converged successfully.')\n\n                elif (i &gt; min_iter) and max_eta_diff &lt; x_abs_tol:\n                    self.optim_result.update(curr_elbo,\n                                             stop_iteration=True,\n                                             success=True,\n                                             message='Variational parameters converged successfully.')\n                # Check for convergence based on the LD-weighted effect sizes:\n                elif sigma_g_icc.counter &gt; patience:\n                    self.optim_result.update(curr_elbo,\n                                             stop_iteration=True,\n                                             success=True,\n                                             message='LD-weighted variational parameters converged successfully.')\n                # Check if the ELBO has been consistently dropping:\n                elif divergence_icc.counter &gt; patience:\n\n                    self.optim_result.update(curr_elbo,\n                                             stop_iteration=True,\n                                             success=False,\n                                             message='The objective (ELBO) is decreasing.')\n\n                else:\n                    self.optim_result.update(curr_elbo)\n\n                prev_elbo = curr_elbo\n                prev_sigma_g = self._sigma_g\n\n        # -------------------------- Post processing / cleaning up / model checking --------------------------\n\n        # Update the posterior moments:\n        self.update_posterior_moments()\n\n        # Inspect the optim result:\n        if not self.optim_result.stop_iteration:\n            self.optim_result.update(self.elbo(),\n                                     stop_iteration=True,\n                                     success=False,\n                                     message=\"Maximum iterations reached without convergence.\\n\"\n                                             \"You may need to run the model for more iterations.\",\n                                     increment=False)\n\n        # Inform the user about potential issues:\n        if not self.optim_result.success:\n            logger.warning(\"\\t\" + self.optim_result.message)\n\n        logger.info(f\"&gt; Final ELBO: {self.history['ELBO'][-1]:.6f}\")\n        logger.info(f\"&gt; Estimated heritability: {self.get_heritability():.6f}\")\n        logger.info(f\"&gt; Estimated proportion of causal variants: {self.get_proportion_causal():.6f}\")\n\n        return self\n</code></pre>"},{"location":"api/model/VIPRS/#viprs.model.VIPRS.VIPRS.__init__","title":"<code>__init__(gdl, fix_params=None, tracked_params=None, lambda_min=None, float_precision='float32', order='F', low_memory=True, dequantize_on_the_fly=False, threads=1)</code>","text":"<p>Initialize the VIPRS model.</p> <p>.. note::     The initialization of the model involves loading the LD matrix to memory.</p> <p>Parameters:</p> Name Type Description Default <code>gdl</code> <p>An instance of GWADataLoader containing harmonized GWAS summary statistics and LD matrices.</p> required <code>fix_params</code> <p>A dictionary of hyperparameters with their fixed values.</p> <code>None</code> <code>tracked_params</code> <p>A list of hyperparameters/quantities to track throughout the optimization procedure. Useful for debugging/model checking. Currently, we allow the user to track the following:  * The proportion of causal variants (<code>pi</code>). * The heritability ('heritability'). * The residual variance (<code>sigma_epsilon</code>). * The prior precision for the effect size (<code>tau_beta</code>). * The additive genotypic variance (<code>sigma_g</code>). * The maximum difference in the posterior mean between iterations (<code>max_eta_diff</code>). * User may also provide arbitrary functions that take the <code>VIPRS</code> object as input and compute any quantity of interest from it.</p> <code>None</code> <code>lambda_min</code> <p>The minimum eigenvalue for the LD matrix (or an approximation of it that will serve as a regularizer). If set to 'infer', the minimum eigenvalue will be computed or retrieved from the LD matrix.</p> <code>None</code> <code>float_precision</code> <p>The precision of the floating point variables. Options are: 'float32' or 'float64'.</p> <code>'float32'</code> <code>order</code> <p>The order of the arrays in memory. Options are: 'C' or 'F'.</p> <code>'F'</code> <code>low_memory</code> <p>A boolean flag to indicate whether to use low memory mode.</p> <code>True</code> <code>dequantize_on_the_fly</code> <p>A boolean flag to indicate whether to dequantize the LD matrix on the fly.</p> <code>False</code> <code>threads</code> <p>The number of threads to use when fitting the model.</p> <code>1</code> Source code in <code>viprs/model/VIPRS.py</code> <pre><code>def __init__(self,\n             gdl,\n             fix_params=None,\n             tracked_params=None,\n             lambda_min=None,\n             float_precision='float32',\n             order='F',\n             low_memory=True,\n             dequantize_on_the_fly=False,\n             threads=1):\n\n    \"\"\"\n\n    Initialize the VIPRS model.\n\n    .. note::\n        The initialization of the model involves loading the LD matrix to memory.\n\n    :param gdl: An instance of GWADataLoader containing harmonized GWAS summary statistics and LD matrices.\n    :param fix_params: A dictionary of hyperparameters with their fixed values.\n    :param tracked_params: A list of hyperparameters/quantities to track throughout the optimization\n    procedure. Useful for debugging/model checking. Currently, we allow the user to track the following:\n\n        * The proportion of causal variants (`pi`).\n        * The heritability ('heritability').\n        * The residual variance (`sigma_epsilon`).\n        * The prior precision for the effect size (`tau_beta`).\n        * The additive genotypic variance (`sigma_g`).\n        * The maximum difference in the posterior mean between iterations (`max_eta_diff`).\n        * User may also provide arbitrary functions that take the `VIPRS` object as input and\n        compute any quantity of interest from it.\n\n    :param lambda_min: The minimum eigenvalue for the LD matrix (or an approximation of it that will serve\n    as a regularizer). If set to 'infer', the minimum eigenvalue will be computed or retrieved from the LD matrix.\n    :param float_precision: The precision of the floating point variables. Options are: 'float32' or 'float64'.\n    :param order: The order of the arrays in memory. Options are: 'C' or 'F'.\n    :param low_memory: A boolean flag to indicate whether to use low memory mode.\n    :param dequantize_on_the_fly: A boolean flag to indicate whether to dequantize the LD matrix on the fly.\n    :param threads: The number of threads to use when fitting the model.\n    \"\"\"\n\n    super().__init__(gdl, float_precision=float_precision)\n\n    # ------------------- Initialize the model -------------------\n\n    # Variational parameters:\n    self.var_gamma = {}\n    self.var_mu = {}\n    self.var_tau = {}\n\n    # Cache this quantity:\n    self._log_var_tau = {}\n\n    # Properties of proposed variational distribution:\n    self.eta = {}  # The posterior mean, E[B] = \\gamma*\\mu_beta\n    self.zeta = {}  # The expectation of B^2 under the posterior, E[B^2] = \\gamma*(\\mu_beta^2 + 1./\\tau_beta)\n\n    # The difference between the etas in two consecutive iterations (can be used for checking convergence,\n    # or implementing optimized updates in the E-Step).\n    self.eta_diff = {}\n\n    # q-factor (keeps track of LD-related terms)\n    self.q = {}\n\n    # ---------- Model hyperparameters ----------\n\n    self.sigma_epsilon = None\n    self.tau_beta = None\n    self.pi = None\n    self._sigma_g = None  # A proxy for the additive genotypic variance\n    self.lambda_min = None\n\n    # ---------- Inputs to the model: ----------\n\n    # NOTE: Here, we typecast the inputs to the model to the specified float precision.\n    # This also needs to be done in the initialization methods.\n\n    # LD-related quantities:\n\n    self.ld_data = {}\n    self.ld_indptr = {}\n    self.ld_left_bound = {}\n\n    logger.debug(\"&gt; Loading LD matrices to memory\")\n\n    for c, ld_mat in self.gdl.get_ld_matrices().items():\n\n        # Determine how to load the LD data:\n        if dequantize_on_the_fly and np.issubdtype(ld_mat.stored_dtype, np.integer):\n            dtype = ld_mat.stored_dtype\n        else:\n\n            if dequantize_on_the_fly:\n                logger.debug(\"Dequantization on the fly is only supported for \"\n                             \"integer data types. Ignoring this flag.\")\n\n            dtype = float_precision\n            dequantize_on_the_fly = False\n\n        ld_lop = ld_mat.load(return_symmetric=not low_memory, dtype=dtype)\n\n        # Load the LD data:\n        self.ld_data[c] = ld_lop.ld_data\n        self.ld_indptr[c] = ld_lop.ld_indptr\n        self.ld_left_bound[c] = ld_lop.leftmost_idx\n\n        # Obtain / infer lambda_min:\n        # TODO: Handle cases where we do inference over multiple chromosomes.\n        # In this case, `lambda_min` should ideally be a dictionary.\n        if lambda_min is None:\n            self.lambda_min = 0.\n        elif is_numeric(lambda_min):\n\n            self.lambda_min = lambda_min\n\n            if not np.isscalar(self.lambda_min):\n                assert self.lambda_min.shape == self.ld_indptr[c].shape[0] - 1, \\\n                    \"Vector-valued lambda_min must have the same shape as the LD matrix.\"\n        else:\n\n            # If lambda min is set to `infer`, we try to retrieve information about the\n            # spectral properties of the LD matrix from the LDMatrix object.\n            # If this is not available, we set the minimum eigenvalue to 0.\n            self.lambda_min = ld_mat.get_lambda_min(min_max_ratio=1e-3)\n\n    # ---------- General properties: ----------\n\n    self.threads = threads\n    self.fix_params = fix_params or {}\n\n    self.order = order\n    self.low_memory = low_memory\n\n    self.dequantize_on_the_fly = dequantize_on_the_fly\n\n    if self.dequantize_on_the_fly:\n        info = np.iinfo(self.ld_data[self.chromosomes[0]].dtype)\n        self.dequantize_scale = 1. / info.max\n    else:\n        self.dequantize_scale = 1.\n\n    self.optim_result = OptimizeResult()\n    self.history = {}\n    self.tracked_params = tracked_params or []\n</code></pre>"},{"location":"api/model/VIPRS/#viprs.model.VIPRS.VIPRS.complete_loglikelihood","title":"<code>complete_loglikelihood()</code>","text":"<p>Compute the complete loglikelihood of the data given the current model parameter values. The complete loglikelihood is the sum of the loglikelihood and the log prior.</p> <p>Returns:</p> Type Description <p>The complete loglikelihood of the data.</p> Source code in <code>viprs/model/VIPRS.py</code> <pre><code>def complete_loglikelihood(self):\n    \"\"\"\n    Compute the complete loglikelihood of the data given the current model parameter values.\n    The complete loglikelihood is the sum of the loglikelihood and the log prior.\n\n    :return: The complete loglikelihood of the data.\n    \"\"\"\n\n    return self.loglikelihood() + self.log_prior()\n</code></pre>"},{"location":"api/model/VIPRS/#viprs.model.VIPRS.VIPRS.compute_eta","title":"<code>compute_eta()</code>","text":"<p>Returns:</p> Type Description <p>The mean for the effect size under the variational posterior.</p> Source code in <code>viprs/model/VIPRS.py</code> <pre><code>def compute_eta(self):\n    \"\"\"\n    :return: The mean for the effect size under the variational posterior.\n    \"\"\"\n    return {c: v*self.var_mu[c] for c, v in self.var_gamma.items()}\n</code></pre>"},{"location":"api/model/VIPRS/#viprs.model.VIPRS.VIPRS.compute_pip","title":"<code>compute_pip()</code>","text":"<p>Returns:</p> Type Description <p>The posterior inclusion probability (PIP) of each variant under the variational posterior.</p> Source code in <code>viprs/model/VIPRS.py</code> <pre><code>def compute_pip(self):\n    \"\"\"\n    :return: The posterior inclusion probability (PIP) of\n    each variant under the variational posterior.\n    \"\"\"\n    return self.var_gamma.copy()\n</code></pre>"},{"location":"api/model/VIPRS/#viprs.model.VIPRS.VIPRS.compute_zeta","title":"<code>compute_zeta()</code>","text":"<p>.. note:: Due to the small magnitude of the variational parameters, we only store zeta using double precision.</p> <p>Returns:</p> Type Description <p>The expectation of the squared effect size under the variational posterior.</p> Source code in <code>viprs/model/VIPRS.py</code> <pre><code>def compute_zeta(self):\n    \"\"\"\n\n    .. note:: Due to the small magnitude of the variational parameters, we only store\n    zeta using double precision.\n\n    :return: The expectation of the squared effect size under the variational posterior.\n    \"\"\"\n    return {c: np.multiply(v, self.var_mu[c].astype(np.float64)**2 + 1./self.var_tau[c].astype(np.float64))\n            for c, v in self.var_gamma.items()}\n</code></pre>"},{"location":"api/model/VIPRS/#viprs.model.VIPRS.VIPRS.e_step","title":"<code>e_step()</code>","text":"<p>Run the E-Step of the Variational EM algorithm. Here, we update the variational parameters for each variant using coordinate ascent optimization techniques. The update equations are outlined in the Supplementary Material of the following paper:</p> <p>Zabad S, Gravel S, Li Y. Fast and accurate Bayesian polygenic risk modeling with variational inference. Am J Hum Genet. 2023 May 4;110(5):741-761. doi: 10.1016/j.ajhg.2023.03.009. Epub 2023 Apr 7. PMID: 37030289; PMCID: PMC10183379.</p> Source code in <code>viprs/model/VIPRS.py</code> <pre><code>def e_step(self):\n    \"\"\"\n    Run the E-Step of the Variational EM algorithm.\n    Here, we update the variational parameters for each variant using coordinate\n    ascent optimization techniques. The update equations are outlined in\n    the Supplementary Material of the following paper:\n\n    &gt; Zabad S, Gravel S, Li Y. Fast and accurate Bayesian polygenic risk modeling with variational inference.\n    Am J Hum Genet. 2023 May 4;110(5):741-761. doi: 10.1016/j.ajhg.2023.03.009.\n    Epub 2023 Apr 7. PMID: 37030289; PMCID: PMC10183379.\n    \"\"\"\n\n    for c, c_size in self.shapes.items():\n\n        # Get the priors:\n        tau_beta = self.get_tau_beta(c)\n        pi = self.get_pi(c)\n\n        # Updates for tau variational parameters:\n        self.var_tau[c] = (self.n_per_snp[c]*(1. + self.lambda_min) / self.sigma_epsilon) + tau_beta\n        np.log(self.var_tau[c], out=self._log_var_tau[c])\n\n        # Compute some quantities that are needed for the per-SNP updates:\n        mu_mult = (self.n_per_snp[c]/(self.var_tau[c]*self.sigma_epsilon)).astype(self.float_precision)\n        u_logs = (np.log(pi) - np.log(1. - pi) + .5*(np.log(tau_beta) -\n                                                     self._log_var_tau[c])).astype(self.float_precision)\n\n        cpp_e_step(self.ld_left_bound[c],\n                   self.ld_indptr[c],\n                   self.ld_data[c],\n                   self.std_beta[c],\n                   self.var_gamma[c],\n                   self.var_mu[c],\n                   self.eta[c],\n                   self.q[c],\n                   self.eta_diff[c],\n                   u_logs,\n                   np.sqrt(0.5*self.var_tau[c]).astype(self.float_precision),\n                   mu_mult,\n                   self.dequantize_scale,\n                   self.threads,\n                   self.low_memory)\n\n    self.zeta = self.compute_zeta()\n</code></pre>"},{"location":"api/model/VIPRS/#viprs.model.VIPRS.VIPRS.elbo","title":"<code>elbo(sum_axis=None)</code>","text":"<p>Compute the variational objective, the Evidence Lower-BOund (ELBO), from GWAS summary statistics and the reference LD data. This implementation assumes that the product of the LD matrix with the current estimate of the effect sizes is already computed and stored in the <code>q</code> dictionary. If this is not the case, we recommend computing q first and then calling this method.</p> <p>Parameters:</p> Name Type Description Default <code>sum_axis</code> <p>The axis along which to sum the ELBO. If None, the ELBO is returned as a scalar.</p> <code>None</code> <p>Returns:</p> Type Description <p>The ELBO of the model.</p> Source code in <code>viprs/model/VIPRS.py</code> <pre><code>def elbo(self, sum_axis=None):\n    \"\"\"\n    Compute the variational objective, the Evidence Lower-BOund (ELBO),\n    from GWAS summary statistics and the reference LD data. This implementation assumes\n    that the product of the LD matrix with the current estimate of the effect sizes\n    is already computed and stored in the `q` dictionary. If this is not the case,\n    we recommend computing q first and then calling this method.\n\n    :param sum_axis: The axis along which to sum the ELBO. If None, the ELBO is returned as a scalar.\n    :return: The ELBO of the model.\n    \"\"\"\n\n    double_resolution = np.finfo(np.float64).resolution\n\n    # Concatenate the dictionary items for easy computation:\n    var_gamma = np.clip(dict_concat(self.var_gamma).astype(np.float64),\n                        a_min=double_resolution,\n                        a_max=1. - double_resolution)\n    # The gamma for the null component\n    null_gamma = np.clip(1. - dict_concat(self.compute_pip()).astype(np.float64),\n                         a_min=double_resolution,\n                         a_max=1. - double_resolution)\n    log_var_tau = dict_concat(self._log_var_tau)\n\n    if isinstance(self.pi, dict):\n        pi = dict_concat(self.pi)\n        null_pi = dict_concat(self.get_null_pi())\n    else:\n        pi = self.pi\n        null_pi = self.get_null_pi()\n\n    if isinstance(self.tau_beta, dict):\n        tau_beta = dict_concat(self.tau_beta).astype(np.float64)\n    else:\n        tau_beta = self.tau_beta\n\n    zeta = dict_concat(self.zeta).astype(np.float64)\n\n    # Initialize the ELBO:\n    elbo = 0.\n\n    # -----------------------------------------------\n    # (1) Compute the log of the joint density:\n\n    #\n    # (1.1) The following terms are an expansion of ||Y - X\\beta||^2\n    #\n    # -N/2log(2pi*sigma_epsilon)\n    elbo -= np.log(2 * np.pi * self.sigma_epsilon)\n\n    # -Y'Y/(2*sigma_epsilon), where we assume Y'Y = N\n    # + (1./sigma_epsilon)*\\beta*(XY), where we assume XY = N\\hat{\\beta}\n    if 'sigma_epsilon' not in self.fix_params:\n        # If sigma_epsilon was updated in the M-Step, then this expression would\n        # simply evaluate to 1. and there's no point in re-computing it again:\n        elbo -= 1.\n    else:\n\n        eta = dict_concat(self.eta).astype(np.float64)\n        std_beta = dict_concat(self.std_beta).astype(np.float64)\n\n        elbo -= (1. / self.sigma_epsilon) * (1. - 2.*std_beta.dot(eta) + self._sigma_g)\n\n    elbo *= 0.5*self.n\n\n    elbo -= np.multiply(var_gamma, np.log(var_gamma) - np.log(pi)).sum(axis=sum_axis)\n    elbo -= np.multiply(null_gamma, np.log(null_gamma) - np.log(null_pi)).sum(axis=sum_axis)\n\n    elbo += .5 * np.multiply(var_gamma, 1. - log_var_tau + np.log(tau_beta)).sum(axis=sum_axis)\n\n    if np.isscalar(tau_beta) or len(zeta.shape) &gt; 1:\n        elbo -= .5*(tau_beta*zeta).sum(axis=sum_axis)\n    else:\n        var_mu = dict_concat(self.var_mu)\n        var_tau = dict_concat(self.var_tau)\n\n        elbo -= .5*(np.multiply(var_gamma, tau_beta) * (var_mu**2 + 1./var_tau)).sum(axis=sum_axis)\n\n    try:\n        if len(elbo) == 1:\n            return elbo[0]\n        else:\n            return elbo\n    except TypeError:\n        return elbo\n</code></pre>"},{"location":"api/model/VIPRS/#viprs.model.VIPRS.VIPRS.entropy","title":"<code>entropy(sum_axis=None)</code>","text":"<p>Compute the entropy of the variational distribution given the current parameter values.</p> <p>Parameters:</p> Name Type Description Default <code>sum_axis</code> <p>The axis along which to sum the ELBO. If None, the ELBO is returned as a scalar.</p> <code>None</code> <p>Returns:</p> Type Description <p>The entropy of the variational distribution.</p> Source code in <code>viprs/model/VIPRS.py</code> <pre><code>def entropy(self, sum_axis=None):\n    \"\"\"\n    Compute the entropy of the variational distribution given the current parameter values.\n\n    :param sum_axis: The axis along which to sum the ELBO. If None, the ELBO is returned as a scalar.\n    :return: The entropy of the variational distribution.\n    \"\"\"\n\n    double_resolution = np.finfo(np.float64).resolution\n\n    # Concatenate the dictionary items for easy computation:\n    var_gamma = np.clip(dict_concat(self.var_gamma),\n                        a_min=double_resolution,\n                        a_max=1. - double_resolution)\n    # The gamma for the null component\n    null_gamma = np.clip(1. - dict_concat(self.compute_pip()),\n                         a_min=double_resolution,\n                         a_max=1. - double_resolution)\n\n    log_var_tau = dict_concat(self._log_var_tau)\n\n    entropy = 0.\n\n    # Bernoulli entropy terms:\n    entropy -= np.multiply(var_gamma, np.log(var_gamma)).sum(axis=sum_axis)\n    entropy -= np.multiply(null_gamma, np.log(null_gamma)).sum(axis=sum_axis)\n    # Gaussian entropy terms:\n    entropy -= .5 * np.multiply(var_gamma, log_var_tau).sum(axis=sum_axis)\n\n    return .5 * self.n_snps * (np.log(2. * np.pi) + 1.) + entropy\n</code></pre>"},{"location":"api/model/VIPRS/#viprs.model.VIPRS.VIPRS.fit","title":"<code>fit(max_iter=1000, theta_0=None, param_0=None, continued=False, disable_pbar=False, min_iter=3, f_abs_tol=1e-06, x_abs_tol=1e-06, patience=10, **kwargs)</code>","text":"<p>A convenience method to fit the model using the Variational EM algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>max_iter</code> <p>Maximum number of iterations.</p> <code>1000</code> <code>theta_0</code> <p>A dictionary of values to initialize the hyperparameters</p> <code>None</code> <code>param_0</code> <p>A dictionary of values to initialize the variational parameters</p> <code>None</code> <code>continued</code> <p>If true, continue the model fitting for more iterations from current parameters instead of starting over.</p> <code>False</code> <code>disable_pbar</code> <p>If True, disable the progress bar.</p> <code>False</code> <code>min_iter</code> <p>The minimum number of iterations to run before checking for convergence.</p> <code>3</code> <code>f_abs_tol</code> <p>The absolute tolerance threshold for the objective (ELBO).</p> <code>1e-06</code> <code>x_abs_tol</code> <p>The absolute tolerance threshold for the variational parameters.</p> <code>1e-06</code> <code>patience</code> <p>The maximum number of consecutive iterations with no improvement in the ELBO or change in model parameters.</p> <code>10</code> <code>kwargs</code> <p>Additional keyword arguments to pass to the optimization routine.</p> <code>{}</code> <p>Returns:</p> Type Description <p>The VIPRS object with the fitted model parameters.</p> Source code in <code>viprs/model/VIPRS.py</code> <pre><code>def fit(self,\n        max_iter=1000,\n        theta_0=None,\n        param_0=None,\n        continued=False,\n        disable_pbar=False,\n        min_iter=3,\n        f_abs_tol=1e-6,\n        x_abs_tol=1e-6,\n        patience=10,\n        **kwargs):\n    \"\"\"\n    A convenience method to fit the model using the Variational EM algorithm.\n\n    :param max_iter: Maximum number of iterations. \n    :param theta_0: A dictionary of values to initialize the hyperparameters\n    :param param_0: A dictionary of values to initialize the variational parameters\n    :param continued: If true, continue the model fitting for more iterations from current parameters\n    instead of starting over.\n    :param disable_pbar: If True, disable the progress bar.\n    :param min_iter: The minimum number of iterations to run before checking for convergence.\n    :param f_abs_tol: The absolute tolerance threshold for the objective (ELBO).\n    :param x_abs_tol: The absolute tolerance threshold for the variational parameters.\n    :param patience: The maximum number of consecutive iterations with no improvement in the ELBO or change\n    in model parameters.\n    :param kwargs: Additional keyword arguments to pass to the optimization routine.\n\n    :return: The VIPRS object with the fitted model parameters.\n    \"\"\"\n\n    if not continued:\n        self.initialize(theta_0, param_0)\n        start_idx = 1\n        self.update_theta_history()\n    else:\n        start_idx = len(self.history['ELBO']) + 1\n        # Update OptimizeResult object to enable continuation of the optimization:\n        self.optim_result.update(self.elbo(), increment=False)\n\n    logger.info(\"&gt; Performing model fit...\")\n    if self.threads &gt; 1:\n        logger.info(f\"&gt; Using up to {self.threads} threads.\")\n\n    # If the model is fit over a single chromosome, append this information to the\n    # tqdm progress bar:\n    if len(self.shapes) == 1:\n        desc = f\"Chromosome {self.chromosomes[0]} ({self.n_snps} variants)\"\n    else:\n        desc = None\n\n    if continued:\n        prev_elbo = self.elbo()\n    else:\n        prev_elbo = -np.inf\n\n    # The following is used to track LD-weighted effect sizes.\n    # This is useful for tracking oscillations in ultra high-dimensions due to high LD.\n    prev_sigma_g = self._sigma_g\n    sigma_g_icc = IterationConditionCounter()\n    divergence_icc = IterationConditionCounter()\n\n    # -------------------------- Main optimization loop (EM Algorithm) --------------------------\n\n    with (logging_redirect_tqdm(loggers=[logger])):\n\n        # Progress bar:\n        pbar = tqdm(range(start_idx, start_idx + max_iter),\n                    disable=disable_pbar,\n                    desc=desc)\n\n        for i in pbar:\n\n            if self.optim_result.stop_iteration:\n                pbar.set_postfix({'Final ELBO': f\"{self.optim_result.objective:.4f}\"})\n                pbar.n = i - 1\n                pbar.total = i - 1\n                pbar.refresh()\n                pbar.close()\n                break\n\n            # Perform parameter updates (E-Step + M-Step):\n            self.e_step()\n            self.m_step()\n\n            # Update the tracked parameters (including objectives):\n            self.update_theta_history()\n\n            # Compute maximum absolute difference in effect sizes:\n            max_eta_diff = max([np.max(np.abs(diff)) for diff in self.eta_diff.values()])\n\n            # Update the current ELBO:\n            curr_elbo = self.history['ELBO'][-1]\n\n            # Update the sigma_g condition counter:\n            sigma_g_icc.update(\n                (i &gt; min_iter) and\n                np.isclose(self._sigma_g, prev_sigma_g, atol=x_abs_tol, rtol=0.) and\n                max_eta_diff &lt; x_abs_tol * 10,\n                i\n            )\n\n            # Update the ELBO drop condition counter:\n            # TODO: Find other ways to determine if the optimization is diverging.\n            divergence_icc.update(\n                (curr_elbo &lt; prev_elbo) and not\n                np.isclose(curr_elbo, prev_elbo, atol=1e3*f_abs_tol, rtol=1e-4),\n                i\n            )\n\n            # Update the progress bar:\n            pbar.set_postfix({'ELBO': f\"{curr_elbo:.4f}\"})\n\n            # --------------------------------------------------------------------------------------\n            # Sanity checking / convergence criteria:\n\n            # Check if the objective / model parameters behave in unexpected/pathological ways:\n            if self.mse() &lt; 0.:\n\n                if 'sigma_epsilon' not in self.fix_params:\n\n                    logger.info(f\"Iteration {i} | MSE is negative; Restarting optimization \"\n                                f\"and fixing residual variance hyperparameter (sigma_epsilon).\")\n\n                    self.initialize_theta(theta_0)\n                    self.initialize_variational_parameters(param_0)\n\n                    # Set the residual variance to a fixed value for now:\n                    self.fix_params['sigma_epsilon'] = self.sigma_epsilon = .95\n\n                    continue\n\n                else:\n                    self.optim_result.update(curr_elbo,\n                                             stop_iteration=True,\n                                             success=False,\n                                             message=f'The MSE is negative ({self.mse():.6f}).')\n\n            elif not np.isfinite(curr_elbo):\n                self.optim_result.update(curr_elbo,\n                                         stop_iteration=True,\n                                         success=False,\n                                         message='Objective (ELBO) is undefined.')\n            elif self.sigma_epsilon &lt; 0.:\n                self.optim_result.update(curr_elbo,\n                                         stop_iteration=True,\n                                         success=False,\n                                         message='Residual variance estimate is negative.')\n            elif self.threads &gt; 1 and self.optim_result.oscillation_counter &gt; 5:\n\n                logger.info(f\"Iteration {i} | Reducing the number of \"\n                            f\"threads for better parameter synchronization.\")\n                self.threads -= 1\n                self.optim_result._reset_oscillation_counter()\n\n            elif self.get_heritability() &gt; 1. or self.get_heritability() &lt; 0.:\n\n                self.optim_result.update(curr_elbo,\n                                         stop_iteration=True,\n                                         success=False,\n                                         message='Estimated heritability is out of bounds.')\n\n            # Check for convergence in the objective + parameters:\n            elif (i &gt; min_iter) and np.isclose(prev_elbo, curr_elbo, atol=f_abs_tol, rtol=0.):\n                self.optim_result.update(curr_elbo,\n                                         stop_iteration=True,\n                                         success=True,\n                                         message='Objective (ELBO) converged successfully.')\n\n            elif (i &gt; min_iter) and max_eta_diff &lt; x_abs_tol:\n                self.optim_result.update(curr_elbo,\n                                         stop_iteration=True,\n                                         success=True,\n                                         message='Variational parameters converged successfully.')\n            # Check for convergence based on the LD-weighted effect sizes:\n            elif sigma_g_icc.counter &gt; patience:\n                self.optim_result.update(curr_elbo,\n                                         stop_iteration=True,\n                                         success=True,\n                                         message='LD-weighted variational parameters converged successfully.')\n            # Check if the ELBO has been consistently dropping:\n            elif divergence_icc.counter &gt; patience:\n\n                self.optim_result.update(curr_elbo,\n                                         stop_iteration=True,\n                                         success=False,\n                                         message='The objective (ELBO) is decreasing.')\n\n            else:\n                self.optim_result.update(curr_elbo)\n\n            prev_elbo = curr_elbo\n            prev_sigma_g = self._sigma_g\n\n    # -------------------------- Post processing / cleaning up / model checking --------------------------\n\n    # Update the posterior moments:\n    self.update_posterior_moments()\n\n    # Inspect the optim result:\n    if not self.optim_result.stop_iteration:\n        self.optim_result.update(self.elbo(),\n                                 stop_iteration=True,\n                                 success=False,\n                                 message=\"Maximum iterations reached without convergence.\\n\"\n                                         \"You may need to run the model for more iterations.\",\n                                 increment=False)\n\n    # Inform the user about potential issues:\n    if not self.optim_result.success:\n        logger.warning(\"\\t\" + self.optim_result.message)\n\n    logger.info(f\"&gt; Final ELBO: {self.history['ELBO'][-1]:.6f}\")\n    logger.info(f\"&gt; Estimated heritability: {self.get_heritability():.6f}\")\n    logger.info(f\"&gt; Estimated proportion of causal variants: {self.get_proportion_causal():.6f}\")\n\n    return self\n</code></pre>"},{"location":"api/model/VIPRS/#viprs.model.VIPRS.VIPRS.get_average_effect_size_variance","title":"<code>get_average_effect_size_variance()</code>","text":"<p>Returns:</p> Type Description <p>The average per-SNP variance for the prior mixture components</p> Source code in <code>viprs/model/VIPRS.py</code> <pre><code>def get_average_effect_size_variance(self):\n    \"\"\"\n    :return: The average per-SNP variance for the prior mixture components\n    \"\"\"\n    if isinstance(self.pi, dict):\n        pi = dict_concat(self.pi, axis=0)\n    else:\n        pi = self.pi\n\n    if isinstance(self.tau_beta, dict):\n        tau_beta = dict_concat(self.tau_beta, axis=0)\n    else:\n        tau_beta = self.tau_beta\n\n    return np.sum(pi / tau_beta, axis=0)\n</code></pre>"},{"location":"api/model/VIPRS/#viprs.model.VIPRS.VIPRS.get_heritability","title":"<code>get_heritability()</code>","text":"<p>Returns:</p> Type Description <p>An estimate of the SNP heritability, or proportion of variance explained by SNPs.</p> Source code in <code>viprs/model/VIPRS.py</code> <pre><code>def get_heritability(self):\n    \"\"\"\n    :return: An estimate of the SNP heritability, or proportion of variance explained by SNPs.\n    \"\"\"\n\n    return self._sigma_g / (self._sigma_g + self.sigma_epsilon)\n</code></pre>"},{"location":"api/model/VIPRS/#viprs.model.VIPRS.VIPRS.get_null_pi","title":"<code>get_null_pi(chrom=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>chrom</code> <p>If provided, get the mixing proportion for the null component on a given chromosome.</p> <code>None</code> <p>Returns:</p> Type Description <p>The value of the prior probability of a variant being null, <code>1 - pi</code>.</p> Source code in <code>viprs/model/VIPRS.py</code> <pre><code>def get_null_pi(self, chrom=None):\n    \"\"\"\n    :param chrom: If provided, get the mixing proportion for the null component on a given chromosome.\n\n    :return: The value of the prior probability of a variant being null, `1 - pi`.\n    \"\"\"\n\n    pi = self.get_pi(chrom=chrom)\n\n    if isinstance(pi, dict):\n        return {c: 1. - c_pi for c, c_pi in pi.items()}\n    else:\n        return 1. - pi\n</code></pre>"},{"location":"api/model/VIPRS/#viprs.model.VIPRS.VIPRS.get_pi","title":"<code>get_pi(chrom=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>chrom</code> <p>Get the value of <code>pi</code> for a given chromosome.</p> <code>None</code> <p>Returns:</p> Type Description <p>The value of the prior probability of a variant being causal, <code>pi</code>.</p> Source code in <code>viprs/model/VIPRS.py</code> <pre><code>def get_pi(self, chrom=None):\n    \"\"\"\n    :param chrom: Get the value of `pi` for a given chromosome.\n\n    :return: The value of the prior probability of a variant being causal, `pi`.\n    \"\"\"\n\n    if chrom is None:\n        return self.pi\n    else:\n        if isinstance(self.pi, dict):\n            return self.pi[chrom]\n        else:\n            return self.pi\n</code></pre>"},{"location":"api/model/VIPRS/#viprs.model.VIPRS.VIPRS.get_proportion_causal","title":"<code>get_proportion_causal()</code>","text":"<p>Returns:</p> Type Description <p>The proportion of causal variants in the model.</p> Source code in <code>viprs/model/VIPRS.py</code> <pre><code>def get_proportion_causal(self):\n    \"\"\"\n    :return: The proportion of causal variants in the model.\n    \"\"\"\n    if isinstance(self.pi, dict):\n        return dict_mean(self.pi, axis=0)\n    else:\n        return self.pi\n</code></pre>"},{"location":"api/model/VIPRS/#viprs.model.VIPRS.VIPRS.get_sigma_epsilon","title":"<code>get_sigma_epsilon()</code>","text":"<p>Returns:</p> Type Description <p>The value of the residual variance, <code>sigma_epsilon</code>.</p> Source code in <code>viprs/model/VIPRS.py</code> <pre><code>def get_sigma_epsilon(self):\n    \"\"\"\n    :return: The value of the residual variance, `sigma_epsilon`.\n    \"\"\"\n    return self.sigma_epsilon\n</code></pre>"},{"location":"api/model/VIPRS/#viprs.model.VIPRS.VIPRS.get_tau_beta","title":"<code>get_tau_beta(chrom=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>chrom</code> <p>Get the value of <code>tau_beta</code> for a given chromosome.</p> <code>None</code> <p>Returns:</p> Type Description <p>The value of the prior precision on the effect size(s), <code>tau_beta</code></p> Source code in <code>viprs/model/VIPRS.py</code> <pre><code>def get_tau_beta(self, chrom=None):\n    \"\"\"\n    :param chrom: Get the value of `tau_beta` for a given chromosome.\n\n    :return: The value of the prior precision on the effect size(s), `tau_beta`\n    \"\"\"\n    if chrom is None:\n        return self.tau_beta\n    else:\n        if isinstance(self.tau_beta, dict):\n            return self.tau_beta[chrom]\n        else:\n            return self.tau_beta\n</code></pre>"},{"location":"api/model/VIPRS/#viprs.model.VIPRS.VIPRS.init_optim_meta","title":"<code>init_optim_meta()</code>","text":"<p>Initialize the various quantities/objects to keep track of the optimization process.  This method initializes the \"history\" object (which keeps track of the objective + other  hyperparameters requested by the user), in addition to the OptimizeResult objects.</p> Source code in <code>viprs/model/VIPRS.py</code> <pre><code>def init_optim_meta(self):\n    \"\"\"\n    Initialize the various quantities/objects to keep track of the optimization process.\n     This method initializes the \"history\" object (which keeps track of the objective + other\n     hyperparameters requested by the user), in addition to the OptimizeResult objects.\n    \"\"\"\n\n    self.history = {\n        'ELBO': [],\n    }\n\n    for tt in self.tracked_params:\n        if isinstance(tt, str):\n            self.history[tt] = []\n        elif callable(tt):\n            self.history[tt.__name__] = []\n\n    self.optim_result.reset()\n</code></pre>"},{"location":"api/model/VIPRS/#viprs.model.VIPRS.VIPRS.initialize","title":"<code>initialize(theta_0=None, param_0=None)</code>","text":"<p>A convenience method to initialize all the objects associated with the model.</p> <p>Parameters:</p> Name Type Description Default <code>theta_0</code> <p>A dictionary of initial values for the hyperparameters theta</p> <code>None</code> <code>param_0</code> <p>A dictionary of initial values for the variational parameters</p> <code>None</code> Source code in <code>viprs/model/VIPRS.py</code> <pre><code>def initialize(self, theta_0=None, param_0=None):\n    \"\"\"\n    A convenience method to initialize all the objects associated with the model.\n    :param theta_0: A dictionary of initial values for the hyperparameters theta\n    :param param_0: A dictionary of initial values for the variational parameters\n    \"\"\"\n\n    logger.debug(\"&gt; Initializing model parameters\")\n\n    self.initialize_theta(theta_0)\n    self.initialize_variational_parameters(param_0)\n    self.init_optim_meta()\n</code></pre>"},{"location":"api/model/VIPRS/#viprs.model.VIPRS.VIPRS.initialize_theta","title":"<code>initialize_theta(theta_0=None)</code>","text":"<p>Initialize the global hyperparameters of the model.</p> <p>Parameters:</p> Name Type Description Default <code>theta_0</code> <p>A dictionary of initial values for the hyperparameters theta</p> <code>None</code> Source code in <code>viprs/model/VIPRS.py</code> <pre><code>def initialize_theta(self, theta_0=None):\n    \"\"\"\n    Initialize the global hyperparameters of the model.\n    :param theta_0: A dictionary of initial values for the hyperparameters theta\n    \"\"\"\n\n    if theta_0 is not None and self.fix_params is not None:\n        theta_0.update(self.fix_params)\n    elif self.fix_params is not None:\n        theta_0 = self.fix_params\n    elif theta_0 is None:\n        theta_0 = {}\n\n    # ----------------------------------------------\n    # (1) If 'pi' is not set, initialize from a uniform\n    if 'pi' not in theta_0:\n\n        min_pi = max(10./self.n_snps, 1e-5)\n        max_pi = min(0.2, 1e4/self.n_snps)\n\n        self.pi = np.random.uniform(low=min_pi, high=max_pi)\n    else:\n        self.pi = theta_0['pi']\n\n    # ----------------------------------------------\n    # (2) Initialize sigma_epsilon and tau_beta\n    # Assuming that the genotype and phenotype are normalized,\n    # these two quantities are conceptually linked.\n    # The initialization routine here assumes that:\n    # Var(y) = h2 + sigma_epsilon\n    # Where, by assumption, Var(y) = 1,\n    # And h2 ~= pi*M/tau_beta\n\n    if 'sigma_epsilon' not in theta_0:\n        if 'tau_beta' not in theta_0:\n\n            # If neither tau_beta nor sigma_epsilon are given,\n            # then initialize using the SNP heritability estimate\n\n            try:\n                from magenpy.stats.h2.ldsc import simple_ldsc\n                naive_h2g = np.clip(simple_ldsc(self.gdl), a_min=.01, a_max=.99)\n            except Exception as e:\n                logger.debug(e)\n                naive_h2g = np.random.uniform(low=.01, high=.1)\n\n            self.sigma_epsilon = 1. - naive_h2g\n            self.tau_beta = self.pi * self.n_snps / max(naive_h2g, 0.01)\n        else:\n\n            # If tau_beta is given, use it to initialize sigma_epsilon\n\n            self.tau_beta = theta_0['tau_beta']\n            self.sigma_epsilon = np.clip(1. - (self.pi * self.n_snps / self.tau_beta),\n                                         a_min=1e-4,\n                                         a_max=1. - 1e-4)\n    else:\n\n        # If sigma_epsilon is given, use it in the initialization\n\n        self.sigma_epsilon = theta_0['sigma_epsilon']\n\n        if 'tau_beta' in theta_0:\n            self.tau_beta = theta_0['tau_beta']\n        else:\n            self.tau_beta = (self.pi * self.n_snps) / np.maximum(0.01, 1. - self.sigma_epsilon)\n\n    # Cast all the hyperparameters to conform to the precision set by the user:\n    self.sigma_epsilon = np.dtype(self.float_precision).type(self.sigma_epsilon)\n    self.pi = np.dtype(self.float_precision).type(self.pi)\n    self.lambda_min = np.dtype(self.float_precision).type(self.lambda_min)\n    self._sigma_g = np.dtype(self.float_precision).type(0.)\n</code></pre>"},{"location":"api/model/VIPRS/#viprs.model.VIPRS.VIPRS.initialize_variational_parameters","title":"<code>initialize_variational_parameters(param_0=None)</code>","text":"<p>Initialize the variational parameters of the model.</p> <p>Parameters:</p> Name Type Description Default <code>param_0</code> <p>A dictionary of initial values for the variational parameters</p> <code>None</code> Source code in <code>viprs/model/VIPRS.py</code> <pre><code>def initialize_variational_parameters(self, param_0=None):\n    \"\"\"\n    Initialize the variational parameters of the model.\n    :param param_0: A dictionary of initial values for the variational parameters\n    \"\"\"\n\n    param_0 = param_0 or {}\n\n    self.var_mu = {}\n    self.var_tau = {}\n    self.var_gamma = {}\n\n    for c, shapes in self.shapes.items():\n\n        # Initialize the variational parameters according to the derived update equations,\n        # ignoring correlations between SNPs.\n        if 'tau' in param_0:\n            self.var_tau[c] = param_0['tau'][c]\n        else:\n            self.var_tau[c] = (self.n_per_snp[c] / self.sigma_epsilon) + self.tau_beta\n\n        self.var_tau[c] = self.var_tau[c]\n\n        if 'mu' in param_0:\n            self.var_mu[c] = param_0['mu'][c].astype(self.float_precision, order=self.order)\n        else:\n            self.var_mu[c] = np.zeros(shapes, dtype=self.float_precision, order=self.order)\n\n        if 'gamma' in param_0:\n            self.var_gamma[c] = param_0['gamma'][c].astype(self.float_precision, order=self.order)\n        else:\n            pi = self.get_pi(c)\n            if isinstance(self.pi, dict):\n                self.var_gamma[c] = pi.astype(self.float_precision, order=self.order)\n            else:\n                self.var_gamma[c] = pi*np.ones(shapes, dtype=self.float_precision, order=self.order)\n\n    self.eta = self.compute_eta()\n    self.zeta = self.compute_zeta()\n    self.eta_diff = {c: np.zeros_like(eta, dtype=self.float_precision) for c, eta in self.eta.items()}\n    self.q = {c: np.zeros_like(eta, dtype=self.float_precision) for c, eta in self.eta.items()}\n    self._log_var_tau = {c: np.log(self.var_tau[c]) for c in self.var_tau}\n</code></pre>"},{"location":"api/model/VIPRS/#viprs.model.VIPRS.VIPRS.log_prior","title":"<code>log_prior(sum_axis=None)</code>","text":"<p>Compute the expectation of the log prior of the model parameters given the current hyperparameter values. The expectation is taken with respect to the variational distribution.</p> <p>Parameters:</p> Name Type Description Default <code>sum_axis</code> <p>The axis along which to sum the log prior.</p> <code>None</code> <p>Returns:</p> Type Description <p>The expectation of the log prior according to the variational density.</p> Source code in <code>viprs/model/VIPRS.py</code> <pre><code>def log_prior(self, sum_axis=None):\n    \"\"\"\n    Compute the expectation of the log prior of the model parameters given the current hyperparameter values.\n    The expectation is taken with respect to the variational distribution.\n\n    :param sum_axis: The axis along which to sum the log prior.\n    :return: The expectation of the log prior according to the variational density.\n    \"\"\"\n\n    double_resolution = np.finfo(np.float64).resolution\n\n    var_gamma = np.clip(dict_concat(self.var_gamma),\n                        a_min=double_resolution,\n                        a_max=1. - double_resolution)\n    # The gamma for the null component\n    null_gamma = np.clip(1. - dict_concat(self.compute_pip()),\n                         a_min=double_resolution,\n                         a_max=1. - double_resolution)\n\n    if isinstance(self.pi, dict):\n        pi = dict_concat(self.pi)\n        null_pi = dict_concat(self.get_null_pi())\n    else:\n        pi = self.pi\n        null_pi = self.get_null_pi()\n\n    if isinstance(self.tau_beta, dict):\n        tau_beta = dict_concat(self.tau_beta)\n    else:\n        tau_beta = self.tau_beta\n\n    zeta = dict_concat(self.zeta)\n\n    log_prior = 0.\n\n    log_prior += .5*(np.multiply(var_gamma, np.log(tau_beta))).sum(axis=sum_axis)\n    log_prior += np.multiply(var_gamma, np.log(pi)).sum(axis=sum_axis)\n    log_prior += np.multiply(null_gamma, np.log(null_pi)).sum(axis=sum_axis)\n\n    if np.isscalar(tau_beta) or len(zeta.shape) &gt; 1:\n        log_prior -= (.5*tau_beta * zeta).sum(axis=sum_axis)\n    else:\n        var_mu = dict_concat(self.var_mu)\n        var_tau = dict_concat(self.var_tau)\n\n        log_prior -= .5 * (np.multiply(var_gamma, tau_beta) * (var_mu ** 2 + 1. / var_tau)).sum(axis=sum_axis)\n\n    return log_prior - .5*self.n_snps*np.log(2.*np.pi)\n</code></pre>"},{"location":"api/model/VIPRS/#viprs.model.VIPRS.VIPRS.loglikelihood","title":"<code>loglikelihood()</code>","text":"<p>Compute the expectation of the loglikelihood of the data given the current model parameter values. The expectation is taken with respect to the variational distribution.</p> <p>Returns:</p> Type Description <p>The loglikelihood of the data.</p> Source code in <code>viprs/model/VIPRS.py</code> <pre><code>def loglikelihood(self):\n    \"\"\"\n    Compute the expectation of the loglikelihood of the data given the current model parameter values.\n    The expectation is taken with respect to the variational distribution.\n\n    :return: The loglikelihood of the data.\n    \"\"\"\n\n    eta = dict_concat(self.eta)\n    std_beta = dict_concat(self.std_beta)\n\n    return -0.5*self.n*(\n            np.log(2.*np.pi*self.sigma_epsilon) +\n            (1./self.sigma_epsilon)*(1. - 2.*std_beta.dot(eta) + self._sigma_g)\n    )\n</code></pre>"},{"location":"api/model/VIPRS/#viprs.model.VIPRS.VIPRS.m_step","title":"<code>m_step()</code>","text":"<p>Run the M-Step of the Variational EM algorithm. Here, we update the hyperparameters of the model, by simply calling the update functions for each hyperparameter separately.</p> Source code in <code>viprs/model/VIPRS.py</code> <pre><code>def m_step(self):\n    \"\"\"\n    Run the M-Step of the Variational EM algorithm.\n    Here, we update the hyperparameters of the model, by simply calling\n    the update functions for each hyperparameter separately.\n\n    \"\"\"\n\n    self.update_pi()\n    self.update_tau_beta()\n    self._update_sigma_g()\n    self.update_sigma_epsilon()\n</code></pre>"},{"location":"api/model/VIPRS/#viprs.model.VIPRS.VIPRS.mse","title":"<code>mse(sum_axis=None)</code>","text":"<p>Compute a summary statistics-based estimate of the mean squared error on the training set.</p> <p>Parameters:</p> Name Type Description Default <code>sum_axis</code> <p>The axis along which to sum the MSE. If None, the MSE is returned as a scalar.</p> <code>None</code> <p>Returns:</p> Type Description <p>The mean squared error.</p> Source code in <code>viprs/model/VIPRS.py</code> <pre><code>def mse(self, sum_axis=None):\n    \"\"\"\n    Compute a summary statistics-based estimate of the mean squared error on the training set.\n\n    :param sum_axis: The axis along which to sum the MSE.\n    If None, the MSE is returned as a scalar.\n    :return: The mean squared error.\n    \"\"\"\n\n    eta = dict_concat(self.eta)\n    std_beta = dict_concat(self.std_beta)\n    zeta = dict_concat(self.zeta)\n\n    return 1. - 2.*std_beta.dot(eta) + (\n            self._sigma_g - zeta.sum(axis=sum_axis) + (eta**2).sum(axis=sum_axis)\n    )\n</code></pre>"},{"location":"api/model/VIPRS/#viprs.model.VIPRS.VIPRS.objective","title":"<code>objective()</code>","text":"<p>The optimization objective for the variational inference problem. The objective for the VIPRS method is the Evidence Lower-Bound (ELBO) in this case.</p> <p>See Also</p> <ul> <li>elbo</li> </ul> Source code in <code>viprs/model/VIPRS.py</code> <pre><code>def objective(self):\n    \"\"\"\n    The optimization objective for the variational inference problem. The objective\n    for the VIPRS method is the Evidence Lower-Bound (ELBO) in this case.\n\n    !!! seealso \"See Also\"\n        * [elbo][viprs.model.VIPRS.VIPRS.elbo]\n\n    \"\"\"\n    return self.elbo()\n</code></pre>"},{"location":"api/model/VIPRS/#viprs.model.VIPRS.VIPRS.set_fixed_params","title":"<code>set_fixed_params(fix_params)</code>","text":"<p>Set the fixed hyperparameters of the model.</p> <p>Parameters:</p> Name Type Description Default <code>fix_params</code> <p>A dictionary of hyperparameters with their fixed values.</p> required Source code in <code>viprs/model/VIPRS.py</code> <pre><code>def set_fixed_params(self, fix_params):\n    \"\"\"\n    Set the fixed hyperparameters of the model.\n    :param fix_params: A dictionary of hyperparameters with their fixed values.\n    \"\"\"\n\n    assert isinstance(fix_params, dict), \"The fixed parameters must be provided as a dictionary.\"\n\n    self.fix_params.update(fix_params)\n\n    for key, val in fix_params.items():\n        if key == 'sigma_epsilon':\n            self.sigma_epsilon = np.dtype(self.float_precision).type(val)\n        elif key == 'tau_beta':\n            self.tau_beta = np.dtype(self.float_precision).type(val)\n        elif key == 'pi':\n            self.pi = np.dtype(self.float_precision).type(val)\n        elif key == 'lambda_min':\n            self.lambda_min = np.dtype(self.float_precision).type(val)\n</code></pre>"},{"location":"api/model/VIPRS/#viprs.model.VIPRS.VIPRS.to_history_table","title":"<code>to_history_table()</code>","text":"<p>Returns:</p> Type Description <p>A <code>pandas</code> DataFrame containing the history of tracked parameters as a function of the number of iterations.</p> Source code in <code>viprs/model/VIPRS.py</code> <pre><code>def to_history_table(self):\n    \"\"\"\n    :return: A `pandas` DataFrame containing the history of tracked parameters as a function of\n    the number of iterations.\n    \"\"\"\n    return pd.DataFrame(self.history)\n</code></pre>"},{"location":"api/model/VIPRS/#viprs.model.VIPRS.VIPRS.to_theta_table","title":"<code>to_theta_table()</code>","text":"<p>Returns:</p> Type Description <p>A <code>pandas</code> DataFrame containing information about the estimated hyperparameters of the model.</p> Source code in <code>viprs/model/VIPRS.py</code> <pre><code>def to_theta_table(self):\n    \"\"\"\n    :return: A `pandas` DataFrame containing information about the estimated hyperparameters of the model.\n    \"\"\"\n\n    theta_table = [\n        {'Parameter': 'ELBO', 'Value': self.elbo()},\n        {'Parameter': 'Residual_variance', 'Value': self.sigma_epsilon},\n        {'Parameter': 'Heritability', 'Value': self.get_heritability()},\n        {'Parameter': 'Proportion_causal', 'Value': self.get_proportion_causal()},\n        {'Parameter': 'Average_effect_variance', 'Value': self.get_average_effect_size_variance()},\n    ]\n\n    if np.isscalar(self.lambda_min):\n        theta_table += [\n            {'Parameter': 'Lambda_min', 'Value': self.lambda_min}\n        ]\n\n    if isinstance(self.tau_beta, dict):\n        taus = dict_mean(self.tau_beta, axis=0)\n    else:\n        taus = self.tau_beta\n\n    try:\n        taus = list(taus)\n        for i in range(len(taus)):\n            theta_table.append({'Parameter': f'tau_beta_{i+1}', 'Value': taus[i]})\n    except TypeError:\n        theta_table.append({'Parameter': 'tau_beta', 'Value': taus})\n\n    return pd.DataFrame(theta_table)\n</code></pre>"},{"location":"api/model/VIPRS/#viprs.model.VIPRS.VIPRS.update_pi","title":"<code>update_pi()</code>","text":"<p>Update the prior probability of a variant being causal, or the proportion of causal variants, <code>pi</code>.</p> Source code in <code>viprs/model/VIPRS.py</code> <pre><code>def update_pi(self):\n    \"\"\"\n    Update the prior probability of a variant being causal, or the proportion of causal variants, `pi`.\n    \"\"\"\n\n    if 'pi' not in self.fix_params:\n\n        # Get the average of the gammas:\n        self.pi = dict_mean(self.var_gamma, axis=0)\n</code></pre>"},{"location":"api/model/VIPRS/#viprs.model.VIPRS.VIPRS.update_posterior_moments","title":"<code>update_posterior_moments()</code>","text":"<p>A convenience method to update the dictionaries containing the posterior moments, including the PIP and posterior mean and variance for the effect size.</p> Source code in <code>viprs/model/VIPRS.py</code> <pre><code>def update_posterior_moments(self):\n    \"\"\"\n    A convenience method to update the dictionaries containing the posterior moments,\n    including the PIP and posterior mean and variance for the effect size.\n    \"\"\"\n\n    self.pip = self.compute_pip()\n    self.post_mean_beta = {c: eta.copy() for c, eta in self.eta.items()}\n    self.post_var_beta = {c: zeta - self.eta[c]**2 for c, zeta in self.zeta.items()}\n</code></pre>"},{"location":"api/model/VIPRS/#viprs.model.VIPRS.VIPRS.update_sigma_epsilon","title":"<code>update_sigma_epsilon()</code>","text":"<p>Update the global residual variance parameter, <code>sigma_epsilon</code>.</p> Source code in <code>viprs/model/VIPRS.py</code> <pre><code>def update_sigma_epsilon(self):\n    \"\"\"\n    Update the global residual variance parameter, `sigma_epsilon`.\n    \"\"\"\n\n    if 'sigma_epsilon' not in self.fix_params:\n\n        sig_eps = 0.\n\n        for c, _ in self.shapes.items():\n            sig_eps -= 2.*self.std_beta[c].dot(self.eta[c])\n\n        self.sigma_epsilon = 1. + sig_eps + self._sigma_g\n</code></pre>"},{"location":"api/model/VIPRS/#viprs.model.VIPRS.VIPRS.update_tau_beta","title":"<code>update_tau_beta()</code>","text":"<p>Update the prior precision (inverse variance) for the effect size, <code>tau_beta</code>.</p> Source code in <code>viprs/model/VIPRS.py</code> <pre><code>def update_tau_beta(self):\n    \"\"\"\n    Update the prior precision (inverse variance) for the effect size, `tau_beta`.\n    \"\"\"\n\n    if 'tau_beta' not in self.fix_params:\n\n        # tau_beta estimate:\n        self.tau_beta = (self.pi * self.n_snps / dict_sum(self.zeta, axis=0))\n</code></pre>"},{"location":"api/model/VIPRS/#viprs.model.VIPRS.VIPRS.update_theta_history","title":"<code>update_theta_history()</code>","text":"<p>A convenience method to update the history of the hyperparameters/objectives/other summary statistics of the model, if the user requested that they should be tracked.</p> Source code in <code>viprs/model/VIPRS.py</code> <pre><code>def update_theta_history(self):\n    \"\"\"\n    A convenience method to update the history of the hyperparameters/objectives/other summary statistics\n    of the model, if the user requested that they should be tracked.\n    \"\"\"\n\n    self.history['ELBO'].append(self.elbo())\n\n    for tt in self.tracked_params:\n        if tt == 'pi':\n            self.history['pi'].append(self.get_proportion_causal())\n        elif tt == 'pis':\n            self.history['pis'].append(self.pi)\n        if tt == 'heritability':\n            self.history['heritability'].append(self.get_heritability())\n        if tt == 'sigma_epsilon':\n            self.history['sigma_epsilon'].append(self.sigma_epsilon)\n        elif tt == 'tau_beta':\n            self.history['tau_beta'].append(self.tau_beta)\n        elif tt == 'sigma_g':\n            self.history['sigma_g'].append(self._sigma_g)\n        elif tt == 'entropy':\n            self.history['entropy'].append(self.entropy())\n        elif tt == 'loglikelihood':\n            self.history['loglikelihood'].append(self.loglikelihood())\n        elif tt == 'log_prior':\n            self.history['log_prior'].append(self.log_prior())\n        elif tt == 'mse':\n            self.history['mse'].append(self.mse())\n        elif tt == 'max_eta_diff':\n            self.history['max_eta_diff'].append(np.max([\n                np.max(np.abs(diff)) for diff in self.eta_diff.values()\n            ]))\n        elif callable(tt):\n            self.history[tt.__name__].append(tt(self))\n</code></pre>"},{"location":"api/model/VIPRS/#viprs.model.VIPRS.VIPRS.write_inferred_theta","title":"<code>write_inferred_theta(f_name, sep='\\t')</code>","text":"<p>A convenience method to write the inferred (and fixed) hyperparameters of the model to file.</p> <p>Parameters:</p> Name Type Description Default <code>f_name</code> <p>The file name</p> required <code>sep</code> <p>The separator for the hyperparameter file.</p> <code>'\\t'</code> Source code in <code>viprs/model/VIPRS.py</code> <pre><code>def write_inferred_theta(self, f_name, sep=\"\\t\"):\n    \"\"\"\n    A convenience method to write the inferred (and fixed) hyperparameters of the model to file.\n    :param f_name: The file name\n    :param sep: The separator for the hyperparameter file.\n    \"\"\"\n\n    # Write the table to file:\n    try:\n        self.to_theta_table().to_csv(f_name, sep=sep, index=False)\n    except Exception as e:\n        raise e\n</code></pre>"},{"location":"api/model/VIPRSMix/","title":"VIPRSMix","text":""},{"location":"api/model/VIPRSMix/#viprs.model.VIPRSMix.VIPRSMix","title":"<code>VIPRSMix</code>","text":"<p>               Bases: <code>VIPRS</code></p> <p>A class for the Variational Inference for Polygenic Risk Scores (VIPRS) model parametrized with the sparse mixture prior on the effect sizes. The class inherits many of the methods and attributes from the <code>VIPRS</code> class unchanged. However, there are many important updates and changes to the model, including the dimensionality of the arrays representing the variational parameters.</p> <p>Details for the algorithm can be found in the Supplementary Material of the following paper:</p> <p>Zabad S, Gravel S, Li Y. Fast and accurate Bayesian polygenic risk modeling with variational inference. Am J Hum Genet. 2023 May 4;110(5):741-761. doi: 10.1016/j.ajhg.2023.03.009. Epub 2023 Apr 7. PMID: 37030289; PMCID: PMC10183379.</p> <p>Attributes:</p> Name Type Description <code>K</code> <p>The number of causal (i.e. non-null) components in the mixture prior (minimum 1). When <code>K=1</code>, this effectively reduces <code>VIPRSMix</code> to the <code>VIPRS</code> model.</p> <code>d</code> <p>Multiplier for the prior on the effect size (vector of size K).</p> Source code in <code>viprs/model/VIPRSMix.py</code> <pre><code>class VIPRSMix(VIPRS):\n    \"\"\"\n    A class for the Variational Inference for Polygenic Risk Scores (VIPRS) model\n    parametrized with the sparse mixture prior on the effect sizes. The class inherits\n    many of the methods and attributes from the `VIPRS` class unchanged. However,\n    there are many important updates and changes to the model, including the dimensionality\n    of the arrays representing the variational parameters.\n\n    Details for the algorithm can be found in the Supplementary Material of the following paper:\n\n    &gt; Zabad S, Gravel S, Li Y. Fast and accurate Bayesian polygenic risk modeling with variational inference.\n    Am J Hum Genet. 2023 May 4;110(5):741-761. doi: 10.1016/j.ajhg.2023.03.009.\n    Epub 2023 Apr 7. PMID: 37030289; PMCID: PMC10183379.\n\n    :ivar K: The number of causal (i.e. non-null) components in the mixture prior (minimum 1). When `K=1`, this\n    effectively reduces `VIPRSMix` to the `VIPRS` model.\n    :ivar d: Multiplier for the prior on the effect size (vector of size K).\n\n    \"\"\"\n\n    def __init__(self,\n                 gdl,\n                 K=1,\n                 prior_multipliers=None,\n                 **kwargs):\n\n        \"\"\"\n        :param gdl: An instance of `GWADataLoader`\n        :param K: The number of causal (i.e. non-null) components in the mixture prior (minimum 1). When `K=1`, this\n            effectively reduces `VIPRSMix` to the `VIPRS` model.\n        :param prior_multipliers: Multiplier for the prior on the effect size (vector of size K).\n        :param kwargs: Additional keyword arguments to pass to the VIPRS model.\n        \"\"\"\n\n        # Make sure that the matrices follow the C-contiguous order:\n        kwargs['order'] = 'C'\n\n        super().__init__(gdl, **kwargs)\n\n        # Sanity checks:\n        assert K &gt; 0  # Check that there is at least 1 causal component\n        self.K = K\n\n        if prior_multipliers is not None:\n            assert len(prior_multipliers) == K\n            self.d = np.array(prior_multipliers).astype(self.float_precision)\n        else:\n            self.d = 2**np.linspace(-min(K - 1, 7), 0, K).astype(self.float_precision)\n\n        # Populate/update relevant fields:\n        self.shapes = {c: (shp, self.K) for c, shp in self.shapes.items()}\n        self.n_per_snp = {c: n[:, None].astype(self.float_precision, order=self.order)\n                          for c, n in self.n_per_snp.items()}\n\n    def initialize_theta(self, theta_0=None):\n        \"\"\"\n        Initialize the global hyperparameters of the model\n        :param theta_0: A dictionary of initial values for the hyperparameters theta\n        \"\"\"\n\n        if theta_0 is not None and self.fix_params is not None:\n            theta_0.update(self.fix_params)\n        elif self.fix_params is not None:\n            theta_0 = self.fix_params\n        elif theta_0 is None:\n            theta_0 = {}\n\n        # ----------------------------------------------\n        # (1) Initialize pi from a uniform\n        if 'pis' in theta_0:\n            self.pi = theta_0['pis']\n        else:\n            if 'pi' in theta_0:\n                overall_pi = theta_0['pi']\n            else:\n                overall_pi = np.random.uniform(low=max(0.005, 1. / self.n_snps), high=.1)\n\n            self.pi = overall_pi*np.random.dirichlet(np.ones(self.K))\n\n        # ----------------------------------------------\n        # (2) Initialize sigma_epsilon and sigma_beta\n        # Assuming that the genotype and phenotype are normalized,\n        # these two quantities are conceptually linked.\n        # The initialization routine here assumes that:\n        # Var(y) = h2 + sigma_epsilon\n        # Where, by assumption, Var(y) = 1,\n        # And h2 ~= pi*M*sigma_beta\n\n        if 'sigma_epsilon' not in theta_0:\n\n            if 'tau_betas' in theta_0:\n\n                # If tau_betas are given, use them to initialize sigma_epsilon\n\n                self.tau_beta = theta_0['tau_betas']\n\n                self.sigma_epsilon = np.clip(1. - np.dot(1./self.tau_beta, self.pi),\n                                             a_min=1e-4,\n                                             a_max=1. - 1e-4)\n\n            elif 'tau_beta' in theta_0:\n                # NOTE: Here, we assume the provided `tau_beta` is a scalar.\n                # This is different from `tau_betas`\n\n                assert self.d is not None\n\n                self.tau_beta = theta_0['tau_beta'] * self.d\n                # Use the provided tau_beta to initialize sigma_epsilon.\n                # First, we derive a naive estimate of the heritability, based on the following equation:\n                # h2g/M = \\sum_k pi_k \\tau_k\n                # Where the per-SNP heritability is defined by the sum over the mixtures.\n\n                # Step (1): Given the provided tau_beta and associated multipliers,\n                # obtain a naive estimate of the heritability:\n                h2g_estimate = (self.n_snps*self.pi/self.tau_beta).sum()\n                # Step (2): Set sigma_epsilon to 1 - h2g_estimate:\n                self.sigma_epsilon = np.clip(1. - h2g_estimate,\n                                             a_min=1e-4,\n                                             a_max=1. - 1e-4)\n\n            else:\n                # If neither sigma_beta nor sigma_epsilon are given,\n                # then initialize using the SNP heritability estimate based on summary statistics\n\n                try:\n                    naive_h2g = np.clip(simple_ldsc(self.gdl), 1e-3, 1. - 1e-3)\n                except Exception as e:\n                    naive_h2g = np.random.uniform(low=.001, high=.999)\n\n                self.sigma_epsilon = 1. - naive_h2g\n\n                global_tau = (self.n_snps * np.dot(1./self.d, self.pi) / naive_h2g)\n\n                self.tau_beta = self.d*global_tau\n        else:\n\n            # If sigma_epsilon is given, use it in the initialization\n\n            self.sigma_epsilon = theta_0['sigma_epsilon']\n\n            # Initialize tau_betas\n            if 'tau_betas' in theta_0:\n                self.tau_beta = theta_0['tau_betas']\n            elif 'tau_beta' in theta_0:\n                self.tau_beta = np.repeat(theta_0['tau_beta'], self.K)\n            else:\n                # If not provided, initialize using sigma_epsilon value\n                global_tau = (self.n_snps * np.dot(1./self.d, self.pi) / (1. - self.sigma_epsilon))\n\n                self.tau_beta = self.d * global_tau\n\n        # Cast all the hyperparameters to conform to the precision set by the user:\n        self.sigma_epsilon = np.dtype(self.float_precision).type(self.sigma_epsilon)\n        self.pi = np.dtype(self.float_precision).type(self.pi)\n        self.lambda_min = np.dtype(self.float_precision).type(self.lambda_min)\n        self._sigma_g = np.dtype(self.float_precision).type(0.)\n\n    def e_step(self):\n        \"\"\"\n        Run the E-Step of the Variational EM algorithm.\n        Here, we update the variational parameters for each variant using coordinate\n        ascent optimization techniques. The update equations are outlined in\n        the Supplementary Material of the following paper:\n\n        &gt; Zabad S, Gravel S, Li Y. Fast and accurate Bayesian polygenic risk modeling with variational inference.\n        Am J Hum Genet. 2023 May 4;110(5):741-761. doi: 10.1016/j.ajhg.2023.03.009.\n        Epub 2023 Apr 7. PMID: 37030289; PMCID: PMC10183379.\n        \"\"\"\n\n        for c, shapes in self.shapes.items():\n\n            # Get the priors:\n            tau_beta = self.get_tau_beta(c)\n            pi = self.get_pi(c)\n\n            # Updates for tau variational parameters:\n            self.var_tau[c] = (self.n_per_snp[c]*(1. + self.lambda_min) / self.sigma_epsilon) + tau_beta\n\n            if isinstance(self.pi, dict):\n                log_null_pi = (np.log(1. - self.pi[c].sum(axis=1)))\n            else:\n                log_null_pi = np.ones_like(self.eta[c])*np.log(1. - self.pi.sum())\n\n            # Compute some quantities that are needed for the per-SNP updates:\n            mu_mult = self.n_per_snp[c] / (self.var_tau[c] * self.sigma_epsilon)\n            u_logs = np.log(pi) - np.log(1. - pi) + .5 * (np.log(tau_beta) - np.log(self.var_tau[c]))\n\n            cpp_e_step_mixture(self.ld_left_bound[c],\n                               self.ld_indptr[c],\n                               self.ld_data[c],\n                               self.std_beta[c],\n                               self.var_gamma[c],\n                               self.var_mu[c],\n                               self.eta[c],\n                               self.q[c],\n                               self.eta_diff[c],\n                               log_null_pi,\n                               u_logs,\n                               np.sqrt(0.5*self.var_tau[c]),\n                               mu_mult,\n                               self.dequantize_scale,\n                               self.threads,\n                               self.low_memory)\n\n        self.zeta = self.compute_zeta()\n\n    def update_pi(self):\n        \"\"\"\n        Update the prior mixing proportions `pi`\n        \"\"\"\n\n        if 'pis' not in self.fix_params:\n\n            pi_estimate = dict_sum(self.var_gamma, axis=0)\n\n            if 'pi' in self.fix_params:\n                # If the user provides an estimate for the total proportion of causal variants,\n                # update the pis such that the proportion of SNPs in the null component becomes 1. - pi.\n                pi_estimate = self.fix_params['pi']*pi_estimate / pi_estimate.sum()\n            else:\n                pi_estimate /= self.n_snps\n\n            # Set pi to the new estimate:\n            self.pi = pi_estimate\n\n    def update_tau_beta(self):\n        \"\"\"\n        Update the prior precision (inverse variance) for the effect sizes, `tau_beta`\n        \"\"\"\n\n        if 'tau_betas' not in self.fix_params:\n\n            # If a list of multipliers is provided,\n            # estimate the global sigma_beta and then multiply it\n            # by the per-component multiplier to get the final sigma_betas.\n\n            zetas = sum(self.compute_zeta(sum_axis=0).values())\n\n            tau_beta_estimate = np.sum(self.pi)*self.m / np.dot(self.d, zetas)\n            tau_beta_estimate = self.d*tau_beta_estimate\n\n            self.tau_beta = np.clip(tau_beta_estimate, a_min=1., a_max=None)\n\n    def get_null_pi(self, chrom=None):\n        \"\"\"\n        Get the proportion of SNPs in the null component\n        :param chrom: If provided, get the mixing proportion for the null component on a given chromosome.\n        :return: The value of the mixing proportion for the null component\n        \"\"\"\n\n        pi = self.get_pi(chrom=chrom)\n\n        if isinstance(pi, dict):\n            return {c: 1. - c_pi.sum(axis=1) for c, c_pi in pi.items()}\n        else:\n            return 1. - np.sum(pi)\n\n    def get_proportion_causal(self):\n        \"\"\"\n        :return: The proportion of variants in the non-null components.\n        \"\"\"\n        if isinstance(self.pi, dict):\n            dict_mean({c: pis.sum(axis=1) for c, pis in self.pi.items()})\n        else:\n            return np.sum(self.pi)\n\n    def get_average_effect_size_variance(self):\n        \"\"\"\n        :return: The average per-SNP variance for the prior mixture components\n        \"\"\"\n\n        avg_sigma = super().get_average_effect_size_variance()\n\n        try:\n            return avg_sigma.sum()\n        except Exception:\n            return avg_sigma\n\n    def compute_pip(self):\n        \"\"\"\n        :return: The posterior inclusion probability\n        \"\"\"\n        return {c: gamma.sum(axis=1) for c, gamma in self.var_gamma.items()}\n\n    def compute_eta(self):\n        \"\"\"\n        :return: The mean for the effect size under the variational posterior.\n        \"\"\"\n        return {c: (v * self.var_mu[c]).sum(axis=1) for c, v in self.var_gamma.items()}\n\n    def compute_zeta(self, sum_axis=1):\n        \"\"\"\n        :return: The expectation of the squared effect size under the variational posterior.\n        \"\"\"\n        return {c: (v * (self.var_mu[c] ** 2 + (1./self.var_tau[c]))).sum(axis=sum_axis)\n                for c, v in self.var_gamma.items()}\n\n    def to_theta_table(self):\n        \"\"\"\n        :return: A `pandas` DataFrame containing information about the estimated hyperparameters of the model.\n        \"\"\"\n\n        table = super().to_theta_table()\n\n        extra_theta = []\n\n        if isinstance(self.pi, dict):\n            pis = list(dict_mean(self.pi, axis=0))\n        else:\n            pis = self.pi\n\n        for i in range(self.K):\n            extra_theta.append({'Parameter': f'pi_{i + 1}', 'Value': pis[i]})\n\n        return pd.concat([table, pd.DataFrame(extra_theta)])\n</code></pre>"},{"location":"api/model/VIPRSMix/#viprs.model.VIPRSMix.VIPRSMix.__init__","title":"<code>__init__(gdl, K=1, prior_multipliers=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>gdl</code> <p>An instance of <code>GWADataLoader</code></p> required <code>K</code> <p>The number of causal (i.e. non-null) components in the mixture prior (minimum 1). When <code>K=1</code>, this effectively reduces <code>VIPRSMix</code> to the <code>VIPRS</code> model.</p> <code>1</code> <code>prior_multipliers</code> <p>Multiplier for the prior on the effect size (vector of size K).</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments to pass to the VIPRS model.</p> <code>{}</code> Source code in <code>viprs/model/VIPRSMix.py</code> <pre><code>def __init__(self,\n             gdl,\n             K=1,\n             prior_multipliers=None,\n             **kwargs):\n\n    \"\"\"\n    :param gdl: An instance of `GWADataLoader`\n    :param K: The number of causal (i.e. non-null) components in the mixture prior (minimum 1). When `K=1`, this\n        effectively reduces `VIPRSMix` to the `VIPRS` model.\n    :param prior_multipliers: Multiplier for the prior on the effect size (vector of size K).\n    :param kwargs: Additional keyword arguments to pass to the VIPRS model.\n    \"\"\"\n\n    # Make sure that the matrices follow the C-contiguous order:\n    kwargs['order'] = 'C'\n\n    super().__init__(gdl, **kwargs)\n\n    # Sanity checks:\n    assert K &gt; 0  # Check that there is at least 1 causal component\n    self.K = K\n\n    if prior_multipliers is not None:\n        assert len(prior_multipliers) == K\n        self.d = np.array(prior_multipliers).astype(self.float_precision)\n    else:\n        self.d = 2**np.linspace(-min(K - 1, 7), 0, K).astype(self.float_precision)\n\n    # Populate/update relevant fields:\n    self.shapes = {c: (shp, self.K) for c, shp in self.shapes.items()}\n    self.n_per_snp = {c: n[:, None].astype(self.float_precision, order=self.order)\n                      for c, n in self.n_per_snp.items()}\n</code></pre>"},{"location":"api/model/VIPRSMix/#viprs.model.VIPRSMix.VIPRSMix.compute_eta","title":"<code>compute_eta()</code>","text":"<p>Returns:</p> Type Description <p>The mean for the effect size under the variational posterior.</p> Source code in <code>viprs/model/VIPRSMix.py</code> <pre><code>def compute_eta(self):\n    \"\"\"\n    :return: The mean for the effect size under the variational posterior.\n    \"\"\"\n    return {c: (v * self.var_mu[c]).sum(axis=1) for c, v in self.var_gamma.items()}\n</code></pre>"},{"location":"api/model/VIPRSMix/#viprs.model.VIPRSMix.VIPRSMix.compute_pip","title":"<code>compute_pip()</code>","text":"<p>Returns:</p> Type Description <p>The posterior inclusion probability</p> Source code in <code>viprs/model/VIPRSMix.py</code> <pre><code>def compute_pip(self):\n    \"\"\"\n    :return: The posterior inclusion probability\n    \"\"\"\n    return {c: gamma.sum(axis=1) for c, gamma in self.var_gamma.items()}\n</code></pre>"},{"location":"api/model/VIPRSMix/#viprs.model.VIPRSMix.VIPRSMix.compute_zeta","title":"<code>compute_zeta(sum_axis=1)</code>","text":"<p>Returns:</p> Type Description <p>The expectation of the squared effect size under the variational posterior.</p> Source code in <code>viprs/model/VIPRSMix.py</code> <pre><code>def compute_zeta(self, sum_axis=1):\n    \"\"\"\n    :return: The expectation of the squared effect size under the variational posterior.\n    \"\"\"\n    return {c: (v * (self.var_mu[c] ** 2 + (1./self.var_tau[c]))).sum(axis=sum_axis)\n            for c, v in self.var_gamma.items()}\n</code></pre>"},{"location":"api/model/VIPRSMix/#viprs.model.VIPRSMix.VIPRSMix.e_step","title":"<code>e_step()</code>","text":"<p>Run the E-Step of the Variational EM algorithm. Here, we update the variational parameters for each variant using coordinate ascent optimization techniques. The update equations are outlined in the Supplementary Material of the following paper:</p> <p>Zabad S, Gravel S, Li Y. Fast and accurate Bayesian polygenic risk modeling with variational inference. Am J Hum Genet. 2023 May 4;110(5):741-761. doi: 10.1016/j.ajhg.2023.03.009. Epub 2023 Apr 7. PMID: 37030289; PMCID: PMC10183379.</p> Source code in <code>viprs/model/VIPRSMix.py</code> <pre><code>def e_step(self):\n    \"\"\"\n    Run the E-Step of the Variational EM algorithm.\n    Here, we update the variational parameters for each variant using coordinate\n    ascent optimization techniques. The update equations are outlined in\n    the Supplementary Material of the following paper:\n\n    &gt; Zabad S, Gravel S, Li Y. Fast and accurate Bayesian polygenic risk modeling with variational inference.\n    Am J Hum Genet. 2023 May 4;110(5):741-761. doi: 10.1016/j.ajhg.2023.03.009.\n    Epub 2023 Apr 7. PMID: 37030289; PMCID: PMC10183379.\n    \"\"\"\n\n    for c, shapes in self.shapes.items():\n\n        # Get the priors:\n        tau_beta = self.get_tau_beta(c)\n        pi = self.get_pi(c)\n\n        # Updates for tau variational parameters:\n        self.var_tau[c] = (self.n_per_snp[c]*(1. + self.lambda_min) / self.sigma_epsilon) + tau_beta\n\n        if isinstance(self.pi, dict):\n            log_null_pi = (np.log(1. - self.pi[c].sum(axis=1)))\n        else:\n            log_null_pi = np.ones_like(self.eta[c])*np.log(1. - self.pi.sum())\n\n        # Compute some quantities that are needed for the per-SNP updates:\n        mu_mult = self.n_per_snp[c] / (self.var_tau[c] * self.sigma_epsilon)\n        u_logs = np.log(pi) - np.log(1. - pi) + .5 * (np.log(tau_beta) - np.log(self.var_tau[c]))\n\n        cpp_e_step_mixture(self.ld_left_bound[c],\n                           self.ld_indptr[c],\n                           self.ld_data[c],\n                           self.std_beta[c],\n                           self.var_gamma[c],\n                           self.var_mu[c],\n                           self.eta[c],\n                           self.q[c],\n                           self.eta_diff[c],\n                           log_null_pi,\n                           u_logs,\n                           np.sqrt(0.5*self.var_tau[c]),\n                           mu_mult,\n                           self.dequantize_scale,\n                           self.threads,\n                           self.low_memory)\n\n    self.zeta = self.compute_zeta()\n</code></pre>"},{"location":"api/model/VIPRSMix/#viprs.model.VIPRSMix.VIPRSMix.get_average_effect_size_variance","title":"<code>get_average_effect_size_variance()</code>","text":"<p>Returns:</p> Type Description <p>The average per-SNP variance for the prior mixture components</p> Source code in <code>viprs/model/VIPRSMix.py</code> <pre><code>def get_average_effect_size_variance(self):\n    \"\"\"\n    :return: The average per-SNP variance for the prior mixture components\n    \"\"\"\n\n    avg_sigma = super().get_average_effect_size_variance()\n\n    try:\n        return avg_sigma.sum()\n    except Exception:\n        return avg_sigma\n</code></pre>"},{"location":"api/model/VIPRSMix/#viprs.model.VIPRSMix.VIPRSMix.get_null_pi","title":"<code>get_null_pi(chrom=None)</code>","text":"<p>Get the proportion of SNPs in the null component</p> <p>Parameters:</p> Name Type Description Default <code>chrom</code> <p>If provided, get the mixing proportion for the null component on a given chromosome.</p> <code>None</code> <p>Returns:</p> Type Description <p>The value of the mixing proportion for the null component</p> Source code in <code>viprs/model/VIPRSMix.py</code> <pre><code>def get_null_pi(self, chrom=None):\n    \"\"\"\n    Get the proportion of SNPs in the null component\n    :param chrom: If provided, get the mixing proportion for the null component on a given chromosome.\n    :return: The value of the mixing proportion for the null component\n    \"\"\"\n\n    pi = self.get_pi(chrom=chrom)\n\n    if isinstance(pi, dict):\n        return {c: 1. - c_pi.sum(axis=1) for c, c_pi in pi.items()}\n    else:\n        return 1. - np.sum(pi)\n</code></pre>"},{"location":"api/model/VIPRSMix/#viprs.model.VIPRSMix.VIPRSMix.get_proportion_causal","title":"<code>get_proportion_causal()</code>","text":"<p>Returns:</p> Type Description <p>The proportion of variants in the non-null components.</p> Source code in <code>viprs/model/VIPRSMix.py</code> <pre><code>def get_proportion_causal(self):\n    \"\"\"\n    :return: The proportion of variants in the non-null components.\n    \"\"\"\n    if isinstance(self.pi, dict):\n        dict_mean({c: pis.sum(axis=1) for c, pis in self.pi.items()})\n    else:\n        return np.sum(self.pi)\n</code></pre>"},{"location":"api/model/VIPRSMix/#viprs.model.VIPRSMix.VIPRSMix.initialize_theta","title":"<code>initialize_theta(theta_0=None)</code>","text":"<p>Initialize the global hyperparameters of the model</p> <p>Parameters:</p> Name Type Description Default <code>theta_0</code> <p>A dictionary of initial values for the hyperparameters theta</p> <code>None</code> Source code in <code>viprs/model/VIPRSMix.py</code> <pre><code>def initialize_theta(self, theta_0=None):\n    \"\"\"\n    Initialize the global hyperparameters of the model\n    :param theta_0: A dictionary of initial values for the hyperparameters theta\n    \"\"\"\n\n    if theta_0 is not None and self.fix_params is not None:\n        theta_0.update(self.fix_params)\n    elif self.fix_params is not None:\n        theta_0 = self.fix_params\n    elif theta_0 is None:\n        theta_0 = {}\n\n    # ----------------------------------------------\n    # (1) Initialize pi from a uniform\n    if 'pis' in theta_0:\n        self.pi = theta_0['pis']\n    else:\n        if 'pi' in theta_0:\n            overall_pi = theta_0['pi']\n        else:\n            overall_pi = np.random.uniform(low=max(0.005, 1. / self.n_snps), high=.1)\n\n        self.pi = overall_pi*np.random.dirichlet(np.ones(self.K))\n\n    # ----------------------------------------------\n    # (2) Initialize sigma_epsilon and sigma_beta\n    # Assuming that the genotype and phenotype are normalized,\n    # these two quantities are conceptually linked.\n    # The initialization routine here assumes that:\n    # Var(y) = h2 + sigma_epsilon\n    # Where, by assumption, Var(y) = 1,\n    # And h2 ~= pi*M*sigma_beta\n\n    if 'sigma_epsilon' not in theta_0:\n\n        if 'tau_betas' in theta_0:\n\n            # If tau_betas are given, use them to initialize sigma_epsilon\n\n            self.tau_beta = theta_0['tau_betas']\n\n            self.sigma_epsilon = np.clip(1. - np.dot(1./self.tau_beta, self.pi),\n                                         a_min=1e-4,\n                                         a_max=1. - 1e-4)\n\n        elif 'tau_beta' in theta_0:\n            # NOTE: Here, we assume the provided `tau_beta` is a scalar.\n            # This is different from `tau_betas`\n\n            assert self.d is not None\n\n            self.tau_beta = theta_0['tau_beta'] * self.d\n            # Use the provided tau_beta to initialize sigma_epsilon.\n            # First, we derive a naive estimate of the heritability, based on the following equation:\n            # h2g/M = \\sum_k pi_k \\tau_k\n            # Where the per-SNP heritability is defined by the sum over the mixtures.\n\n            # Step (1): Given the provided tau_beta and associated multipliers,\n            # obtain a naive estimate of the heritability:\n            h2g_estimate = (self.n_snps*self.pi/self.tau_beta).sum()\n            # Step (2): Set sigma_epsilon to 1 - h2g_estimate:\n            self.sigma_epsilon = np.clip(1. - h2g_estimate,\n                                         a_min=1e-4,\n                                         a_max=1. - 1e-4)\n\n        else:\n            # If neither sigma_beta nor sigma_epsilon are given,\n            # then initialize using the SNP heritability estimate based on summary statistics\n\n            try:\n                naive_h2g = np.clip(simple_ldsc(self.gdl), 1e-3, 1. - 1e-3)\n            except Exception as e:\n                naive_h2g = np.random.uniform(low=.001, high=.999)\n\n            self.sigma_epsilon = 1. - naive_h2g\n\n            global_tau = (self.n_snps * np.dot(1./self.d, self.pi) / naive_h2g)\n\n            self.tau_beta = self.d*global_tau\n    else:\n\n        # If sigma_epsilon is given, use it in the initialization\n\n        self.sigma_epsilon = theta_0['sigma_epsilon']\n\n        # Initialize tau_betas\n        if 'tau_betas' in theta_0:\n            self.tau_beta = theta_0['tau_betas']\n        elif 'tau_beta' in theta_0:\n            self.tau_beta = np.repeat(theta_0['tau_beta'], self.K)\n        else:\n            # If not provided, initialize using sigma_epsilon value\n            global_tau = (self.n_snps * np.dot(1./self.d, self.pi) / (1. - self.sigma_epsilon))\n\n            self.tau_beta = self.d * global_tau\n\n    # Cast all the hyperparameters to conform to the precision set by the user:\n    self.sigma_epsilon = np.dtype(self.float_precision).type(self.sigma_epsilon)\n    self.pi = np.dtype(self.float_precision).type(self.pi)\n    self.lambda_min = np.dtype(self.float_precision).type(self.lambda_min)\n    self._sigma_g = np.dtype(self.float_precision).type(0.)\n</code></pre>"},{"location":"api/model/VIPRSMix/#viprs.model.VIPRSMix.VIPRSMix.to_theta_table","title":"<code>to_theta_table()</code>","text":"<p>Returns:</p> Type Description <p>A <code>pandas</code> DataFrame containing information about the estimated hyperparameters of the model.</p> Source code in <code>viprs/model/VIPRSMix.py</code> <pre><code>def to_theta_table(self):\n    \"\"\"\n    :return: A `pandas` DataFrame containing information about the estimated hyperparameters of the model.\n    \"\"\"\n\n    table = super().to_theta_table()\n\n    extra_theta = []\n\n    if isinstance(self.pi, dict):\n        pis = list(dict_mean(self.pi, axis=0))\n    else:\n        pis = self.pi\n\n    for i in range(self.K):\n        extra_theta.append({'Parameter': f'pi_{i + 1}', 'Value': pis[i]})\n\n    return pd.concat([table, pd.DataFrame(extra_theta)])\n</code></pre>"},{"location":"api/model/VIPRSMix/#viprs.model.VIPRSMix.VIPRSMix.update_pi","title":"<code>update_pi()</code>","text":"<p>Update the prior mixing proportions <code>pi</code></p> Source code in <code>viprs/model/VIPRSMix.py</code> <pre><code>def update_pi(self):\n    \"\"\"\n    Update the prior mixing proportions `pi`\n    \"\"\"\n\n    if 'pis' not in self.fix_params:\n\n        pi_estimate = dict_sum(self.var_gamma, axis=0)\n\n        if 'pi' in self.fix_params:\n            # If the user provides an estimate for the total proportion of causal variants,\n            # update the pis such that the proportion of SNPs in the null component becomes 1. - pi.\n            pi_estimate = self.fix_params['pi']*pi_estimate / pi_estimate.sum()\n        else:\n            pi_estimate /= self.n_snps\n\n        # Set pi to the new estimate:\n        self.pi = pi_estimate\n</code></pre>"},{"location":"api/model/VIPRSMix/#viprs.model.VIPRSMix.VIPRSMix.update_tau_beta","title":"<code>update_tau_beta()</code>","text":"<p>Update the prior precision (inverse variance) for the effect sizes, <code>tau_beta</code></p> Source code in <code>viprs/model/VIPRSMix.py</code> <pre><code>def update_tau_beta(self):\n    \"\"\"\n    Update the prior precision (inverse variance) for the effect sizes, `tau_beta`\n    \"\"\"\n\n    if 'tau_betas' not in self.fix_params:\n\n        # If a list of multipliers is provided,\n        # estimate the global sigma_beta and then multiply it\n        # by the per-component multiplier to get the final sigma_betas.\n\n        zetas = sum(self.compute_zeta(sum_axis=0).values())\n\n        tau_beta_estimate = np.sum(self.pi)*self.m / np.dot(self.d, zetas)\n        tau_beta_estimate = self.d*tau_beta_estimate\n\n        self.tau_beta = np.clip(tau_beta_estimate, a_min=1., a_max=None)\n</code></pre>"},{"location":"api/model/gridsearch/HyperparameterGrid/","title":"HyperparameterGrid","text":""},{"location":"api/model/gridsearch/HyperparameterGrid/#viprs.model.gridsearch.HyperparameterGrid.HyperparameterGrid","title":"<code>HyperparameterGrid</code>","text":"<p>               Bases: <code>object</code></p> <p>A utility class to facilitate generating grids for the hyperparameters of the standard <code>VIPRS</code> models. It is designed to interface with models that operate on grids of hyperparameters, such as <code>VIPRSGridSeach</code> and <code>VIPRSBMA</code>. The hyperparameters for the standard VIPRS model are:</p> <ul> <li><code>sigma_epsilon</code>: The residual variance for the phenotype.</li> <li><code>tau_beta</code>: The precision (inverse variance) of the prior for the effect sizes.</li> <li><code>pi</code>: The proportion of non-zero effect sizes (polygenicity).</li> <li><code>lambda_min</code>: The extra ridge penalty that compensates for the non-PSD nature of the LD matrix.</li> </ul> <p>Attributes:</p> Name Type Description <code>sigma_epsilon</code> <p>A grid of values for the residual variance hyperparameter.</p> <code>tau_beta</code> <p>A grid of values for the precision of the prior for the effect sizes.</p> <code>pi</code> <p>A grid of values for the proportion of non-zero effect sizes.</p> <code>lambda_min</code> <p>A grid of values for the extra ridge penalty that compensates for the non-PSD nature of the LD matrix.</p> <code>h2_est</code> <p>An estimate of the heritability for the trait under consideration.</p> <code>h2_se</code> <p>The standard error of the heritability estimate.</p> <code>n_snps</code> <p>The number of common variants that may be relevant for this analysis.</p> Source code in <code>viprs/model/gridsearch/HyperparameterGrid.py</code> <pre><code>class HyperparameterGrid(object):\n    \"\"\"\n    A utility class to facilitate generating grids for the\n    hyperparameters of the standard `VIPRS` models. It is designed to\n    interface with models that operate on grids of hyperparameters,\n    such as `VIPRSGridSeach` and `VIPRSBMA`. The hyperparameters for\n    the standard VIPRS model are:\n\n    * `sigma_epsilon`: The residual variance for the phenotype.\n    * `tau_beta`: The precision (inverse variance) of the prior for the effect sizes.\n    * `pi`: The proportion of non-zero effect sizes (polygenicity).\n    * `lambda_min`: The extra ridge penalty that compensates for the non-PSD nature of the LD matrix.\n\n    :ivar sigma_epsilon: A grid of values for the residual variance hyperparameter.\n    :ivar tau_beta: A grid of values for the precision of the prior for the effect sizes.\n    :ivar pi: A grid of values for the proportion of non-zero effect sizes.\n    :ivar lambda_min: A grid of values for the extra ridge penalty that compensates for\n    the non-PSD nature of the LD matrix.\n    :ivar h2_est: An estimate of the heritability for the trait under consideration.\n    :ivar h2_se: The standard error of the heritability estimate.\n    :ivar n_snps: The number of common variants that may be relevant for this analysis.\n\n    \"\"\"\n\n    def __init__(self,\n                 sigma_epsilon_grid=None,\n                 sigma_epsilon_steps=None,\n                 tau_beta_grid=None,\n                 tau_beta_steps=None,\n                 pi_grid=None,\n                 pi_steps=None,\n                 lambda_min_grid=None,\n                 lambda_min_steps=None,\n                 h2_est=None,\n                 h2_se=None,\n                 n_snps=1e6):\n        \"\"\"\n\n        Create a hyperparameter grid for the standard VIPRS model with the\n        spike-and-slab prior. The hyperparameters for this model are:\n\n        * `sigma_epsilon`: The residual variance\n        * `tau_beta`: The precision (inverse variance) of the prior for the effect sizes\n        * `pi`: The proportion of non-zero effect sizes\n        * `lambda_min`: The extra ridge penalty that compensates for the non-PSD LD matrices.\n\n        For each of these hyperparameters, we can provide a grid of values to search over.\n        If the heritability estimate and standard error (from e.g. LDSC) are provided,\n        we can generate grids for sigma_epsilon and tau_beta that are informed by these estimates.\n\n        For each hyperparameter to be included in the grid, user must specify either the grid\n        itself, or the number of steps to use to generate the grid.\n\n        :param sigma_epsilon_grid: An array containing a grid of values for the sigma_epsilon hyperparameter.\n        :param sigma_epsilon_steps: The number of steps for the sigma_epsilon grid\n        :param tau_beta_grid: An array containing a grid of values for the tau_beta hyperparameter.\n        :param tau_beta_steps: The number of steps for the tau_beta grid\n        :param pi_grid: An array containing a grid of values for the pi hyperparameter\n        :param pi_steps: The number of steps for the pi grid\n        :param h2_est: An estimate of the heritability for the trait under consideration. If provided,\n        we can generate grids for some of the hyperparameters that are consistent with this estimate.\n        :param h2_se: The standard error of the heritability estimate. If provided, we can generate grids\n        for some of the hyperparameters that are consistent with this estimate.\n        :param n_snps: Number of common variants that may be relevant for this analysis. This estimate can\n        be used to generate grids that are based on this number.\n        \"\"\"\n\n        # If the heritability estimate is not provided, use a reasonable default value of 0.1\n        # with a wide standard error of 0.1.\n\n        self.h2_est = h2_est or 0.1\n        self.h2_se = h2_se or 0.1\n\n        self.n_snps = n_snps\n        self._search_params = []\n\n        # Initialize the grid for sigma_epsilon:\n        self.sigma_epsilon = sigma_epsilon_grid\n        if self.sigma_epsilon is not None:\n            self._search_params.append('sigma_epsilon')\n        elif sigma_epsilon_steps is not None:\n            self.generate_sigma_epsilon_grid(steps=sigma_epsilon_steps)\n\n        # Initialize the grid for the tau_beta:\n        self.tau_beta = tau_beta_grid\n        if self.tau_beta is not None:\n            self._search_params.append('tau_beta')\n        elif tau_beta_steps is not None:\n            self.generate_tau_beta_grid(steps=tau_beta_steps)\n\n        # Initialize the grid for pi:\n        self.pi = pi_grid\n        if self.pi is not None:\n            self._search_params.append('pi')\n        elif pi_steps is not None:\n            self.generate_pi_grid(steps=pi_steps)\n\n        # Initialize the grid for lambda_min:\n        self.lambda_min = lambda_min_grid\n        if self.lambda_min is not None:\n            self._search_params.append('lambda_min')\n        elif lambda_min_steps is not None:\n            self.generate_lambda_min_grid(steps=lambda_min_steps)\n\n    def _generate_h2_grid(self, steps=5):\n        \"\"\"\n        Use the heritability estimate and standard error to generate a grid of values for\n        the heritability parameter. Specifically, given the estimate and standard error, we\n        generate heritability estimates from the percentiles of the normal distribution,\n        with mean `h2_est` and standard deviation `h2_se`. The grid values range from the 10th\n        percentile to the 90th percentile of this normal distribution.\n\n        :param steps: The number of steps for the heritability grid.\n        :return: A grid of values for the heritability parameter.\n\n        \"\"\"\n\n        assert steps &gt; 0\n        assert self.h2_est is not None\n\n        # If the heritability standard error is not provided, we use half of the heritability estimate\n        # by default.\n        # *Justification*: Under the assumption that heritability for the trait being analyzed\n        # is significantly greater than 0, the standard error should be, at a maximum,\n        # half of the heritability estimate itself to get us a Z-score with absolute value\n        # greater than 2.\n        if self.h2_se is None:\n            h2_se = self.h2_est * 0.5\n        else:\n            h2_se = self.h2_se\n\n        # Sanity checking steps:\n        assert 0. &lt; self.h2_est &lt; 1.\n        assert h2_se &gt; 0\n\n        from scipy.stats import norm\n\n        # First, determine the percentile boundaries to avoid producing\n        # invalid values for the heritability grid:\n\n        percentile_start = max(0.1, norm.cdf(1e-5, loc=self.h2_est, scale=h2_se))\n        percentile_stop = min(0.9, norm.cdf(1. - 1e-5, loc=self.h2_est, scale=h2_se))\n\n        # Generate the heritability grid:\n        return norm.ppf(np.linspace(percentile_start, percentile_stop, steps),\n                        loc=self.h2_est, scale=h2_se)\n\n    def generate_sigma_epsilon_grid(self, steps=5):\n        \"\"\"\n        Generate a grid of values for the `sigma_epsilon` (residual variance) hyperparameter.\n\n        :param steps: The number of steps for the sigma_epsilon grid.\n        \"\"\"\n\n        assert steps &gt; 0\n\n        h2_grid = self._generate_h2_grid(steps)\n        self.sigma_epsilon = 1. - h2_grid\n\n        if 'sigma_epsilon' not in self._search_params:\n            self._search_params.append('sigma_epsilon')\n\n    def generate_tau_beta_grid(self, steps=5):\n        \"\"\"\n        Generate a grid of values for the `tau_beta`\n        (precision of the prior for the effect sizes) hyperparameter.\n        :param steps: The number of steps for the `tau_beta` grid\n        \"\"\"\n\n        assert steps &gt; 0\n\n        h2_grid = self._generate_h2_grid(steps)\n        # Assume ~1% of SNPs are causal:\n        self.tau_beta = 0.01*self.n_snps / h2_grid\n\n        if 'tau_beta' not in self._search_params:\n            self._search_params.append('tau_beta')\n\n    def generate_pi_grid(self, steps=5, max_pi=0.2):\n        \"\"\"\n        Generate a grid of values for the `pi` (proportion of non-zero effect sizes) hyperparameter.\n        :param steps: The number of steps for the `pi` grid\n        :param max_pi: The maximum value for the `pi` grid.\n        \"\"\"\n\n        assert steps &gt; 0\n\n        min_pi = np.log10(max(10./self.n_snps, 1e-5))\n        # For now, we impose a limit of 10k causal variants\n        # Need to figure out better ways to determine the maximum\n        # value here.\n        max_pi = np.log10(min(10000 / self.n_snps, max_pi))\n\n        assert min_pi &lt; max_pi\n\n        self.pi = np.logspace(\n            min_pi,\n            max_pi,\n            steps\n        )\n\n        if 'pi' not in self._search_params:\n            self._search_params.append('pi')\n\n    def generate_lambda_min_grid(self, steps=5, emp_lambda_min=None):\n        \"\"\"\n        Generate a grid of values for the `lambda_min` parameter, associated with extra ridge penalty\n        that compensates for the non-PSD nature of the LD matrix.\n        :param steps: The number of steps for the `lambda_min` grid.\n        :param emp_lambda_min: The empirical value of lambda_min to use as a reference point.\n        \"\"\"\n\n        assert steps &gt; 0\n\n        self.lambda_min = np.concatenate([[0.], np.logspace(-4, 1., steps - 1)])\n\n        if emp_lambda_min is not None:\n            self.lambda_min *= emp_lambda_min\n\n        if 'lambda_min' not in self._search_params:\n            self._search_params.append('lambda_min')\n\n    def combine_grids(self):\n        \"\"\"\n        Weave together the different hyperparameter grids and return a list of\n        dictionaries where the key is the hyperparameter name and the value is\n        value for that hyperparameter.\n\n        :return: A list of dictionaries containing the hyperparameter values.\n        :raises ValueError: If all the grids are empty.\n\n        \"\"\"\n        hyp_names = [name for name, value in self.__dict__.items()\n                     if value is not None and name in self._search_params]\n\n        if len(hyp_names) &gt; 0:\n            hyp_values = itertools.product(*[hyp_grid for hyp_name, hyp_grid in self.__dict__.items()\n                                             if hyp_grid is not None and hyp_name in hyp_names])\n\n            return [dict(zip(hyp_names, hyp_v)) for hyp_v in hyp_values]\n        else:\n            raise ValueError(\"All the grids are empty!\")\n\n    def to_table(self):\n        \"\"\"\n        :return: The hyperparameter grid as a pandas `DataFrame`.\n        :raises ValueError: If all the grids are empty.\n        \"\"\"\n\n        combined_grids = self.combine_grids()\n\n        return pd.DataFrame(combined_grids)\n</code></pre>"},{"location":"api/model/gridsearch/HyperparameterGrid/#viprs.model.gridsearch.HyperparameterGrid.HyperparameterGrid.__init__","title":"<code>__init__(sigma_epsilon_grid=None, sigma_epsilon_steps=None, tau_beta_grid=None, tau_beta_steps=None, pi_grid=None, pi_steps=None, lambda_min_grid=None, lambda_min_steps=None, h2_est=None, h2_se=None, n_snps=1000000.0)</code>","text":"<p>Create a hyperparameter grid for the standard VIPRS model with the spike-and-slab prior. The hyperparameters for this model are:</p> <ul> <li><code>sigma_epsilon</code>: The residual variance</li> <li><code>tau_beta</code>: The precision (inverse variance) of the prior for the effect sizes</li> <li><code>pi</code>: The proportion of non-zero effect sizes</li> <li><code>lambda_min</code>: The extra ridge penalty that compensates for the non-PSD LD matrices.</li> </ul> <p>For each of these hyperparameters, we can provide a grid of values to search over. If the heritability estimate and standard error (from e.g. LDSC) are provided, we can generate grids for sigma_epsilon and tau_beta that are informed by these estimates.</p> <p>For each hyperparameter to be included in the grid, user must specify either the grid itself, or the number of steps to use to generate the grid.</p> <p>Parameters:</p> Name Type Description Default <code>sigma_epsilon_grid</code> <p>An array containing a grid of values for the sigma_epsilon hyperparameter.</p> <code>None</code> <code>sigma_epsilon_steps</code> <p>The number of steps for the sigma_epsilon grid</p> <code>None</code> <code>tau_beta_grid</code> <p>An array containing a grid of values for the tau_beta hyperparameter.</p> <code>None</code> <code>tau_beta_steps</code> <p>The number of steps for the tau_beta grid</p> <code>None</code> <code>pi_grid</code> <p>An array containing a grid of values for the pi hyperparameter</p> <code>None</code> <code>pi_steps</code> <p>The number of steps for the pi grid</p> <code>None</code> <code>h2_est</code> <p>An estimate of the heritability for the trait under consideration. If provided, we can generate grids for some of the hyperparameters that are consistent with this estimate.</p> <code>None</code> <code>h2_se</code> <p>The standard error of the heritability estimate. If provided, we can generate grids for some of the hyperparameters that are consistent with this estimate.</p> <code>None</code> <code>n_snps</code> <p>Number of common variants that may be relevant for this analysis. This estimate can be used to generate grids that are based on this number.</p> <code>1000000.0</code> Source code in <code>viprs/model/gridsearch/HyperparameterGrid.py</code> <pre><code>def __init__(self,\n             sigma_epsilon_grid=None,\n             sigma_epsilon_steps=None,\n             tau_beta_grid=None,\n             tau_beta_steps=None,\n             pi_grid=None,\n             pi_steps=None,\n             lambda_min_grid=None,\n             lambda_min_steps=None,\n             h2_est=None,\n             h2_se=None,\n             n_snps=1e6):\n    \"\"\"\n\n    Create a hyperparameter grid for the standard VIPRS model with the\n    spike-and-slab prior. The hyperparameters for this model are:\n\n    * `sigma_epsilon`: The residual variance\n    * `tau_beta`: The precision (inverse variance) of the prior for the effect sizes\n    * `pi`: The proportion of non-zero effect sizes\n    * `lambda_min`: The extra ridge penalty that compensates for the non-PSD LD matrices.\n\n    For each of these hyperparameters, we can provide a grid of values to search over.\n    If the heritability estimate and standard error (from e.g. LDSC) are provided,\n    we can generate grids for sigma_epsilon and tau_beta that are informed by these estimates.\n\n    For each hyperparameter to be included in the grid, user must specify either the grid\n    itself, or the number of steps to use to generate the grid.\n\n    :param sigma_epsilon_grid: An array containing a grid of values for the sigma_epsilon hyperparameter.\n    :param sigma_epsilon_steps: The number of steps for the sigma_epsilon grid\n    :param tau_beta_grid: An array containing a grid of values for the tau_beta hyperparameter.\n    :param tau_beta_steps: The number of steps for the tau_beta grid\n    :param pi_grid: An array containing a grid of values for the pi hyperparameter\n    :param pi_steps: The number of steps for the pi grid\n    :param h2_est: An estimate of the heritability for the trait under consideration. If provided,\n    we can generate grids for some of the hyperparameters that are consistent with this estimate.\n    :param h2_se: The standard error of the heritability estimate. If provided, we can generate grids\n    for some of the hyperparameters that are consistent with this estimate.\n    :param n_snps: Number of common variants that may be relevant for this analysis. This estimate can\n    be used to generate grids that are based on this number.\n    \"\"\"\n\n    # If the heritability estimate is not provided, use a reasonable default value of 0.1\n    # with a wide standard error of 0.1.\n\n    self.h2_est = h2_est or 0.1\n    self.h2_se = h2_se or 0.1\n\n    self.n_snps = n_snps\n    self._search_params = []\n\n    # Initialize the grid for sigma_epsilon:\n    self.sigma_epsilon = sigma_epsilon_grid\n    if self.sigma_epsilon is not None:\n        self._search_params.append('sigma_epsilon')\n    elif sigma_epsilon_steps is not None:\n        self.generate_sigma_epsilon_grid(steps=sigma_epsilon_steps)\n\n    # Initialize the grid for the tau_beta:\n    self.tau_beta = tau_beta_grid\n    if self.tau_beta is not None:\n        self._search_params.append('tau_beta')\n    elif tau_beta_steps is not None:\n        self.generate_tau_beta_grid(steps=tau_beta_steps)\n\n    # Initialize the grid for pi:\n    self.pi = pi_grid\n    if self.pi is not None:\n        self._search_params.append('pi')\n    elif pi_steps is not None:\n        self.generate_pi_grid(steps=pi_steps)\n\n    # Initialize the grid for lambda_min:\n    self.lambda_min = lambda_min_grid\n    if self.lambda_min is not None:\n        self._search_params.append('lambda_min')\n    elif lambda_min_steps is not None:\n        self.generate_lambda_min_grid(steps=lambda_min_steps)\n</code></pre>"},{"location":"api/model/gridsearch/HyperparameterGrid/#viprs.model.gridsearch.HyperparameterGrid.HyperparameterGrid.combine_grids","title":"<code>combine_grids()</code>","text":"<p>Weave together the different hyperparameter grids and return a list of dictionaries where the key is the hyperparameter name and the value is value for that hyperparameter.</p> <p>Returns:</p> Type Description <p>A list of dictionaries containing the hyperparameter values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If all the grids are empty.</p> Source code in <code>viprs/model/gridsearch/HyperparameterGrid.py</code> <pre><code>def combine_grids(self):\n    \"\"\"\n    Weave together the different hyperparameter grids and return a list of\n    dictionaries where the key is the hyperparameter name and the value is\n    value for that hyperparameter.\n\n    :return: A list of dictionaries containing the hyperparameter values.\n    :raises ValueError: If all the grids are empty.\n\n    \"\"\"\n    hyp_names = [name for name, value in self.__dict__.items()\n                 if value is not None and name in self._search_params]\n\n    if len(hyp_names) &gt; 0:\n        hyp_values = itertools.product(*[hyp_grid for hyp_name, hyp_grid in self.__dict__.items()\n                                         if hyp_grid is not None and hyp_name in hyp_names])\n\n        return [dict(zip(hyp_names, hyp_v)) for hyp_v in hyp_values]\n    else:\n        raise ValueError(\"All the grids are empty!\")\n</code></pre>"},{"location":"api/model/gridsearch/HyperparameterGrid/#viprs.model.gridsearch.HyperparameterGrid.HyperparameterGrid.generate_lambda_min_grid","title":"<code>generate_lambda_min_grid(steps=5, emp_lambda_min=None)</code>","text":"<p>Generate a grid of values for the <code>lambda_min</code> parameter, associated with extra ridge penalty that compensates for the non-PSD nature of the LD matrix.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <p>The number of steps for the <code>lambda_min</code> grid.</p> <code>5</code> <code>emp_lambda_min</code> <p>The empirical value of lambda_min to use as a reference point.</p> <code>None</code> Source code in <code>viprs/model/gridsearch/HyperparameterGrid.py</code> <pre><code>def generate_lambda_min_grid(self, steps=5, emp_lambda_min=None):\n    \"\"\"\n    Generate a grid of values for the `lambda_min` parameter, associated with extra ridge penalty\n    that compensates for the non-PSD nature of the LD matrix.\n    :param steps: The number of steps for the `lambda_min` grid.\n    :param emp_lambda_min: The empirical value of lambda_min to use as a reference point.\n    \"\"\"\n\n    assert steps &gt; 0\n\n    self.lambda_min = np.concatenate([[0.], np.logspace(-4, 1., steps - 1)])\n\n    if emp_lambda_min is not None:\n        self.lambda_min *= emp_lambda_min\n\n    if 'lambda_min' not in self._search_params:\n        self._search_params.append('lambda_min')\n</code></pre>"},{"location":"api/model/gridsearch/HyperparameterGrid/#viprs.model.gridsearch.HyperparameterGrid.HyperparameterGrid.generate_pi_grid","title":"<code>generate_pi_grid(steps=5, max_pi=0.2)</code>","text":"<p>Generate a grid of values for the <code>pi</code> (proportion of non-zero effect sizes) hyperparameter.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <p>The number of steps for the <code>pi</code> grid</p> <code>5</code> <code>max_pi</code> <p>The maximum value for the <code>pi</code> grid.</p> <code>0.2</code> Source code in <code>viprs/model/gridsearch/HyperparameterGrid.py</code> <pre><code>def generate_pi_grid(self, steps=5, max_pi=0.2):\n    \"\"\"\n    Generate a grid of values for the `pi` (proportion of non-zero effect sizes) hyperparameter.\n    :param steps: The number of steps for the `pi` grid\n    :param max_pi: The maximum value for the `pi` grid.\n    \"\"\"\n\n    assert steps &gt; 0\n\n    min_pi = np.log10(max(10./self.n_snps, 1e-5))\n    # For now, we impose a limit of 10k causal variants\n    # Need to figure out better ways to determine the maximum\n    # value here.\n    max_pi = np.log10(min(10000 / self.n_snps, max_pi))\n\n    assert min_pi &lt; max_pi\n\n    self.pi = np.logspace(\n        min_pi,\n        max_pi,\n        steps\n    )\n\n    if 'pi' not in self._search_params:\n        self._search_params.append('pi')\n</code></pre>"},{"location":"api/model/gridsearch/HyperparameterGrid/#viprs.model.gridsearch.HyperparameterGrid.HyperparameterGrid.generate_sigma_epsilon_grid","title":"<code>generate_sigma_epsilon_grid(steps=5)</code>","text":"<p>Generate a grid of values for the <code>sigma_epsilon</code> (residual variance) hyperparameter.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <p>The number of steps for the sigma_epsilon grid.</p> <code>5</code> Source code in <code>viprs/model/gridsearch/HyperparameterGrid.py</code> <pre><code>def generate_sigma_epsilon_grid(self, steps=5):\n    \"\"\"\n    Generate a grid of values for the `sigma_epsilon` (residual variance) hyperparameter.\n\n    :param steps: The number of steps for the sigma_epsilon grid.\n    \"\"\"\n\n    assert steps &gt; 0\n\n    h2_grid = self._generate_h2_grid(steps)\n    self.sigma_epsilon = 1. - h2_grid\n\n    if 'sigma_epsilon' not in self._search_params:\n        self._search_params.append('sigma_epsilon')\n</code></pre>"},{"location":"api/model/gridsearch/HyperparameterGrid/#viprs.model.gridsearch.HyperparameterGrid.HyperparameterGrid.generate_tau_beta_grid","title":"<code>generate_tau_beta_grid(steps=5)</code>","text":"<p>Generate a grid of values for the <code>tau_beta</code> (precision of the prior for the effect sizes) hyperparameter.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <p>The number of steps for the <code>tau_beta</code> grid</p> <code>5</code> Source code in <code>viprs/model/gridsearch/HyperparameterGrid.py</code> <pre><code>def generate_tau_beta_grid(self, steps=5):\n    \"\"\"\n    Generate a grid of values for the `tau_beta`\n    (precision of the prior for the effect sizes) hyperparameter.\n    :param steps: The number of steps for the `tau_beta` grid\n    \"\"\"\n\n    assert steps &gt; 0\n\n    h2_grid = self._generate_h2_grid(steps)\n    # Assume ~1% of SNPs are causal:\n    self.tau_beta = 0.01*self.n_snps / h2_grid\n\n    if 'tau_beta' not in self._search_params:\n        self._search_params.append('tau_beta')\n</code></pre>"},{"location":"api/model/gridsearch/HyperparameterGrid/#viprs.model.gridsearch.HyperparameterGrid.HyperparameterGrid.to_table","title":"<code>to_table()</code>","text":"<p>Returns:</p> Type Description <p>The hyperparameter grid as a pandas <code>DataFrame</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If all the grids are empty.</p> Source code in <code>viprs/model/gridsearch/HyperparameterGrid.py</code> <pre><code>def to_table(self):\n    \"\"\"\n    :return: The hyperparameter grid as a pandas `DataFrame`.\n    :raises ValueError: If all the grids are empty.\n    \"\"\"\n\n    combined_grids = self.combine_grids()\n\n    return pd.DataFrame(combined_grids)\n</code></pre>"},{"location":"api/model/gridsearch/HyperparameterSearch/","title":"HyperparameterSearch","text":""},{"location":"api/model/gridsearch/HyperparameterSearch/#viprs.model.gridsearch.HyperparameterSearch.BaseHyperparamSearch","title":"<code>BaseHyperparamSearch</code>","text":"<p>               Bases: <code>object</code></p> <p>A generic class for performing hyperparameter search on any genetic PRS model. This API is under active development and some of the components may change in the near future.</p> <p>TODO: Allow users to choose different metrics under each criterion.</p> Source code in <code>viprs/model/gridsearch/HyperparameterSearch.py</code> <pre><code>class BaseHyperparamSearch(object):\n    \"\"\"\n    A generic class for performing hyperparameter search on any genetic PRS model.\n    This API is under active development and some of the components may change in the near future.\n\n    TODO: Allow users to choose different metrics under each criterion.\n\n    \"\"\"\n\n    def __init__(self,\n                 gdl,\n                 model=None,\n                 criterion='training_objective',\n                 validation_gdl=None,\n                 n_jobs=1):\n        \"\"\"\n        A generic hyperparameter search class that implements common functionalities\n        that may be required by hyperparameter search strategies.\n        :param gdl: A GWADataLoader object containing the GWAS summary statistics for inference.\n        :param model: An instance of the PRS model to use for the hyperparameter search. By default,\n        we use `VIPRS`.\n        :param criterion: The objective function for the hyperparameter search.\n        Options are: `training_objective`, `pseudo_validation` or `validation`. In the case of `VIPRS`, the training\n        objective is the ELBO.\n        :param validation_gdl: If the objective is validation or pseudo-validation, provide the GWADataLoader\n        object for the validation dataset. If the criterion is pseudo-validation, the `validation_gdl` should\n        contain summary statistics from a held-out test set. If the criterion is validation, `validation_gdl` should\n        contain individual-level data from a held-out test set.\n        :param n_jobs: The number of processes to use for the hyperparameters search.\n        \"\"\"\n\n        # Sanity checking:\n        assert criterion in ('training_objective', 'validation', 'pseudo_validation')\n\n        self.gdl = gdl\n        self.n_jobs = n_jobs\n\n        if model is None:\n            self.model = VIPRS(gdl)\n        else:\n            import inspect\n            if inspect.isclass(model):\n                self.model = model(gdl)\n            else:\n                self.model = model\n\n        self.validation_result = None\n\n        self.criterion = criterion\n        self._validation_gdl = validation_gdl\n\n        self._model_coefs = None\n        self._model_hyperparams = None\n        self._training_objective = None\n\n        # Sanity checks:\n        if self.criterion == 'training_objective':\n            assert hasattr(self.model, 'objective')\n        elif self.criterion == 'pseudo_validation':\n            assert self._validation_gdl is not None\n            assert self._validation_gdl.sumstats_table is not None\n        if self.criterion == 'validation':\n            assert self._validation_gdl is not None\n            assert self._validation_gdl.genotype is not None\n            assert self._validation_gdl.sample_table.phenotype is not None\n\n    def to_validation_table(self):\n        \"\"\"\n        Summarize the validation results in a pandas table.\n        :return: A pandas DataFrame with the validation results.\n        \"\"\"\n        if self.validation_result is None:\n            raise Exception(\"Validation result is not set!\")\n        elif len(self.validation_result) &lt; 1:\n            raise Exception(\"Validation result is not set!\")\n\n        return pd.DataFrame(self.validation_result)\n\n    def write_validation_result(self, v_filename, sep=\"\\t\"):\n        \"\"\"\n        After performing hyperparameter search, write a table\n        that records that value of the objective for each combination\n        of hyperparameters.\n        :param v_filename: The filename for the validation table.\n        :param sep: The separator for the validation table\n        \"\"\"\n\n        v_df = self.to_validation_table()\n        v_df.to_csv(v_filename, index=False, sep=sep)\n\n    def _evaluate_models(self):\n        \"\"\"\n        This method evaluates multiple PRS models to determine their relative performance based on the\n        criterion set by the user. The criterion can be the training objective (e.g. ELBO in the case of VIPRS),\n        pseudo-validation or validation using held-out test data.\n\n        :return: The metrics associated with each model setup.\n        \"\"\"\n\n        assert self._training_objective is not None\n        assert self._model_coefs is not None\n\n        if self.criterion == 'training_objective':\n            metrics = self._training_objective\n        elif self.criterion == 'pseudo_validation':\n            from viprs.eval.pseudo_metrics import pseudo_r2\n\n            metrics = pseudo_r2(self._validation_gdl, self._model_coefs)\n\n        else:\n\n            prs_m = BayesPRSModel(self._validation_gdl)\n            prs_m.set_model_parameters(self._model_coefs)\n\n            prs = prs_m.predict(test_gdl=self._validation_gdl)\n\n            if self._validation_gdl.phenotype_likelihood == 'binomial':\n                eval_func = roc_auc\n            else:\n                eval_func = r2\n\n            metrics = [eval_func(prs[:, i].flatten(), self._validation_gdl.sample_table.phenotype)\n                       for i in range(prs.shape[1])]\n\n        return metrics\n\n    def fit(self):\n        raise NotImplementedError\n</code></pre>"},{"location":"api/model/gridsearch/HyperparameterSearch/#viprs.model.gridsearch.HyperparameterSearch.BaseHyperparamSearch.__init__","title":"<code>__init__(gdl, model=None, criterion='training_objective', validation_gdl=None, n_jobs=1)</code>","text":"<p>A generic hyperparameter search class that implements common functionalities that may be required by hyperparameter search strategies.</p> <p>Parameters:</p> Name Type Description Default <code>gdl</code> <p>A GWADataLoader object containing the GWAS summary statistics for inference.</p> required <code>model</code> <p>An instance of the PRS model to use for the hyperparameter search. By default, we use <code>VIPRS</code>.</p> <code>None</code> <code>criterion</code> <p>The objective function for the hyperparameter search. Options are: <code>training_objective</code>, <code>pseudo_validation</code> or <code>validation</code>. In the case of <code>VIPRS</code>, the training objective is the ELBO.</p> <code>'training_objective'</code> <code>validation_gdl</code> <p>If the objective is validation or pseudo-validation, provide the GWADataLoader object for the validation dataset. If the criterion is pseudo-validation, the <code>validation_gdl</code> should contain summary statistics from a held-out test set. If the criterion is validation, <code>validation_gdl</code> should contain individual-level data from a held-out test set.</p> <code>None</code> <code>n_jobs</code> <p>The number of processes to use for the hyperparameters search.</p> <code>1</code> Source code in <code>viprs/model/gridsearch/HyperparameterSearch.py</code> <pre><code>def __init__(self,\n             gdl,\n             model=None,\n             criterion='training_objective',\n             validation_gdl=None,\n             n_jobs=1):\n    \"\"\"\n    A generic hyperparameter search class that implements common functionalities\n    that may be required by hyperparameter search strategies.\n    :param gdl: A GWADataLoader object containing the GWAS summary statistics for inference.\n    :param model: An instance of the PRS model to use for the hyperparameter search. By default,\n    we use `VIPRS`.\n    :param criterion: The objective function for the hyperparameter search.\n    Options are: `training_objective`, `pseudo_validation` or `validation`. In the case of `VIPRS`, the training\n    objective is the ELBO.\n    :param validation_gdl: If the objective is validation or pseudo-validation, provide the GWADataLoader\n    object for the validation dataset. If the criterion is pseudo-validation, the `validation_gdl` should\n    contain summary statistics from a held-out test set. If the criterion is validation, `validation_gdl` should\n    contain individual-level data from a held-out test set.\n    :param n_jobs: The number of processes to use for the hyperparameters search.\n    \"\"\"\n\n    # Sanity checking:\n    assert criterion in ('training_objective', 'validation', 'pseudo_validation')\n\n    self.gdl = gdl\n    self.n_jobs = n_jobs\n\n    if model is None:\n        self.model = VIPRS(gdl)\n    else:\n        import inspect\n        if inspect.isclass(model):\n            self.model = model(gdl)\n        else:\n            self.model = model\n\n    self.validation_result = None\n\n    self.criterion = criterion\n    self._validation_gdl = validation_gdl\n\n    self._model_coefs = None\n    self._model_hyperparams = None\n    self._training_objective = None\n\n    # Sanity checks:\n    if self.criterion == 'training_objective':\n        assert hasattr(self.model, 'objective')\n    elif self.criterion == 'pseudo_validation':\n        assert self._validation_gdl is not None\n        assert self._validation_gdl.sumstats_table is not None\n    if self.criterion == 'validation':\n        assert self._validation_gdl is not None\n        assert self._validation_gdl.genotype is not None\n        assert self._validation_gdl.sample_table.phenotype is not None\n</code></pre>"},{"location":"api/model/gridsearch/HyperparameterSearch/#viprs.model.gridsearch.HyperparameterSearch.BaseHyperparamSearch.to_validation_table","title":"<code>to_validation_table()</code>","text":"<p>Summarize the validation results in a pandas table.</p> <p>Returns:</p> Type Description <p>A pandas DataFrame with the validation results.</p> Source code in <code>viprs/model/gridsearch/HyperparameterSearch.py</code> <pre><code>def to_validation_table(self):\n    \"\"\"\n    Summarize the validation results in a pandas table.\n    :return: A pandas DataFrame with the validation results.\n    \"\"\"\n    if self.validation_result is None:\n        raise Exception(\"Validation result is not set!\")\n    elif len(self.validation_result) &lt; 1:\n        raise Exception(\"Validation result is not set!\")\n\n    return pd.DataFrame(self.validation_result)\n</code></pre>"},{"location":"api/model/gridsearch/HyperparameterSearch/#viprs.model.gridsearch.HyperparameterSearch.BaseHyperparamSearch.write_validation_result","title":"<code>write_validation_result(v_filename, sep='\\t')</code>","text":"<p>After performing hyperparameter search, write a table that records that value of the objective for each combination of hyperparameters.</p> <p>Parameters:</p> Name Type Description Default <code>v_filename</code> <p>The filename for the validation table.</p> required <code>sep</code> <p>The separator for the validation table</p> <code>'\\t'</code> Source code in <code>viprs/model/gridsearch/HyperparameterSearch.py</code> <pre><code>def write_validation_result(self, v_filename, sep=\"\\t\"):\n    \"\"\"\n    After performing hyperparameter search, write a table\n    that records that value of the objective for each combination\n    of hyperparameters.\n    :param v_filename: The filename for the validation table.\n    :param sep: The separator for the validation table\n    \"\"\"\n\n    v_df = self.to_validation_table()\n    v_df.to_csv(v_filename, index=False, sep=sep)\n</code></pre>"},{"location":"api/model/gridsearch/HyperparameterSearch/#viprs.model.gridsearch.HyperparameterSearch.GridSearch","title":"<code>GridSearch</code>","text":"<p>               Bases: <code>BaseHyperparamSearch</code></p> <p>Hyperparameter search using Grid Search</p> Source code in <code>viprs/model/gridsearch/HyperparameterSearch.py</code> <pre><code>class GridSearch(BaseHyperparamSearch):\n    \"\"\"\n    Hyperparameter search using Grid Search\n    \"\"\"\n\n    def __init__(self,\n                 gdl,\n                 grid,\n                 model=None,\n                 criterion='training_objective',\n                 validation_gdl=None,\n                 n_jobs=1):\n\n        \"\"\"\n        Perform hyperparameter search using grid search\n        :param gdl: A GWADataLoader object containing the GWAS summary statistics for inference.\n        :param model: An instance of the PRS model to use for the hyperparameter search. By default,\n        we use `VIPRS`.\n        :param criterion: The objective function for the hyperparameter search.\n        Options are: `training_objective`, `pseudo_validation` or `validation`. In the case of `VIPRS`, the training\n        objective is the ELBO.\n        :param validation_gdl: If the objective is validation or pseudo-validation, provide the GWADataLoader\n        object for the validation dataset. If the criterion is pseudo-validation, the `validation_gdl` should\n        contain summary statistics from a held-out test set. If the criterion is validation, `validation_gdl` should\n        contain individual-level data from a held-out test set.\n        :param n_jobs: The number of processes to use for the hyperparameters search.\n        \"\"\"\n\n        super().__init__(gdl,\n                         model=model,\n                         criterion=criterion,\n                         validation_gdl=validation_gdl,\n                         n_jobs=n_jobs)\n\n        self.grid = grid\n        self.model.threads = 1\n\n    def fit(self, max_iter=1000, f_abs_tol=1e-6, x_abs_tol=1e-6):\n        \"\"\"\n        Perform grid search over the hyperparameters to determine the\n        best model based on the criterion set by the user. This utility method\n        performs model fitting across the grid of hyperparameters, potentially in parallel\n        if `n_jobs` is greater than 1.\n\n        :param max_iter: The maximum number of iterations to run for each model fit.\n        :param f_abs_tol: The absolute tolerance for the function convergence criterion.\n        :param x_abs_tol: The absolute tolerance for the parameter convergence criterion.\n\n        :return: The best model based on the criterion set by the user.\n        \"\"\"\n\n        logger.info(\"&gt; Performing Grid Search over the following grid:\")\n        logger.info(self.grid.to_table())\n\n        if self.n_jobs &gt; 1:\n            # Only create the shared memory object if the number of processes is more than 1.\n            # Otherwise, this would be a waste of resources.\n\n            # ----------------- Copy the LD data to shared memory -----------------\n            ld_data_arr = self.model.ld_data[self.model.chromosomes[0]]\n            # Create a shared memory block for the array\n            shm = shared_memory.SharedMemory(create=True, size=ld_data_arr.nbytes)\n\n            # Create a NumPy array backed by the shared memory block\n            shared_array = np.ndarray(ld_data_arr.shape, dtype=ld_data_arr.dtype, buffer=shm.buf)\n\n            np.copyto(shared_array, ld_data_arr)\n\n            del ld_data_arr\n            self.model.ld_data = None\n\n            shm_args = {\n                'shm_name': shm.name,\n                'chromosome': self.model.chromosomes[0],\n                'shm_shape': shared_array.shape,\n                'shm_dtype': shared_array.dtype\n            }\n\n        else:\n            shm_args = None\n\n        # --------------------------------------------------------------------\n        # Perform grid search:\n\n        grid = self.grid.combine_grids()\n\n        parallel = Parallel(n_jobs=self.n_jobs, backend='multiprocessing')\n\n        with parallel:\n\n            fitted_models = parallel(\n                delayed(fit_model_fixed_params)(self.model, g, shm_args,\n                                                max_iter=max_iter,\n                                                f_abs_tol=f_abs_tol,\n                                                x_abs_tol=x_abs_tol)\n                for g in grid\n            )\n\n        # Clean up after performing model fit:\n        self.model.ld_data = None  # To minimize memory usage with validation/pseudo-validation\n\n        # Close and unlink shared memory objects:\n        if shm_args is not None:\n            shm.close()\n            shm.unlink()\n\n        # --------------------------------------------------------------------\n        # Post-process the results and determine the best model:\n\n        assert not all([fm is None for fm in fitted_models]), \"None of the models converged successfully.\"\n\n        # 1) Extract the data from the trained models:\n        from viprs.utils.compute_utils import combine_coefficient_tables\n\n        self._model_coefs = combine_coefficient_tables([fm['coef_table'] for fm in fitted_models if fm is not None])\n        self._model_hyperparams = [fm['hyp_table'] for fm in fitted_models if fm is not None]\n        self._training_objective = [fm['training_objective'] for fm in fitted_models if fm is not None]\n\n        # 2) Perform evaluation on the models that converged:\n        eval_metrics = self._evaluate_models()\n\n        # 3) Combine all the results together into a single table (populate records in\n        # self.validation_result):\n\n        self.validation_result = []\n        success_counter = 0\n\n        for i, vr in enumerate(grid):\n            if fitted_models[i] is not None:\n                vr['Converged'] = True\n                vr['training_objective'] = self._training_objective[success_counter]\n                if self.criterion != 'training_objective':\n                    vr[self.criterion] = eval_metrics[success_counter]\n                success_counter += 1\n            else:\n                vr['Converged'] = False\n                vr['training_objective'] = np.NaN\n                if self.criterion != 'training_objective':\n                    vr[self.criterion] = np.NaN\n\n            self.validation_result.append(vr)\n\n        # --------------------------------------------------------------------\n        # Determine and return the best model:\n\n        best_idx = np.argmax(self.to_validation_table()[self.criterion].values)\n\n        logger.info(\"&gt; Grid search identified the best hyperparameters as:\")\n        logger.info(grid[best_idx])\n\n        self.model.fix_params = grid[best_idx]\n        self.model.initialize()\n        self.model.set_model_parameters(fitted_models[best_idx]['coef_table'])\n\n        return self.model\n</code></pre>"},{"location":"api/model/gridsearch/HyperparameterSearch/#viprs.model.gridsearch.HyperparameterSearch.GridSearch.__init__","title":"<code>__init__(gdl, grid, model=None, criterion='training_objective', validation_gdl=None, n_jobs=1)</code>","text":"<p>Perform hyperparameter search using grid search</p> <p>Parameters:</p> Name Type Description Default <code>gdl</code> <p>A GWADataLoader object containing the GWAS summary statistics for inference.</p> required <code>model</code> <p>An instance of the PRS model to use for the hyperparameter search. By default, we use <code>VIPRS</code>.</p> <code>None</code> <code>criterion</code> <p>The objective function for the hyperparameter search. Options are: <code>training_objective</code>, <code>pseudo_validation</code> or <code>validation</code>. In the case of <code>VIPRS</code>, the training objective is the ELBO.</p> <code>'training_objective'</code> <code>validation_gdl</code> <p>If the objective is validation or pseudo-validation, provide the GWADataLoader object for the validation dataset. If the criterion is pseudo-validation, the <code>validation_gdl</code> should contain summary statistics from a held-out test set. If the criterion is validation, <code>validation_gdl</code> should contain individual-level data from a held-out test set.</p> <code>None</code> <code>n_jobs</code> <p>The number of processes to use for the hyperparameters search.</p> <code>1</code> Source code in <code>viprs/model/gridsearch/HyperparameterSearch.py</code> <pre><code>def __init__(self,\n             gdl,\n             grid,\n             model=None,\n             criterion='training_objective',\n             validation_gdl=None,\n             n_jobs=1):\n\n    \"\"\"\n    Perform hyperparameter search using grid search\n    :param gdl: A GWADataLoader object containing the GWAS summary statistics for inference.\n    :param model: An instance of the PRS model to use for the hyperparameter search. By default,\n    we use `VIPRS`.\n    :param criterion: The objective function for the hyperparameter search.\n    Options are: `training_objective`, `pseudo_validation` or `validation`. In the case of `VIPRS`, the training\n    objective is the ELBO.\n    :param validation_gdl: If the objective is validation or pseudo-validation, provide the GWADataLoader\n    object for the validation dataset. If the criterion is pseudo-validation, the `validation_gdl` should\n    contain summary statistics from a held-out test set. If the criterion is validation, `validation_gdl` should\n    contain individual-level data from a held-out test set.\n    :param n_jobs: The number of processes to use for the hyperparameters search.\n    \"\"\"\n\n    super().__init__(gdl,\n                     model=model,\n                     criterion=criterion,\n                     validation_gdl=validation_gdl,\n                     n_jobs=n_jobs)\n\n    self.grid = grid\n    self.model.threads = 1\n</code></pre>"},{"location":"api/model/gridsearch/HyperparameterSearch/#viprs.model.gridsearch.HyperparameterSearch.GridSearch.fit","title":"<code>fit(max_iter=1000, f_abs_tol=1e-06, x_abs_tol=1e-06)</code>","text":"<p>Perform grid search over the hyperparameters to determine the best model based on the criterion set by the user. This utility method performs model fitting across the grid of hyperparameters, potentially in parallel if <code>n_jobs</code> is greater than 1.</p> <p>Parameters:</p> Name Type Description Default <code>max_iter</code> <p>The maximum number of iterations to run for each model fit.</p> <code>1000</code> <code>f_abs_tol</code> <p>The absolute tolerance for the function convergence criterion.</p> <code>1e-06</code> <code>x_abs_tol</code> <p>The absolute tolerance for the parameter convergence criterion.</p> <code>1e-06</code> <p>Returns:</p> Type Description <p>The best model based on the criterion set by the user.</p> Source code in <code>viprs/model/gridsearch/HyperparameterSearch.py</code> <pre><code>def fit(self, max_iter=1000, f_abs_tol=1e-6, x_abs_tol=1e-6):\n    \"\"\"\n    Perform grid search over the hyperparameters to determine the\n    best model based on the criterion set by the user. This utility method\n    performs model fitting across the grid of hyperparameters, potentially in parallel\n    if `n_jobs` is greater than 1.\n\n    :param max_iter: The maximum number of iterations to run for each model fit.\n    :param f_abs_tol: The absolute tolerance for the function convergence criterion.\n    :param x_abs_tol: The absolute tolerance for the parameter convergence criterion.\n\n    :return: The best model based on the criterion set by the user.\n    \"\"\"\n\n    logger.info(\"&gt; Performing Grid Search over the following grid:\")\n    logger.info(self.grid.to_table())\n\n    if self.n_jobs &gt; 1:\n        # Only create the shared memory object if the number of processes is more than 1.\n        # Otherwise, this would be a waste of resources.\n\n        # ----------------- Copy the LD data to shared memory -----------------\n        ld_data_arr = self.model.ld_data[self.model.chromosomes[0]]\n        # Create a shared memory block for the array\n        shm = shared_memory.SharedMemory(create=True, size=ld_data_arr.nbytes)\n\n        # Create a NumPy array backed by the shared memory block\n        shared_array = np.ndarray(ld_data_arr.shape, dtype=ld_data_arr.dtype, buffer=shm.buf)\n\n        np.copyto(shared_array, ld_data_arr)\n\n        del ld_data_arr\n        self.model.ld_data = None\n\n        shm_args = {\n            'shm_name': shm.name,\n            'chromosome': self.model.chromosomes[0],\n            'shm_shape': shared_array.shape,\n            'shm_dtype': shared_array.dtype\n        }\n\n    else:\n        shm_args = None\n\n    # --------------------------------------------------------------------\n    # Perform grid search:\n\n    grid = self.grid.combine_grids()\n\n    parallel = Parallel(n_jobs=self.n_jobs, backend='multiprocessing')\n\n    with parallel:\n\n        fitted_models = parallel(\n            delayed(fit_model_fixed_params)(self.model, g, shm_args,\n                                            max_iter=max_iter,\n                                            f_abs_tol=f_abs_tol,\n                                            x_abs_tol=x_abs_tol)\n            for g in grid\n        )\n\n    # Clean up after performing model fit:\n    self.model.ld_data = None  # To minimize memory usage with validation/pseudo-validation\n\n    # Close and unlink shared memory objects:\n    if shm_args is not None:\n        shm.close()\n        shm.unlink()\n\n    # --------------------------------------------------------------------\n    # Post-process the results and determine the best model:\n\n    assert not all([fm is None for fm in fitted_models]), \"None of the models converged successfully.\"\n\n    # 1) Extract the data from the trained models:\n    from viprs.utils.compute_utils import combine_coefficient_tables\n\n    self._model_coefs = combine_coefficient_tables([fm['coef_table'] for fm in fitted_models if fm is not None])\n    self._model_hyperparams = [fm['hyp_table'] for fm in fitted_models if fm is not None]\n    self._training_objective = [fm['training_objective'] for fm in fitted_models if fm is not None]\n\n    # 2) Perform evaluation on the models that converged:\n    eval_metrics = self._evaluate_models()\n\n    # 3) Combine all the results together into a single table (populate records in\n    # self.validation_result):\n\n    self.validation_result = []\n    success_counter = 0\n\n    for i, vr in enumerate(grid):\n        if fitted_models[i] is not None:\n            vr['Converged'] = True\n            vr['training_objective'] = self._training_objective[success_counter]\n            if self.criterion != 'training_objective':\n                vr[self.criterion] = eval_metrics[success_counter]\n            success_counter += 1\n        else:\n            vr['Converged'] = False\n            vr['training_objective'] = np.NaN\n            if self.criterion != 'training_objective':\n                vr[self.criterion] = np.NaN\n\n        self.validation_result.append(vr)\n\n    # --------------------------------------------------------------------\n    # Determine and return the best model:\n\n    best_idx = np.argmax(self.to_validation_table()[self.criterion].values)\n\n    logger.info(\"&gt; Grid search identified the best hyperparameters as:\")\n    logger.info(grid[best_idx])\n\n    self.model.fix_params = grid[best_idx]\n    self.model.initialize()\n    self.model.set_model_parameters(fitted_models[best_idx]['coef_table'])\n\n    return self.model\n</code></pre>"},{"location":"api/model/gridsearch/HyperparameterSearch/#viprs.model.gridsearch.HyperparameterSearch.fit_model_fixed_params","title":"<code>fit_model_fixed_params(model, fixed_params, shm_data=None, **fit_kwargs)</code>","text":"<p>Perform model fitting using a set of fixed set of hyperparameters. This is a helper function to allow users to use the <code>multiprocessing</code> module to fit PRS models in parallel.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>A PRS model object that implements a <code>.fit()</code> method and takes <code>fix_params</code> as an attribute.</p> required <code>fixed_params</code> <p>A dictionary of fixed parameters to use for the model fitting.</p> required <code>shm_data</code> <p>A dictionary of shared memory data to use for the model fitting. This is primarily used to share LD data across multiple processes.</p> <code>None</code> <code>fit_kwargs</code> <p>Key-word arguments to pass to the <code>.fit()</code> method of the PRS model.</p> <code>{}</code> <p>Returns:</p> Type Description <p>A dictionary containing the coefficient table, hyperparameter table and the training objective. If the model did not converge successfully, return <code>None</code>.</p> Source code in <code>viprs/model/gridsearch/HyperparameterSearch.py</code> <pre><code>def fit_model_fixed_params(model, fixed_params, shm_data=None, **fit_kwargs):\n    \"\"\"\n\n    Perform model fitting using a set of fixed set of hyperparameters.\n    This is a helper function to allow users to use the `multiprocessing` module\n    to fit PRS models in parallel.\n\n    :param model: A PRS model object that implements a `.fit()` method and takes `fix_params` as an attribute.\n    :param fixed_params: A dictionary of fixed parameters to use for the model fitting.\n    :param shm_data: A dictionary of shared memory data to use for the model fitting. This is primarily used to\n    share LD data across multiple processes.\n    :param fit_kwargs: Key-word arguments to pass to the `.fit()` method of the PRS model.\n\n    :return: A dictionary containing the coefficient table, hyperparameter table and the training objective.\n    If the model did not converge successfully, return `None`.\n    \"\"\"\n\n    model.fix_params = fixed_params\n\n    if shm_data is not None:\n\n        model.ld_data = {}\n\n        try:\n            ld_data_shm = shared_memory.SharedMemory(name=shm_data['shm_name'])\n            model.ld_data[shm_data['chromosome']] = np.ndarray(\n                shape=shm_data['shm_shape'],\n                dtype=shm_data['shm_dtype'],\n                buffer=ld_data_shm.buf\n            )\n        except FileNotFoundError:\n            raise Exception(\"LD data not found in shared memory.\")\n\n    try:\n        model.fit(**fit_kwargs)\n    except Exception as e:\n        logger.warning(\"Exception encountered when fitting model:\", e)\n        model = None\n    finally:\n        if shm_data is not None:\n            ld_data_shm.close()\n            model.ld_data = None\n\n    if model is not None:\n        return {\n            'coef_table': model.to_table()[['CHR', 'SNP', 'POS', 'A1', 'A2', 'BETA']],\n            'hyp_table': model.to_theta_table(),\n            'training_objective': model.objective()\n        }\n</code></pre>"},{"location":"api/model/gridsearch/VIPRSGrid/","title":"VIPRSGrid","text":""},{"location":"api/model/gridsearch/VIPRSGrid/#viprs.model.gridsearch.VIPRSGrid.VIPRSGrid","title":"<code>VIPRSGrid</code>","text":"<p>               Bases: <code>VIPRS</code></p> <p>A class to fit the <code>VIPRS</code> model to data using a grid of hyperparameters. Instead of having a single set of hyperparameters, we simultaneously fit multiple models with different hyperparameters and compare their performance at the end. The models with different hyperparameters are fit serially and in a pathwise manner, meaning that fit one model at a time and use its inferred parameters to initialize the next model.</p> <p>The class inherits all the basic attributes from the VIPRS class.</p> <p>Attributes:</p> Name Type Description <code>grid_table</code> <p>A pandas table containing the hyperparameters for each model.</p> <code>validation_result</code> <p>A pandas table summarizing the performance of each model.</p> <code>optim_results</code> <p>A list of optimization results for each model.</p> <code>n_models</code> <p>The number of models to fit.</p> Source code in <code>viprs/model/gridsearch/VIPRSGrid.py</code> <pre><code>class VIPRSGrid(VIPRS):\n    \"\"\"\n    A class to fit the `VIPRS` model to data using a grid of hyperparameters.\n    Instead of having a single set of hyperparameters, we simultaneously fit\n    multiple models with different hyperparameters and compare their performance\n    at the end. The models with different hyperparameters are fit serially and in\n    a pathwise manner, meaning that fit one model at a time and use its inferred parameters\n    to initialize the next model.\n\n    The class inherits all the basic attributes from the [VIPRS][viprs.model.VIPRS.VIPRS] class.\n\n    :ivar grid_table: A pandas table containing the hyperparameters for each model.\n    :ivar validation_result: A pandas table summarizing the performance of each model.\n    :ivar optim_results: A list of optimization results for each model.\n    :ivar n_models: The number of models to fit.\n\n    \"\"\"\n\n    def __init__(self,\n                 gdl,\n                 grid,\n                 **kwargs):\n        \"\"\"\n        Initialize the `VIPRS` model with a grid of hyperparameters.\n\n        :param gdl: An instance of `GWADataLoader`\n        :param grid: An instance of `HyperparameterGrid`\n        :param kwargs: Additional keyword arguments to pass to the parent `VIPRS` class.\n        \"\"\"\n\n        self.grid_table = grid.to_table()\n\n        # Placeholders:\n        self.n_models = len(self.grid_table)\n        self.validation_result = None\n        self.optim_results = None\n\n        self._reset_search()\n\n        super().__init__(gdl, **kwargs)\n\n    def _reset_search(self):\n        \"\"\"\n        Reset the grid search object. This might be useful after\n        fitting the model and performing model selection/BMA, to start over.\n        \"\"\"\n        self.n_models = len(self.grid_table)\n        assert self.n_models &gt; 1, \"Grid search requires at least 2 models.\"\n        self.validation_result = None\n        self.optim_results = []\n\n    @property\n    def models_to_keep(self):\n        \"\"\"\n        :return: A boolean array indicating which models have converged successfully.\n        \"\"\"\n        return np.logical_or(~self.terminated_models, self.converged_models)\n\n    @property\n    def converged_models(self):\n        \"\"\"\n        :return: A boolean array indicating which models have converged successfully.\n        \"\"\"\n        return np.array([optr.success for optr in self.optim_results])\n\n    @property\n    def terminated_models(self):\n        \"\"\"\n        :return: A boolean array indicating which models have terminated.\n        \"\"\"\n        return np.array([optr.stop_iteration for optr in self.optim_results])\n\n    @property\n    def valid_terminated_models(self):\n        \"\"\"\n        :return: A boolean array indicating which models have terminated without error.\n        \"\"\"\n        return np.array([optr.valid_optim_result for optr in self.optim_results])\n\n    def to_validation_table(self):\n        \"\"\"\n        :return: The validation table summarizing the performance of each model.\n        :raises ValueError: if the validation result is not set.\n        \"\"\"\n\n        if self.validation_result is None or len(self.validation_result) &lt; 1:\n            raise ValueError(\"Validation result is not set!\")\n\n        return pd.DataFrame(self.validation_result)\n\n    def write_validation_result(self, v_filename, sep=\"\\t\"):\n        \"\"\"\n        After performing hyperparameter search, write a table\n        that records that value of the objective for each combination\n        of hyperparameters.\n        :param v_filename: The filename for the validation table.\n        :param sep: The separator for the validation table\n        \"\"\"\n\n        v_df = self.to_validation_table()\n        v_df.to_csv(v_filename, index=False, sep=sep)\n\n    def init_optim_meta(self):\n        \"\"\"\n        Initialize the various quantities/objects to keep track of the optimization process.\n         This method initializes the \"history\" object (which keeps track of the objective + other\n         hyperparameters requested by the user), in addition to the OptimizeResult objects.\n        \"\"\"\n        super().init_optim_meta()\n\n        # Reset the OptimizeResult objects:\n        self.optim_results = []\n\n    def fit(self,\n            pathwise=True,\n            **fit_kwargs):\n        \"\"\"\n        Fit the VIPRS model to the data using a grid of hyperparameters.\n        The method fits multiple models with different hyperparameters and compares their performance\n        at the end. By default, the models with different hyperparameters are fit serially and\n        in a pathwise manner, meaning that fit one model at a time and use its inferred\n        parameters to initialize the next model. The user can also fit the models independently by\n        setting `pathwise=False`.\n\n        :param pathwise: Whether to fit the models in a pathwise manner. Default is `True`.\n        :param fit_kwargs: Additional keyword arguments to pass to fit method of the parent `VIPRS` class.\n\n        :return: An instance of the `VIPRSGrid` class.\n        \"\"\"\n\n        if self.n_models == 1:\n            return super().fit(**fit_kwargs)\n\n        # -----------------------------------------------------------------------\n        # Setup the parameters that need to be tracked:\n\n        var_gamma = {c: np.empty((size, self.n_models), dtype=self.float_precision)\n                     for c, size in self.shapes.items()}\n        var_mu = {c: np.empty((size, self.n_models), dtype=self.float_precision)\n                  for c, size in self.shapes.items()}\n        var_tau = {c: np.empty((size, self.n_models), dtype=self.float_precision)\n                   for c, size in self.shapes.items()}\n        q = {c: np.empty((size, self.n_models), dtype=self.float_precision)\n             for c, size in self.shapes.items()}\n\n        sigma_epsilon = np.empty(self.n_models, dtype=self.float_precision)\n        pi = np.empty(self.n_models, dtype=self.float_precision)\n        sigma_g = np.empty(self.n_models, dtype=self.float_precision)\n        tau_beta = np.empty(self.n_models, dtype=self.float_precision)\n\n        elbos = np.empty(self.n_models, dtype=self.float_precision)\n\n        # -----------------------------------------------------------------------\n\n        # Get a list of fixed hyperparameters from the grid table:\n        params = self.grid_table.to_dict(orient='records')\n        orig_threads = self.threads\n        optim_results = []\n        history = []\n\n        # If the model is fit over a single chromosome, append this information to the\n        # tqdm progress bar:\n        if len(self.shapes) == 1:\n            chrom, num_snps = list(self.shapes.items())[0]\n            desc = f\"Grid search | Chromosome {chrom} ({num_snps} variants)\"\n        else:\n            desc = None\n\n        disable_pbar = fit_kwargs.pop('disable_pbar', False)\n        restart = not pathwise\n\n        with logging_redirect_tqdm(loggers=[logger]):\n\n            # Set up the progress bar for grid search:\n            pbar = tqdm(range(self.n_models),\n                        total=self.n_models,\n                        disable=disable_pbar,\n                        desc=desc)\n\n            for i in pbar:\n\n                # Fix the new set of hyperparameters:\n                self.set_fixed_params(params[i])\n\n                # Perform model fit:\n                super().fit(continued=i &gt; 0 and not restart,\n                            disable_pbar=True,\n                            **fit_kwargs)\n\n                # Save the optimization result:\n                optim_results.append(copy.deepcopy(self.optim_result))\n                # Reset the optimization result:\n                self.optim_result.reset()\n                self.threads = orig_threads\n\n                elbos[i] = self.history['ELBO'][-1]\n\n                pbar.set_postfix({'ELBO': f\"{self.history['ELBO'][-1]:.4f}\",\n                                  'Models Terminated': f\"{i+1}/{self.n_models}\"})\n\n                # Update the saved parameters:\n                for c in self.shapes:\n                    var_gamma[c][:, i] = self.var_gamma[c]\n                    var_mu[c][:, i] = self.var_mu[c]\n                    var_tau[c][:, i] = self.var_tau[c]\n                    q[c][:, i] = self.q[c]\n\n                sigma_epsilon[i] = self.sigma_epsilon\n                pi[i] = self.pi\n                sigma_g[i] = self._sigma_g\n                tau_beta[i] = self.tau_beta\n\n        # Update the total number of iterations:\n        self.optim_result.nit = np.sum([optr.nit for optr in self.optim_results])\n        self.optim_results = optim_results\n\n        # -----------------------------------------------------------------------\n        # Update the object attributes:\n        self.var_gamma = var_gamma\n        self.var_mu = var_mu\n        self.var_tau = var_tau\n        self.q = q\n        self.eta = self.compute_eta()\n        self.zeta = self.compute_zeta()\n        self._log_var_tau = {c: np.log(self.var_tau[c]) for c in self.var_tau}\n\n        # Update posterior moments:\n        self.update_posterior_moments()\n\n        # Hyperparameters:\n        self.sigma_epsilon = sigma_epsilon\n        self.pi = pi\n        self._sigma_g = sigma_g\n        self.tau_beta = tau_beta\n\n        # -----------------------------------------------------------------------\n\n        # Population the validation result:\n        self.validation_result = self.grid_table.copy()\n        self.validation_result['ELBO'] = elbos\n        self.validation_result['Converged'] = self.converged_models\n        self.validation_result['Optimization_message'] = [optr.message for optr in self.optim_results]\n\n        return self\n</code></pre>"},{"location":"api/model/gridsearch/VIPRSGrid/#viprs.model.gridsearch.VIPRSGrid.VIPRSGrid.converged_models","title":"<code>converged_models</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>A boolean array indicating which models have converged successfully.</p>"},{"location":"api/model/gridsearch/VIPRSGrid/#viprs.model.gridsearch.VIPRSGrid.VIPRSGrid.models_to_keep","title":"<code>models_to_keep</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>A boolean array indicating which models have converged successfully.</p>"},{"location":"api/model/gridsearch/VIPRSGrid/#viprs.model.gridsearch.VIPRSGrid.VIPRSGrid.terminated_models","title":"<code>terminated_models</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>A boolean array indicating which models have terminated.</p>"},{"location":"api/model/gridsearch/VIPRSGrid/#viprs.model.gridsearch.VIPRSGrid.VIPRSGrid.valid_terminated_models","title":"<code>valid_terminated_models</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>A boolean array indicating which models have terminated without error.</p>"},{"location":"api/model/gridsearch/VIPRSGrid/#viprs.model.gridsearch.VIPRSGrid.VIPRSGrid.__init__","title":"<code>__init__(gdl, grid, **kwargs)</code>","text":"<p>Initialize the <code>VIPRS</code> model with a grid of hyperparameters.</p> <p>Parameters:</p> Name Type Description Default <code>gdl</code> <p>An instance of <code>GWADataLoader</code></p> required <code>grid</code> <p>An instance of <code>HyperparameterGrid</code></p> required <code>kwargs</code> <p>Additional keyword arguments to pass to the parent <code>VIPRS</code> class.</p> <code>{}</code> Source code in <code>viprs/model/gridsearch/VIPRSGrid.py</code> <pre><code>def __init__(self,\n             gdl,\n             grid,\n             **kwargs):\n    \"\"\"\n    Initialize the `VIPRS` model with a grid of hyperparameters.\n\n    :param gdl: An instance of `GWADataLoader`\n    :param grid: An instance of `HyperparameterGrid`\n    :param kwargs: Additional keyword arguments to pass to the parent `VIPRS` class.\n    \"\"\"\n\n    self.grid_table = grid.to_table()\n\n    # Placeholders:\n    self.n_models = len(self.grid_table)\n    self.validation_result = None\n    self.optim_results = None\n\n    self._reset_search()\n\n    super().__init__(gdl, **kwargs)\n</code></pre>"},{"location":"api/model/gridsearch/VIPRSGrid/#viprs.model.gridsearch.VIPRSGrid.VIPRSGrid.fit","title":"<code>fit(pathwise=True, **fit_kwargs)</code>","text":"<p>Fit the VIPRS model to the data using a grid of hyperparameters. The method fits multiple models with different hyperparameters and compares their performance at the end. By default, the models with different hyperparameters are fit serially and in a pathwise manner, meaning that fit one model at a time and use its inferred parameters to initialize the next model. The user can also fit the models independently by setting <code>pathwise=False</code>.</p> <p>Parameters:</p> Name Type Description Default <code>pathwise</code> <p>Whether to fit the models in a pathwise manner. Default is <code>True</code>.</p> <code>True</code> <code>fit_kwargs</code> <p>Additional keyword arguments to pass to fit method of the parent <code>VIPRS</code> class.</p> <code>{}</code> <p>Returns:</p> Type Description <p>An instance of the <code>VIPRSGrid</code> class.</p> Source code in <code>viprs/model/gridsearch/VIPRSGrid.py</code> <pre><code>def fit(self,\n        pathwise=True,\n        **fit_kwargs):\n    \"\"\"\n    Fit the VIPRS model to the data using a grid of hyperparameters.\n    The method fits multiple models with different hyperparameters and compares their performance\n    at the end. By default, the models with different hyperparameters are fit serially and\n    in a pathwise manner, meaning that fit one model at a time and use its inferred\n    parameters to initialize the next model. The user can also fit the models independently by\n    setting `pathwise=False`.\n\n    :param pathwise: Whether to fit the models in a pathwise manner. Default is `True`.\n    :param fit_kwargs: Additional keyword arguments to pass to fit method of the parent `VIPRS` class.\n\n    :return: An instance of the `VIPRSGrid` class.\n    \"\"\"\n\n    if self.n_models == 1:\n        return super().fit(**fit_kwargs)\n\n    # -----------------------------------------------------------------------\n    # Setup the parameters that need to be tracked:\n\n    var_gamma = {c: np.empty((size, self.n_models), dtype=self.float_precision)\n                 for c, size in self.shapes.items()}\n    var_mu = {c: np.empty((size, self.n_models), dtype=self.float_precision)\n              for c, size in self.shapes.items()}\n    var_tau = {c: np.empty((size, self.n_models), dtype=self.float_precision)\n               for c, size in self.shapes.items()}\n    q = {c: np.empty((size, self.n_models), dtype=self.float_precision)\n         for c, size in self.shapes.items()}\n\n    sigma_epsilon = np.empty(self.n_models, dtype=self.float_precision)\n    pi = np.empty(self.n_models, dtype=self.float_precision)\n    sigma_g = np.empty(self.n_models, dtype=self.float_precision)\n    tau_beta = np.empty(self.n_models, dtype=self.float_precision)\n\n    elbos = np.empty(self.n_models, dtype=self.float_precision)\n\n    # -----------------------------------------------------------------------\n\n    # Get a list of fixed hyperparameters from the grid table:\n    params = self.grid_table.to_dict(orient='records')\n    orig_threads = self.threads\n    optim_results = []\n    history = []\n\n    # If the model is fit over a single chromosome, append this information to the\n    # tqdm progress bar:\n    if len(self.shapes) == 1:\n        chrom, num_snps = list(self.shapes.items())[0]\n        desc = f\"Grid search | Chromosome {chrom} ({num_snps} variants)\"\n    else:\n        desc = None\n\n    disable_pbar = fit_kwargs.pop('disable_pbar', False)\n    restart = not pathwise\n\n    with logging_redirect_tqdm(loggers=[logger]):\n\n        # Set up the progress bar for grid search:\n        pbar = tqdm(range(self.n_models),\n                    total=self.n_models,\n                    disable=disable_pbar,\n                    desc=desc)\n\n        for i in pbar:\n\n            # Fix the new set of hyperparameters:\n            self.set_fixed_params(params[i])\n\n            # Perform model fit:\n            super().fit(continued=i &gt; 0 and not restart,\n                        disable_pbar=True,\n                        **fit_kwargs)\n\n            # Save the optimization result:\n            optim_results.append(copy.deepcopy(self.optim_result))\n            # Reset the optimization result:\n            self.optim_result.reset()\n            self.threads = orig_threads\n\n            elbos[i] = self.history['ELBO'][-1]\n\n            pbar.set_postfix({'ELBO': f\"{self.history['ELBO'][-1]:.4f}\",\n                              'Models Terminated': f\"{i+1}/{self.n_models}\"})\n\n            # Update the saved parameters:\n            for c in self.shapes:\n                var_gamma[c][:, i] = self.var_gamma[c]\n                var_mu[c][:, i] = self.var_mu[c]\n                var_tau[c][:, i] = self.var_tau[c]\n                q[c][:, i] = self.q[c]\n\n            sigma_epsilon[i] = self.sigma_epsilon\n            pi[i] = self.pi\n            sigma_g[i] = self._sigma_g\n            tau_beta[i] = self.tau_beta\n\n    # Update the total number of iterations:\n    self.optim_result.nit = np.sum([optr.nit for optr in self.optim_results])\n    self.optim_results = optim_results\n\n    # -----------------------------------------------------------------------\n    # Update the object attributes:\n    self.var_gamma = var_gamma\n    self.var_mu = var_mu\n    self.var_tau = var_tau\n    self.q = q\n    self.eta = self.compute_eta()\n    self.zeta = self.compute_zeta()\n    self._log_var_tau = {c: np.log(self.var_tau[c]) for c in self.var_tau}\n\n    # Update posterior moments:\n    self.update_posterior_moments()\n\n    # Hyperparameters:\n    self.sigma_epsilon = sigma_epsilon\n    self.pi = pi\n    self._sigma_g = sigma_g\n    self.tau_beta = tau_beta\n\n    # -----------------------------------------------------------------------\n\n    # Population the validation result:\n    self.validation_result = self.grid_table.copy()\n    self.validation_result['ELBO'] = elbos\n    self.validation_result['Converged'] = self.converged_models\n    self.validation_result['Optimization_message'] = [optr.message for optr in self.optim_results]\n\n    return self\n</code></pre>"},{"location":"api/model/gridsearch/VIPRSGrid/#viprs.model.gridsearch.VIPRSGrid.VIPRSGrid.init_optim_meta","title":"<code>init_optim_meta()</code>","text":"<p>Initialize the various quantities/objects to keep track of the optimization process.  This method initializes the \"history\" object (which keeps track of the objective + other  hyperparameters requested by the user), in addition to the OptimizeResult objects.</p> Source code in <code>viprs/model/gridsearch/VIPRSGrid.py</code> <pre><code>def init_optim_meta(self):\n    \"\"\"\n    Initialize the various quantities/objects to keep track of the optimization process.\n     This method initializes the \"history\" object (which keeps track of the objective + other\n     hyperparameters requested by the user), in addition to the OptimizeResult objects.\n    \"\"\"\n    super().init_optim_meta()\n\n    # Reset the OptimizeResult objects:\n    self.optim_results = []\n</code></pre>"},{"location":"api/model/gridsearch/VIPRSGrid/#viprs.model.gridsearch.VIPRSGrid.VIPRSGrid.to_validation_table","title":"<code>to_validation_table()</code>","text":"<p>Returns:</p> Type Description <p>The validation table summarizing the performance of each model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the validation result is not set.</p> Source code in <code>viprs/model/gridsearch/VIPRSGrid.py</code> <pre><code>def to_validation_table(self):\n    \"\"\"\n    :return: The validation table summarizing the performance of each model.\n    :raises ValueError: if the validation result is not set.\n    \"\"\"\n\n    if self.validation_result is None or len(self.validation_result) &lt; 1:\n        raise ValueError(\"Validation result is not set!\")\n\n    return pd.DataFrame(self.validation_result)\n</code></pre>"},{"location":"api/model/gridsearch/VIPRSGrid/#viprs.model.gridsearch.VIPRSGrid.VIPRSGrid.write_validation_result","title":"<code>write_validation_result(v_filename, sep='\\t')</code>","text":"<p>After performing hyperparameter search, write a table that records that value of the objective for each combination of hyperparameters.</p> <p>Parameters:</p> Name Type Description Default <code>v_filename</code> <p>The filename for the validation table.</p> required <code>sep</code> <p>The separator for the validation table</p> <code>'\\t'</code> Source code in <code>viprs/model/gridsearch/VIPRSGrid.py</code> <pre><code>def write_validation_result(self, v_filename, sep=\"\\t\"):\n    \"\"\"\n    After performing hyperparameter search, write a table\n    that records that value of the objective for each combination\n    of hyperparameters.\n    :param v_filename: The filename for the validation table.\n    :param sep: The separator for the validation table\n    \"\"\"\n\n    v_df = self.to_validation_table()\n    v_df.to_csv(v_filename, index=False, sep=sep)\n</code></pre>"},{"location":"api/model/gridsearch/grid_utils/","title":"Grid utils","text":""},{"location":"api/model/gridsearch/grid_utils/#viprs.model.gridsearch.grid_utils.bayesian_model_average","title":"<code>bayesian_model_average(viprs_grid_model, normalization='softmax')</code>","text":"<p>Use Bayesian model averaging (BMA) to obtain a weighing scheme for the  variational parameters of a grid of VIPRS models. The parameters of each model in the grid  are assigned weights proportional to their final ELBO.</p> <p>Parameters:</p> Name Type Description Default <code>viprs_grid_model</code> <p>An instance of <code>VIPRSGrid</code> or <code>VIPRSGridPathwise</code> containing the fitted grid of VIPRS models.</p> required <code>normalization</code> <p>The normalization scheme for the final ELBOs. Options are (<code>softmax</code>, <code>sum</code>).</p> <code>'softmax'</code> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the normalization scheme is not recognized.</p> Source code in <code>viprs/model/gridsearch/grid_utils.py</code> <pre><code>def bayesian_model_average(viprs_grid_model, normalization='softmax'):\n    \"\"\"\n    Use Bayesian model averaging (BMA) to obtain a weighing scheme for the\n     variational parameters of a grid of VIPRS models. The parameters of each model in the grid\n     are assigned weights proportional to their final ELBO.\n\n    :param viprs_grid_model: An instance of `VIPRSGrid` or `VIPRSGridPathwise` containing the fitted grid\n    of VIPRS models.\n    :param normalization: The normalization scheme for the final ELBOs.\n    Options are (`softmax`, `sum`).\n    :raises KeyError: If the normalization scheme is not recognized.\n    \"\"\"\n\n    if viprs_grid_model.n_models &lt; 2:\n        return viprs_grid_model\n\n    if np.sum(viprs_grid_model.valid_terminated_models) &lt; 1:\n        raise ValueError(\"No models converged successfully. \"\n                         \"Cannot average models.\")\n\n    # Extract the models that converged successfully:\n    models_to_keep = np.where(viprs_grid_model.valid_terminated_models)[0]\n\n    elbos = viprs_grid_model.elbo()\n\n    if normalization == 'softmax':\n        from scipy.special import softmax\n        weights = np.array(softmax(elbos))\n    elif normalization == 'sum':\n        weights = np.array(elbos)\n\n        # Correction for negative ELBOs:\n        weights = weights - weights.min() + 1.\n        weights /= weights.sum()\n    else:\n        raise KeyError(\"Normalization scheme not recognized. \"\n                       \"Valid options are: `softmax`, `sum`. \"\n                       \"Got: {}\".format(normalization))\n\n    logger.info(\"Averaging PRS models with weights:\", weights)\n\n    # Average the model parameters:\n    for param in (viprs_grid_model.var_gamma, viprs_grid_model.var_mu, viprs_grid_model.var_tau,\n                  viprs_grid_model.q):\n        for c in param:\n            param[c] = (param[c][:, models_to_keep] * weights).sum(axis=1)\n\n    viprs_grid_model.eta = viprs_grid_model.compute_eta()\n    viprs_grid_model.zeta = viprs_grid_model.compute_zeta()\n\n    # Update posterior moments:\n    viprs_grid_model.update_posterior_moments()\n\n    # Update the log of the variational tau parameters:\n    viprs_grid_model._log_var_tau = {c: np.log(viprs_grid_model.var_tau[c])\n                                     for c in viprs_grid_model.var_tau}\n\n    # Update the hyperparameters based on the averaged weights\n    import copy\n    # TODO: double check to make sure this makes sense.\n    fix_params_before = copy.deepcopy(viprs_grid_model.fix_params)\n    viprs_grid_model.fix_params = {}\n    viprs_grid_model.m_step()\n    viprs_grid_model.fix_params = fix_params_before\n\n    # -----------------------------------------------------------------------\n\n    # Set the number of models to 1:\n    viprs_grid_model.n_models = 1\n\n    # -----------------------------------------------------------------------\n\n    return viprs_grid_model\n</code></pre>"},{"location":"api/model/gridsearch/grid_utils/#viprs.model.gridsearch.grid_utils.select_best_model","title":"<code>select_best_model(viprs_grid_model, validation_gdl=None, criterion='ELBO')</code>","text":"<p>From the grid of models that were fit to the data, select the best model according to the specified <code>criterion</code>. If the criterion is the ELBO, the model with the highest ELBO will be selected. If the criterion is validation or pseudo-validation, the model with the highest R^2 on the held-out validation set will be selected.</p> <p>Parameters:</p> Name Type Description Default <code>viprs_grid_model</code> <p>An instance of <code>VIPRSGrid</code> or <code>VIPRSGridPathwise</code> containing the fitted grid of VIPRS models.</p> required <code>validation_gdl</code> <p>An instance of <code>GWADataLoader</code> containing data from the validation set.</p> <code>None</code> <code>criterion</code> <p>The criterion for selecting the best model. Options are: (<code>ELBO</code>, <code>validation</code>, <code>pseudo_validation</code>)</p> <code>'ELBO'</code> Source code in <code>viprs/model/gridsearch/grid_utils.py</code> <pre><code>def select_best_model(viprs_grid_model, validation_gdl=None, criterion='ELBO'):\n    \"\"\"\n    From the grid of models that were fit to the data, select the best\n    model according to the specified `criterion`. If the criterion is the ELBO,\n    the model with the highest ELBO will be selected. If the criterion is\n    validation or pseudo-validation, the model with the highest R^2 on the\n    held-out validation set will be selected.\n\n    :param viprs_grid_model: An instance of `VIPRSGrid` or `VIPRSGridPathwise` containing the fitted grid\n    of VIPRS models.\n    :param validation_gdl: An instance of `GWADataLoader` containing data from the validation set.\n    :param criterion: The criterion for selecting the best model.\n    Options are: (`ELBO`, `validation`, `pseudo_validation`)\n    \"\"\"\n\n    assert criterion in ('ELBO', 'validation', 'pseudo_validation')\n\n    if criterion == 'validation':\n        assert validation_gdl is not None, \"Validation GWADataLoader must be provided for validation criterion.\"\n    elif criterion == 'pseudo_validation' and validation_gdl is None and viprs_grid_model.validation_std_beta is None:\n        raise ValueError(\"Validation GWADataLoader or standardized betas from a validation set must be \"\n                         \"initialized for the pseudo_validation criterion.\")\n\n    # Extract the models that converged successfully:\n    models_converged = viprs_grid_model.valid_terminated_models\n    best_model_idx = None\n\n    if np.sum(models_converged) &lt; 2:\n        raise ValueError(\"Less than two models converged successfully. Cannot perform model selection.\")\n    else:\n\n        if criterion == 'ELBO':\n            elbo = viprs_grid_model.elbo()\n            elbo[~models_converged] = -np.inf\n            best_model_idx = np.argmax(elbo)\n        elif criterion == 'validation':\n\n            assert validation_gdl is not None\n            assert validation_gdl.sample_table is not None\n            assert validation_gdl.sample_table.phenotype is not None\n\n            from viprs.eval.continuous_metrics import r2\n\n            prs = viprs_grid_model.predict(test_gdl=validation_gdl)\n            prs_r2 = np.array([r2(prs[:, i], validation_gdl.sample_table.phenotype)\n                               for i in range(viprs_grid_model.n_models)])\n            prs_r2[~models_converged] = -np.inf\n            viprs_grid_model.validation_result['Validation_R2'] = prs_r2\n            best_model_idx = np.argmax(prs_r2)\n        elif criterion == 'pseudo_validation':\n\n            pseudo_r2 = viprs_grid_model.pseudo_validate(validation_gdl)\n            pseudo_r2[~models_converged] = -np.inf\n            viprs_grid_model.validation_result['Pseudo_Validation_R2'] = pseudo_r2\n            best_model_idx = np.argmax(np.nan_to_num(pseudo_r2, nan=0., neginf=0., posinf=0.))\n\n    logger.info(f\"&gt; Based on the {criterion} criterion, selected model: {best_model_idx}\")\n    logger.info(\"&gt; Model details:\\n\")\n    logger.info(viprs_grid_model.validation_result.iloc[best_model_idx, :])\n\n    # -----------------------------------------------------------------------\n    # Update the variational parameters and their dependencies to only select the best model:\n    for param in (viprs_grid_model.pip, viprs_grid_model.post_mean_beta, viprs_grid_model.post_var_beta,\n                  viprs_grid_model.var_gamma, viprs_grid_model.var_mu, viprs_grid_model.var_tau,\n                  viprs_grid_model.eta, viprs_grid_model.zeta, viprs_grid_model.q,\n                  viprs_grid_model._log_var_tau):\n        for c in param:\n            param[c] = param[c][:, best_model_idx]\n\n    # Update the eta diff:\n    try:\n        for c in viprs_grid_model.eta_diff:\n            viprs_grid_model.eta_diff[c] = viprs_grid_model.eta_diff[c][:, best_model_idx]\n    except IndexError:\n        # Don't need to update this for the VIPRSGridPathwise model.\n        pass\n\n    # Update sigma_epsilon:\n    viprs_grid_model.sigma_epsilon = viprs_grid_model.sigma_epsilon[best_model_idx]\n\n    # Update sigma_g:\n    viprs_grid_model._sigma_g = viprs_grid_model._sigma_g[best_model_idx]\n\n    # Update sigma beta:\n    if isinstance(viprs_grid_model.tau_beta, dict):\n        for c in viprs_grid_model.tau_beta:\n            viprs_grid_model.tau_beta[c] = viprs_grid_model.tau_beta[c][:, best_model_idx]\n    else:\n        viprs_grid_model.tau_beta = viprs_grid_model.tau_beta[best_model_idx]\n\n    # Update pi\n\n    if isinstance(viprs_grid_model.pi, dict):\n        for c in viprs_grid_model.pi:\n            viprs_grid_model.pi[c] = viprs_grid_model.pi[c][:, best_model_idx]\n    else:\n        viprs_grid_model.pi = viprs_grid_model.pi[best_model_idx]\n\n    # -----------------------------------------------------------------------\n\n    # Set the number of models to 1:\n    viprs_grid_model.n_models = 1\n\n    # Update the fixed parameters of the model:\n    viprs_grid_model.set_fixed_params(\n        viprs_grid_model.grid_table.iloc[best_model_idx].to_dict()\n    )\n\n    # -----------------------------------------------------------------------\n\n    return viprs_grid_model\n</code></pre>"},{"location":"api/plot/diagnostics/","title":"Diagnostics","text":""},{"location":"api/plot/diagnostics/#viprs.plot.diagnostics.plot_history","title":"<code>plot_history(prs_model, quantity=None)</code>","text":"<p>This function plots the optimization history for various model parameters and/or objectives. For every iteration step, we generally save quantities such as the ELBO, the heritability, etc. For the purposes of debugging and checking model convergence, it is useful to visually observe the trajectory of these quantities as a function of training iteration.</p> <p>Parameters:</p> Name Type Description Default <code>prs_model</code> <p>A <code>VIPRS</code> (or its derived classes) object.</p> required <code>quantity</code> <p>The quantities to plot (e.g. <code>ELBO</code>, <code>heritability</code>, etc.).</p> <code>None</code> <p>Returns:</p> Type Description <p>A seaborn <code>FacetGrid</code> object containing the plots.</p> Source code in <code>viprs/plot/diagnostics.py</code> <pre><code>def plot_history(prs_model, quantity=None):\n    \"\"\"\n    This function plots the optimization history for various model parameters and/or objectives. For\n    every iteration step, we generally save quantities such as the ELBO, the heritability, etc. For the purposes\n    of debugging and checking model convergence, it is useful to visually observe the trajectory\n    of these quantities as a function of training iteration.\n\n    :param prs_model: A `VIPRS` (or its derived classes) object.\n    :param quantity: The quantities to plot (e.g. `ELBO`, `heritability`, etc.).\n\n    :return: A seaborn `FacetGrid` object containing the plots.\n    \"\"\"\n\n    if quantity is None:\n        quantity = prs_model.history.keys()\n    elif isinstance(quantity, str):\n        quantity = [quantity]\n\n    q_dfs = []\n\n    for attr in quantity:\n\n        df = pd.DataFrame({'Value': prs_model.history[attr]})\n        df.reset_index(inplace=True)\n        df.columns = ['Step', 'Value']\n        df['Quantity'] = attr\n\n        q_dfs.append(df)\n\n    q_dfs = pd.concat(q_dfs)\n\n    g = sns.relplot(\n        data=q_dfs, x=\"Step\", y=\"Value\",\n        row=\"Quantity\",\n        facet_kws={'sharey': False, 'sharex': True},\n        kind=\"scatter\",\n        marker=\".\"\n    )\n\n    return g\n</code></pre>"},{"location":"api/utils/OptimizeResult/","title":"OptimizeResult","text":""},{"location":"api/utils/OptimizeResult/#viprs.utils.OptimizeResult.IterationConditionCounter","title":"<code>IterationConditionCounter</code>","text":"<p>               Bases: <code>object</code></p> <p>A class to keep track of the number of (consecutive) iterations that a condition has been met.</p> <p>Attributes:</p> Name Type Description <code>_counter</code> <p>The number of consecutive iterations that the condition has been met.</p> <code>_nit</code> <p>The current iteration number.</p> Source code in <code>viprs/utils/OptimizeResult.py</code> <pre><code>class IterationConditionCounter(object):\n    \"\"\"\n    A class to keep track of the number of (consecutive) iterations that a condition has been met.\n\n    :ivar _counter: The number of consecutive iterations that the condition has been met.\n    :ivar _nit: The current iteration number.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize the counter.\n        \"\"\"\n        self._counter = 0\n        self._nit = 0\n\n    @property\n    def counter(self):\n        \"\"\"\n        :return: The number of consecutive iterations that the condition has been met.\n        \"\"\"\n        return self._counter\n\n    def update(self, condition, iteration):\n        \"\"\"\n        Update the counter based on the condition.\n        :param condition: The condition to check\n        :param iteration: The current iteration\n        \"\"\"\n        if condition and (iteration == self._nit + 1):\n            self._counter += 1\n        else:\n            self._counter = 0\n\n        self._nit = iteration\n</code></pre>"},{"location":"api/utils/OptimizeResult/#viprs.utils.OptimizeResult.IterationConditionCounter.counter","title":"<code>counter</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The number of consecutive iterations that the condition has been met.</p>"},{"location":"api/utils/OptimizeResult/#viprs.utils.OptimizeResult.IterationConditionCounter.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the counter.</p> Source code in <code>viprs/utils/OptimizeResult.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initialize the counter.\n    \"\"\"\n    self._counter = 0\n    self._nit = 0\n</code></pre>"},{"location":"api/utils/OptimizeResult/#viprs.utils.OptimizeResult.IterationConditionCounter.update","title":"<code>update(condition, iteration)</code>","text":"<p>Update the counter based on the condition.</p> <p>Parameters:</p> Name Type Description Default <code>condition</code> <p>The condition to check</p> required <code>iteration</code> <p>The current iteration</p> required Source code in <code>viprs/utils/OptimizeResult.py</code> <pre><code>def update(self, condition, iteration):\n    \"\"\"\n    Update the counter based on the condition.\n    :param condition: The condition to check\n    :param iteration: The current iteration\n    \"\"\"\n    if condition and (iteration == self._nit + 1):\n        self._counter += 1\n    else:\n        self._counter = 0\n\n    self._nit = iteration\n</code></pre>"},{"location":"api/utils/OptimizeResult/#viprs.utils.OptimizeResult.OptimizeResult","title":"<code>OptimizeResult</code>","text":"<p>               Bases: <code>object</code></p> <p>A class to store the results/progress of an optimization algorithm. Similar to the <code>OptimizeResult</code> class from <code>scipy.optimize</code>, but with a few additional fields and parameters.</p> <p>Attributes:</p> Name Type Description <code>message</code> <p>A message about the optimization result</p> <code>stop_iteration</code> <p>A flag to indicate whether the optimization algorithm has stopped iterating</p> <code>success</code> <p>A flag to indicate whether the optimization algorithm has succeeded</p> <code>fun</code> <p>The current objective function value</p> <code>nit</code> <p>The current number of iterations</p> <code>error_on_termination</code> <p>A flag to indicate whether the optimization algorithm stopped due to an error.</p> Source code in <code>viprs/utils/OptimizeResult.py</code> <pre><code>class OptimizeResult(object):\n    \"\"\"\n    A class to store the results/progress of an optimization algorithm.\n    Similar to the `OptimizeResult` class from `scipy.optimize`,\n    but with a few additional fields and parameters.\n\n    :ivar message: A message about the optimization result\n    :ivar stop_iteration: A flag to indicate whether the optimization algorithm has stopped iterating\n    :ivar success: A flag to indicate whether the optimization algorithm has succeeded\n    :ivar fun: The current objective function value\n    :ivar nit: The current number of iterations\n    :ivar error_on_termination: A flag to indicate whether the optimization algorithm stopped due to an error.\n    \"\"\"\n\n    def __init__(self):\n\n        self.message = None\n        self.stop_iteration = None\n        self.success = None\n        self.fun = None\n        self.nit = 0\n        self.error_on_termination = False\n\n        self._last_drop_iter = None\n        self._oscillation_counter = 0\n\n    @property\n    def iterations(self):\n        \"\"\"\n        :return: The current number of iterations.\n        \"\"\"\n        return self.nit\n\n    @property\n    def objective(self):\n        \"\"\"\n        :return: The current value for the objective function.\n        \"\"\"\n        return self.fun\n\n    @property\n    def converged(self):\n        \"\"\"\n        :return: The flag indicating whether the optimization algorithm has converged.\n        \"\"\"\n        return self.success\n\n    @property\n    def valid_optim_result(self):\n        \"\"\"\n        :return: Boolean flag indicating whether the optimization result is valid in\n        the sense tht it either successfully converged OR it stopped iterating without\n        an error (due to e.g. reaching maximum number of iterations).\n        \"\"\"\n        return self.success or (self.stop_iteration and not self.error_on_termination)\n\n    @property\n    def oscillation_counter(self):\n        \"\"\"\n        :return: The number of oscillations in the objective function value.\n        \"\"\"\n        return self._oscillation_counter\n\n    def reset(self):\n        \"\"\"\n        Reset the stored values to their initial state.\n        \"\"\"\n\n        self.message = None\n        self.stop_iteration = False\n        self.success = False\n        self.fun = None\n        self.nit = 0\n        self.error_on_termination = False\n        self._last_drop_iter = None\n        self._oscillation_counter = 0\n\n    def _reset_oscillation_counter(self):\n        \"\"\"\n        Reset the oscillation counter.\n        \"\"\"\n        self._oscillation_counter = 0\n\n    def update(self, fun, stop_iteration=False, success=False, message=None, increment=True):\n        \"\"\"\n        Update the stored values with new values.\n        :param fun: The new objective function value\n        :param stop_iteration: A flag to indicate whether the optimization algorithm has stopped iterating\n        :param success: A flag to indicate whether the optimization algorithm has succeeded\n        :param message: A detailed message about the optimization result.\n        :param increment: A flag to indicate whether to increment the number of iterations.\n        \"\"\"\n\n        # If there's a drop in the objective, start tracking potential oscillations:\n        if self.fun is not None and fun &lt; self.fun:\n            if self._last_drop_iter is not None and self.nit - self._last_drop_iter == 1:\n                self._oscillation_counter += 1\n\n            self._last_drop_iter = self.nit + 1\n        elif self._last_drop_iter is not None and self.nit &gt; self._last_drop_iter:\n            # If there's no drop and the last drop is more than 2 iteration ago,\n            # then reset the oscillation counter\n            self._reset_oscillation_counter()\n\n        self.fun = fun\n        self.stop_iteration = stop_iteration\n        self.success = success\n        self.message = message\n\n        self.nit += int(increment)\n\n        if stop_iteration and not success and \"Maximum iterations\" not in message:\n            self.error_on_termination = True\n\n    def __str__(self):\n        return str(self.__dict__)\n</code></pre>"},{"location":"api/utils/OptimizeResult/#viprs.utils.OptimizeResult.OptimizeResult.converged","title":"<code>converged</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The flag indicating whether the optimization algorithm has converged.</p>"},{"location":"api/utils/OptimizeResult/#viprs.utils.OptimizeResult.OptimizeResult.iterations","title":"<code>iterations</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The current number of iterations.</p>"},{"location":"api/utils/OptimizeResult/#viprs.utils.OptimizeResult.OptimizeResult.objective","title":"<code>objective</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The current value for the objective function.</p>"},{"location":"api/utils/OptimizeResult/#viprs.utils.OptimizeResult.OptimizeResult.oscillation_counter","title":"<code>oscillation_counter</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>The number of oscillations in the objective function value.</p>"},{"location":"api/utils/OptimizeResult/#viprs.utils.OptimizeResult.OptimizeResult.valid_optim_result","title":"<code>valid_optim_result</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <p>Boolean flag indicating whether the optimization result is valid in the sense tht it either successfully converged OR it stopped iterating without an error (due to e.g. reaching maximum number of iterations).</p>"},{"location":"api/utils/OptimizeResult/#viprs.utils.OptimizeResult.OptimizeResult.reset","title":"<code>reset()</code>","text":"<p>Reset the stored values to their initial state.</p> Source code in <code>viprs/utils/OptimizeResult.py</code> <pre><code>def reset(self):\n    \"\"\"\n    Reset the stored values to their initial state.\n    \"\"\"\n\n    self.message = None\n    self.stop_iteration = False\n    self.success = False\n    self.fun = None\n    self.nit = 0\n    self.error_on_termination = False\n    self._last_drop_iter = None\n    self._oscillation_counter = 0\n</code></pre>"},{"location":"api/utils/OptimizeResult/#viprs.utils.OptimizeResult.OptimizeResult.update","title":"<code>update(fun, stop_iteration=False, success=False, message=None, increment=True)</code>","text":"<p>Update the stored values with new values.</p> <p>Parameters:</p> Name Type Description Default <code>fun</code> <p>The new objective function value</p> required <code>stop_iteration</code> <p>A flag to indicate whether the optimization algorithm has stopped iterating</p> <code>False</code> <code>success</code> <p>A flag to indicate whether the optimization algorithm has succeeded</p> <code>False</code> <code>message</code> <p>A detailed message about the optimization result.</p> <code>None</code> <code>increment</code> <p>A flag to indicate whether to increment the number of iterations.</p> <code>True</code> Source code in <code>viprs/utils/OptimizeResult.py</code> <pre><code>def update(self, fun, stop_iteration=False, success=False, message=None, increment=True):\n    \"\"\"\n    Update the stored values with new values.\n    :param fun: The new objective function value\n    :param stop_iteration: A flag to indicate whether the optimization algorithm has stopped iterating\n    :param success: A flag to indicate whether the optimization algorithm has succeeded\n    :param message: A detailed message about the optimization result.\n    :param increment: A flag to indicate whether to increment the number of iterations.\n    \"\"\"\n\n    # If there's a drop in the objective, start tracking potential oscillations:\n    if self.fun is not None and fun &lt; self.fun:\n        if self._last_drop_iter is not None and self.nit - self._last_drop_iter == 1:\n            self._oscillation_counter += 1\n\n        self._last_drop_iter = self.nit + 1\n    elif self._last_drop_iter is not None and self.nit &gt; self._last_drop_iter:\n        # If there's no drop and the last drop is more than 2 iteration ago,\n        # then reset the oscillation counter\n        self._reset_oscillation_counter()\n\n    self.fun = fun\n    self.stop_iteration = stop_iteration\n    self.success = success\n    self.message = message\n\n    self.nit += int(increment)\n\n    if stop_iteration and not success and \"Maximum iterations\" not in message:\n        self.error_on_termination = True\n</code></pre>"},{"location":"api/utils/compute_utils/","title":"Compute utils","text":""},{"location":"api/utils/compute_utils/#viprs.utils.compute_utils.combine_coefficient_tables","title":"<code>combine_coefficient_tables(coef_tables, coef_col='BETA')</code>","text":"<p>Combine a list of coefficient tables (output from a PRS model) into a single table that can be used for downstream tasks, such scoring and evaluation. Note that this implementation assumes that the coefficients tables were generated for the same set of variants, from a grid-search or similar procedure.</p> <p>Parameters:</p> Name Type Description Default <code>coef_tables</code> <p>A list of pandas dataframes containing variant information as well as inferred coefficients.</p> required <code>coef_col</code> <p>The name of the column containing the coefficients.</p> <code>'BETA'</code> <p>Returns:</p> Type Description <p>A single pandas dataframe with the combined coefficients. The new coefficient columns will be labelled as BETA_0, BETA_1, etc.</p> Source code in <code>viprs/utils/compute_utils.py</code> <pre><code>def combine_coefficient_tables(coef_tables, coef_col='BETA'):\n    \"\"\"\n    Combine a list of coefficient tables (output from a PRS model) into a single\n    table that can be used for downstream tasks, such scoring and evaluation. Note that\n    this implementation assumes that the coefficients tables were generated for the same\n    set of variants, from a grid-search or similar procedure.\n\n    :param coef_tables: A list of pandas dataframes containing variant information as well as\n    inferred coefficients.\n    :param coef_col: The name of the column containing the coefficients.\n    :return: A single pandas dataframe with the combined coefficients. The new coefficient columns will be\n    labelled as BETA_0, BETA_1, etc.\n    \"\"\"\n\n    # Sanity checks:\n    assert all([coef_col in t.columns for t in coef_tables]), \"All tables must contain the coefficient column.\"\n    assert all([len(t) == len(coef_tables[0]) for t in coef_tables]), \"All tables must have the same number of rows.\"\n\n    if len(coef_tables) == 1:\n        return coef_tables[0]\n\n    ref_table = coef_tables[0].copy()\n    ref_table.rename(columns={coef_col: f'{coef_col}_0'}, inplace=True)\n\n    # Extract the coefficients from the other tables:\n    return pd.concat([ref_table, *[t[[coef_col]].rename(columns={coef_col: f'{coef_col}_{i}'})\n                                   for i, t in enumerate(coef_tables[1:], 1)]], axis=1)\n</code></pre>"},{"location":"api/utils/compute_utils/#viprs.utils.compute_utils.dict_concat","title":"<code>dict_concat(d, axis=0)</code>","text":"<p>Concatenate the values of a dictionary into a single vector</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <p>A dictionary where values are numeric scalars or vectors</p> required <code>axis</code> <p>Concatenate along given axis.</p> <code>0</code> Source code in <code>viprs/utils/compute_utils.py</code> <pre><code>def dict_concat(d, axis=0):\n    \"\"\"\n    Concatenate the values of a dictionary into a single vector\n    :param d: A dictionary where values are numeric scalars or vectors\n    :param axis: Concatenate along given axis.\n    \"\"\"\n    if len(d) == 1:\n        return d[next(iter(d))]\n    else:\n        return np.concatenate([d[c] for c in sorted(d.keys())], axis=axis)\n</code></pre>"},{"location":"api/utils/compute_utils/#viprs.utils.compute_utils.dict_dot","title":"<code>dict_dot(d1, d2)</code>","text":"<p>Perform dot product on the elements of d1 and d2</p> <p>Parameters:</p> Name Type Description Default <code>d1</code> <p>A dictionary where values are numeric scalars or vectors</p> required <code>d2</code> <p>A dictionary where values are numeric scalars or vectors</p> required Source code in <code>viprs/utils/compute_utils.py</code> <pre><code>def dict_dot(d1, d2):\n    \"\"\"\n    Perform dot product on the elements of d1 and d2\n    :param d1: A dictionary where values are numeric scalars or vectors\n    :param d2: A dictionary where values are numeric scalars or vectors\n    \"\"\"\n    return np.sum([np.dot(d1[c], d2[c]) for c in d1.keys()])\n</code></pre>"},{"location":"api/utils/compute_utils/#viprs.utils.compute_utils.dict_elementwise_dot","title":"<code>dict_elementwise_dot(d1, d2)</code>","text":"<p>Apply element-wise product between the values of two dictionaries</p> <p>Parameters:</p> Name Type Description Default <code>d1</code> <p>A dictionary where values are numeric scalars or vectors</p> required <code>d2</code> <p>A dictionary where values are numeric scalars or vectors</p> required Source code in <code>viprs/utils/compute_utils.py</code> <pre><code>def dict_elementwise_dot(d1, d2):\n    \"\"\"\n    Apply element-wise product between the values of two dictionaries\n\n    :param d1: A dictionary where values are numeric scalars or vectors\n    :param d2: A dictionary where values are numeric scalars or vectors\n    \"\"\"\n    return {c: d1[c]*d2[c] for c, v in d1.items()}\n</code></pre>"},{"location":"api/utils/compute_utils/#viprs.utils.compute_utils.dict_elementwise_transform","title":"<code>dict_elementwise_transform(d, transform)</code>","text":"<p>Apply a transformation to values of a dictionary</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <p>A dictionary where values are numeric scalars or vectors</p> required <code>transform</code> <p>A function to apply to</p> required Source code in <code>viprs/utils/compute_utils.py</code> <pre><code>def dict_elementwise_transform(d, transform):\n    \"\"\"\n    Apply a transformation to values of a dictionary\n    :param d: A dictionary where values are numeric scalars or vectors\n    :param transform: A function to apply to\n    \"\"\"\n    return {c: np.vectorize(transform)(v) for c, v in d.items()}\n</code></pre>"},{"location":"api/utils/compute_utils/#viprs.utils.compute_utils.dict_max","title":"<code>dict_max(d, axis=None)</code>","text":"<p>Estimate the maximum of the values of a dictionary</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <p>A dictionary where values are numeric scalars or vectors</p> required <code>axis</code> <p>Perform aggregation along given axis.</p> <code>None</code> Source code in <code>viprs/utils/compute_utils.py</code> <pre><code>def dict_max(d, axis=None):\n    \"\"\"\n    Estimate the maximum of the values of a dictionary\n    :param d: A dictionary where values are numeric scalars or vectors\n    :param axis: Perform aggregation along given axis.\n    \"\"\"\n    return np.max(np.array([np.max(v, axis=axis) for v in d.values()]), axis=axis)\n</code></pre>"},{"location":"api/utils/compute_utils/#viprs.utils.compute_utils.dict_mean","title":"<code>dict_mean(d, axis=None)</code>","text":"<p>Estimate the mean of the values of a dictionary</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <p>A dictionary where values are numeric scalars or vectors</p> required <code>axis</code> <p>Perform aggregation along given axis.</p> <code>None</code> Source code in <code>viprs/utils/compute_utils.py</code> <pre><code>def dict_mean(d, axis=None):\n    \"\"\"\n    Estimate the mean of the values of a dictionary\n    :param d: A dictionary where values are numeric scalars or vectors\n    :param axis: Perform aggregation along given axis.\n    \"\"\"\n    return np.mean(np.array([np.mean(v, axis=axis) for v in d.values()]), axis=axis)\n</code></pre>"},{"location":"api/utils/compute_utils/#viprs.utils.compute_utils.dict_repeat","title":"<code>dict_repeat(value, shapes)</code>","text":"<p>Given a value, create a dictionary where the value is repeated according to the shapes parameter</p> <p>Parameters:</p> Name Type Description Default <code>shapes</code> <p>A dictionary of shapes. Key is arbitrary, value is integer input to np.repeat</p> required <code>value</code> <p>The value to repeat</p> required Source code in <code>viprs/utils/compute_utils.py</code> <pre><code>def dict_repeat(value, shapes):\n    \"\"\"\n    Given a value, create a dictionary where the value is repeated\n    according to the shapes parameter\n    :param shapes: A dictionary of shapes. Key is arbitrary, value is integer input to np.repeat\n    :param value:  The value to repeat\n    \"\"\"\n    return {c: value*np.ones(shp) for c, shp in shapes.items()}\n</code></pre>"},{"location":"api/utils/compute_utils/#viprs.utils.compute_utils.dict_set","title":"<code>dict_set(d, value)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>d</code> <p>A dictionary where values are numeric vectors</p> required <code>value</code> <p>A value to set for all vectors</p> required Source code in <code>viprs/utils/compute_utils.py</code> <pre><code>def dict_set(d, value):\n    \"\"\"\n    :param d: A dictionary where values are numeric vectors\n    :param value: A value to set for all vectors\n    \"\"\"\n    for c in d:\n        d[c][:] = value\n\n    return d\n</code></pre>"},{"location":"api/utils/compute_utils/#viprs.utils.compute_utils.dict_sum","title":"<code>dict_sum(d, axis=None, transform=None)</code>","text":"<p>Estimate the sum of the values of a dictionary</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <p>A dictionary where values are numeric scalars or vectors</p> required <code>axis</code> <p>Perform aggregation along given axis.</p> <code>None</code> <code>transform</code> <p>Transformation to apply before summing.</p> <code>None</code> Source code in <code>viprs/utils/compute_utils.py</code> <pre><code>def dict_sum(d, axis=None, transform=None):\n    \"\"\"\n    Estimate the sum of the values of a dictionary\n    :param d: A dictionary where values are numeric scalars or vectors\n    :param axis: Perform aggregation along given axis.\n    :param transform: Transformation to apply before summing.\n    \"\"\"\n    if transform is None:\n        return np.sum(np.array([np.sum(v, axis=axis) for v in d.values()]), axis=axis)\n    else:\n        return np.sum(np.array([np.sum(transform(v), axis=axis) for v in d.values()]), axis=axis)\n</code></pre>"},{"location":"api/utils/compute_utils/#viprs.utils.compute_utils.expand_column_names","title":"<code>expand_column_names(c_name, shape, sep='_')</code>","text":"<p>Given a desired column name <code>c_name</code> and a matrix <code>shape</code> that we'd like to apply the column name to, return a list of column names for every column in the matrix. The column names will be in the form of <code>c_name</code> followed by an index, separated by <code>sep</code>.</p> <p>For example, if the column name is <code>BETA</code>, the shape is (100, 3) and the separator is <code>_</code>, we return a list with: [<code>BETA_0</code>, <code>BETA_1</code>, <code>BETA_2</code>]</p> <p>If the matrix in question is a vector, we just return the column name without any indices appended to it.</p> <p>Parameters:</p> Name Type Description Default <code>c_name</code> <p>A string object</p> required <code>shape</code> <p>The shape of a numpy matrix or vector</p> required <code>sep</code> <p>The separator</p> <code>'_'</code> <p>Returns:</p> Type Description <p>A list of column names</p> Source code in <code>viprs/utils/compute_utils.py</code> <pre><code>def expand_column_names(c_name, shape, sep='_'):\n    \"\"\"\n    Given a desired column name `c_name` and a matrix `shape`\n    that we'd like to apply the column name to, return a list of\n    column names for every column in the matrix. The column names will be\n    in the form of `c_name` followed by an index, separated by `sep`.\n\n    For example, if the column name is `BETA`, the\n    shape is (100, 3) and the separator is `_`, we return a list with:\n    [`BETA_0`, `BETA_1`, `BETA_2`]\n\n    If the matrix in question is a vector, we just return the column name\n    without any indices appended to it.\n\n    :param c_name: A string object\n    :param shape: The shape of a numpy matrix or vector\n    :param sep: The separator\n\n    :return: A list of column names\n    \"\"\"\n\n    if len(shape) &lt; 2:\n        return [c_name]\n    elif shape[1] == 1:\n        return [c_name]\n    else:\n        return [f'{c_name}{sep}{i}' for i in range(shape[1])]\n</code></pre>"},{"location":"api/utils/compute_utils/#viprs.utils.compute_utils.fits_in_memory","title":"<code>fits_in_memory(alloc_size, max_prop=0.9)</code>","text":"<p>Check whether there's enough memory resources to load an object with the given allocation size (in MB).</p> <p>Parameters:</p> Name Type Description Default <code>alloc_size</code> <p>The allocation size</p> required <code>max_prop</code> <p>The maximum proportion of available memory allowed for the object</p> <code>0.9</code> Source code in <code>viprs/utils/compute_utils.py</code> <pre><code>def fits_in_memory(alloc_size, max_prop=.9):\n    \"\"\"\n    Check whether there's enough memory resources to load an object\n    with the given allocation size (in MB).\n    :param alloc_size: The allocation size\n    :param max_prop: The maximum proportion of available memory allowed for the object\n    \"\"\"\n\n    avail_mem = psutil.virtual_memory().available / (1024.0 ** 2)\n\n    if alloc_size / avail_mem &gt; max_prop:\n        return False\n    else:\n        return True\n</code></pre>"},{"location":"api/utils/data_utils/","title":"Data utils","text":""},{"location":"api/utils/data_utils/#viprs.utils.data_utils.download_ld_matrix","title":"<code>download_ld_matrix(target_dir='.', chromosome=None)</code>","text":"<p>Download LD matrices for VIPRS software.</p> <p>TODO: Update this once data is made available.</p> <p>Parameters:</p> Name Type Description Default <code>target_dir</code> <p>The path or directory where to store the LD matrix</p> <code>'.'</code> <code>chromosome</code> <p>An integer or list of integers with the chromosome numbers for which to download the LD matrices from Zenodo.</p> <code>None</code> Source code in <code>viprs/utils/data_utils.py</code> <pre><code>def download_ld_matrix(target_dir='.', chromosome=None):\n    \"\"\"\n    Download LD matrices for VIPRS software.\n\n    TODO: Update this once data is made available.\n\n    :param target_dir: The path or directory where to store the LD matrix\n    :param chromosome: An integer or list of integers with the chromosome numbers for which to download\n    the LD matrices from Zenodo.\n    \"\"\"\n\n    raise NotImplementedError(\"This function is not yet implemented.\")\n</code></pre>"},{"location":"api/utils/exceptions/","title":"Exceptions","text":""},{"location":"api/utils/exceptions/#viprs.utils.exceptions.OptimizationDivergence","title":"<code>OptimizationDivergence</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when the optimization algorithm diverges.</p> Source code in <code>viprs/utils/exceptions.py</code> <pre><code>class OptimizationDivergence(Exception):\n    \"\"\"\n    Exception raised when the optimization algorithm diverges.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"commandline/overview/","title":"Overview","text":"<p>In addition to the python package interface, users may also opt to use <code>viprs</code> via commandline scripts. The commandline interface is designed to be user-friendly and to provide a variety of options for the user to  customize the inference process. </p> <p>When you install <code>viprs</code> using <code>pip</code>, the commandline scripts are automatically installed on your system and  are available for use. The following scripts are meant to facilitate the entire pipeline of polygenic score inference,  from fitting and estimating the posterior distribution of the variant effect sizes to predicting the PRS for a set of  test individuals and evaluating the performance of the PRS predictions on held out samples.</p> <ul> <li> <p><code>viprs_fit</code>: This script is used to fit the variational PRS model to the GWAS summary statistics and to estimate the      posterior distribution of the variant effect sizes. The script provides a variety of options for the user to      customize the inference process, including the choice of prior distributions and the choice of      optimization algorithms.</p> </li> <li> <p><code>viprs_score</code>: This script is used to predict the PRS for a set of individuals using the      estimated variant effect sizes from the <code>viprs_fit</code> script. This is the script that generates the PRS per     individual.</p> </li> <li> <p><code>viprs_evaluate</code>: This script is used to evaluate the performance of the PRS predictions      using the PRS computed in the previous step. The script provides a variety of      options for the user to customize the evaluation process, including the choice of performance metrics and      the choice of evaluation datasets.</p> </li> </ul>"},{"location":"commandline/overview/#todo","title":"TODO","text":"<ul> <li>Create a <code>nextflow</code> pipeline that runs all of the above steps in a single command.</li> </ul>"},{"location":"commandline/viprs_evaluate/","title":"viprs_evaluate","text":""},{"location":"commandline/viprs_evaluate/#evaluate-predictive-performance-of-prs-viprs_evaluate","title":"Evaluate Predictive Performance of PRS (<code>viprs_evaluate</code>)","text":"<p>The <code>viprs_evaluate</code> script is used to evaluate the performance of the PRS predictions using the PRS computed in  the previous step. The script provides a variety of options for the user to customize the evaluation process,  including the choice of performance metrics and the choice of evaluation datasets.</p> <p>A full listing of the options available for the <code>viprs_evaluate</code> script can be found by running the  following command in your terminal:</p> <pre><code>viprs_evaluate -h\n</code></pre> <p>Which outputs the following help message:</p> <pre><code>\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001**********************************************\n\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001   \u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001_____\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001    \n\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001   ___\u2001\u2001\u2001_____(_)________\u2001________________    \n\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001   __\u2001|\u2001/\u2001/__\u2001\u2001/\u2001___\u2001\u2001__\u2001\\__\u2001\u2001___/__\u2001\u2001___/    \n\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001   __\u2001|/\u2001/\u2001_\u2001\u2001/\u2001\u2001__\u2001\u2001/_/\u2001/_\u2001\u2001/\u2001\u2001\u2001\u2001_(__\u2001\u2001)\u2001    \n\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001   _____/\u2001\u2001/_/\u2001\u2001\u2001_\u2001\u2001.___/\u2001/_/\u2001\u2001\u2001\u2001\u2001/____/\u2001\u2001    \n\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001   \u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001/_/\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001    \n\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001                     \u2001\u2001\u2001\u2001                     \n\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001Variational Inference of Polygenic Risk Scores\n\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001  Version: 0.1.3 | Release date: April 2025   \n\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001    Author: Shadi Zabad, McGill University    \n\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001**********************************************\n\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001&lt; Evaluate Prediction Accuracy of PRS Models &gt;\n\nusage: viprs_evaluate [-h] --prs-file PRS_FILE --phenotype-file PHENO_FILE [--phenotype-col PHENO_COL]\n                      [--phenotype-likelihood {binomial,gaussian,infer}] [--keep KEEP] --output-file OUTPUT_FILE\n                      [--metrics METRICS [METRICS ...]] [--covariates-file COVARIATES_FILE]\n                      [--log-level {CRITICAL,WARNING,INFO,DEBUG,ERROR}]\n\nCommandline arguments for evaluating polygenic scores\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --prs-file PRS_FILE   The path to the PRS file (expected format: FID IID PRS, tab-separated)\n  --phenotype-file PHENO_FILE\n                        The path to the phenotype file. The expected format is: FID IID phenotype (no header), tab-separated.\n  --phenotype-col PHENO_COL\n                        The column index for the phenotype in the phenotype file (0-based index).\n  --phenotype-likelihood {binomial,gaussian,infer}\n                        The phenotype likelihood (\"gaussian\" for continuous, \"binomial\" for case-control). If not set, will be inferred\n                        automatically based on the phenotype file.\n  --keep KEEP           A plink-style keep file to select a subset of individuals for the evaluation.\n  --output-file OUTPUT_FILE\n                        The output file where to store the evaluation metrics (with no extension).\n  --metrics METRICS [METRICS ...]\n                        The evaluation metrics to compute (default: all available metrics that are relevant for the phenotype). For a full\n                        list of supported metrics, check the documentation.\n  --covariates-file COVARIATES_FILE\n                        A file with covariates for the samples included in the analysis. This tab-separated file should not have a header\n                        and the first two columns should be the FID and IID of the samples.\n  --log-level {CRITICAL,WARNING,INFO,DEBUG,ERROR}\n                        The logging level for the console output.\n</code></pre>"},{"location":"commandline/viprs_fit/","title":"viprs_fit","text":""},{"location":"commandline/viprs_fit/#fit-viprs-model-to-gwas-summary-statistics-viprs_fit","title":"Fit VIPRS model to GWAS summary statistics (<code>viprs_fit</code>)","text":"<p>The <code>viprs_fit</code> script is used to fit the variational PRS model to the GWAS summary statistics and to estimate the  posterior distribution of the variant effect sizes. The script provides a variety of options for the user to  customize the inference process, including the choice of prior distributions and the choice of  optimization algorithms.</p> <p>A full listing of the options available for the <code>viprs_fit</code> script can be found by running the following command in your terminal:</p> <pre><code>viprs_fit -h\n</code></pre> <p>Which outputs the following help message:</p> <pre><code>\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001**********************************************\n\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001   \u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001_____\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001    \n\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001   ___\u2001\u2001\u2001_____(_)________\u2001________________    \n\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001   __\u2001|\u2001/\u2001/__\u2001\u2001/\u2001___\u2001\u2001__\u2001\\__\u2001\u2001___/__\u2001\u2001___/    \n\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001   __\u2001|/\u2001/\u2001_\u2001\u2001/\u2001\u2001__\u2001\u2001/_/\u2001/_\u2001\u2001/\u2001\u2001\u2001\u2001_(__\u2001\u2001)\u2001    \n\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001   _____/\u2001\u2001/_/\u2001\u2001\u2001_\u2001\u2001.___/\u2001/_/\u2001\u2001\u2001\u2001\u2001/____/\u2001\u2001    \n\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001   \u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001/_/\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001    \n\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001                     \u2001\u2001\u2001\u2001                     \n\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001Variational Inference of Polygenic Risk Scores\n\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001  Version: 0.1.3 | Release date: April 2025   \n\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001    Author: Shadi Zabad, McGill University    \n\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001**********************************************\n\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001   &lt; Fit VIPRS to GWAS summary statistics &gt;   \n\nusage: viprs_fit [-h] -l LD_DIR -s SUMSTATS_PATH --output-dir OUTPUT_DIR [--output-file-prefix OUTPUT_PREFIX] [--temp-dir TEMP_DIR]\n                 [--sumstats-format {gwas-ssf,saige,plink,custom,fastgwa,plink2,cojo,ssf,magenpy,gwascatalog,plink1.9}]\n                 [--custom-sumstats-mapper CUSTOM_SUMSTATS_MAPPER] [--custom-sumstats-sep CUSTOM_SUMSTATS_SEP] [--gwas-sample-size GWAS_SAMPLE_SIZE]\n                 [--validation-bfile VALIDATION_BED] [--validation-pheno VALIDATION_PHENO] [--validation-keep VALIDATION_KEEP]\n                 [--validation-ld-panel VALIDATION_LD_PANEL] [--validation-sumstats VALIDATION_SUMSTATS_PATH]\n                 [--validation-sumstats-format {gwas-ssf,saige,plink,custom,fastgwa,plink2,cojo,ssf,magenpy,gwascatalog,plink1.9}] [-m {VIPRS,VIPRSMix}]\n                 [--float-precision {float32,float64}] [--use-symmetric-ld] [--dequantize-on-the-fly] [--fix-sigma-epsilon FIX_SIGMA_EPSILON]\n                 [--lambda-min LAMBDA_MIN] [--n-components N_COMPONENTS] [--max-iter MAX_ITER] [--h2-est H2_EST] [--h2-se H2_SE] [--hyp-search {GS,BMA,EM}]\n                 [--grid-metric {ELBO,validation,pseudo_validation}] [--grid-search-mode {pathwise,independent}] [--prop-train PROP_TRAIN] [--pi-grid PI_GRID]\n                 [--pi-steps PI_STEPS] [--sigma-epsilon-grid SIGMA_EPSILON_GRID] [--sigma-epsilon-steps SIGMA_EPSILON_STEPS]\n                 [--lambda-min-steps LAMBDA_MIN_STEPS] [--genomewide] [--exclude-lrld] [--backend {plink,xarray}] [--n-jobs N_JOBS] [--threads THREADS]\n                 [--output-profiler-metrics] [--log-level {DEBUG,WARNING,INFO,ERROR,CRITICAL}] [--seed SEED]\n\nCommandline arguments for fitting VIPRS to GWAS summary statistics\n\noptions:\n  -h, --help            show this help message and exit\n  -l LD_DIR, --ld-panel LD_DIR\n                        The path to the directory where the LD matrices are stored. Can be a wildcard of the form ld/chr_*\n  -s SUMSTATS_PATH, --sumstats SUMSTATS_PATH\n                        The summary statistics directory or file. Can be a wildcard of the form sumstats/chr_*\n  --output-dir OUTPUT_DIR\n                        The output directory where to store the inference results.\n  --output-file-prefix OUTPUT_PREFIX\n                        A prefix to append to the names of the output files (optional).\n  --temp-dir TEMP_DIR   The temporary directory where to store intermediate files.\n  --sumstats-format {gwas-ssf,saige,plink,custom,fastgwa,plink2,cojo,ssf,magenpy,gwascatalog,plink1.9}\n                        The format for the summary statistics file(s).\n  --custom-sumstats-mapper CUSTOM_SUMSTATS_MAPPER\n                        A comma-separated string with column name mappings between the custom summary statistics format and the standard format expected by\n                        magenpy/VIPRS. Provide only mappings for column names that are different, in the form of:--custom-sumstats-mapper\n                        rsid=SNP,eff_allele=A1,beta=BETA\n  --custom-sumstats-sep CUSTOM_SUMSTATS_SEP\n                        The delimiter for the summary statistics file with custom format.\n  --gwas-sample-size GWAS_SAMPLE_SIZE\n                        The overall sample size for the GWAS study. This must be provided if the sample size per-SNP is not in the summary statistics file.\n  --validation-bfile VALIDATION_BED\n                        The BED files containing the genotype data for the validation set. You may use a wildcard here (e.g. \"data/chr_*.bed\")\n  --validation-pheno VALIDATION_PHENO\n                        A tab-separated file containing the phenotype for the validation set. The expected format is: FID IID phenotype (no header)\n  --validation-keep VALIDATION_KEEP\n                        A plink-style keep file to select a subset of individuals for the validation set.\n  --validation-ld-panel VALIDATION_LD_PANEL\n                        The path to the directory where the LD matrices for the validation set are stored. Can be a wildcard of the form ld/chr_*\n  --validation-sumstats VALIDATION_SUMSTATS_PATH\n                        The summary statistics directory or file for the validation set. Can be a wildcard of the form sumstats/chr_*\n  --validation-sumstats-format {gwas-ssf,saige,plink,custom,fastgwa,plink2,cojo,ssf,magenpy,gwascatalog,plink1.9}\n                        The format for the summary statistics file(s) for the validation set.\n  -m {VIPRS,VIPRSMix}, --model {VIPRS,VIPRSMix}\n                        The type of PRS model to fit to the GWAS data\n  --float-precision {float32,float64}\n                        The float precision to use when fitting the model.\n  --use-symmetric-ld    Use the symmetric form of the LD matrix when fitting the model.\n  --dequantize-on-the-fly\n                        Dequantize the entries of the LD matrix on-the-fly during inference.\n  --fix-sigma-epsilon FIX_SIGMA_EPSILON\n                        Set the value of the residual variance hyperparameter, sigma_epsilon, to the provided value.\n  --lambda-min LAMBDA_MIN\n                        Set the value of the lambda_min parameter, which acts as a regularizer for the effect sizes and compensates for noise in the LD matrix.\n                        Set to \"infer\" to derive this parameter from the properties of the LD matrix itself.\n  --n-components N_COMPONENTS\n                        The number of non-null Gaussian mixture components to use with the VIPRSMix model (i.e. excluding the spike component).\n  --max-iter MAX_ITER   The maximum number of iterations to run the coordinate ascent algorithm.\n  --h2-est H2_EST       The estimated heritability of the trait. If available, this value can be used for parameter initialization or hyperparameter grid\n                        search.\n  --h2-se H2_SE         The standard error for the heritability estimate for the trait. If available, this value can be used for parameter initialization or\n                        hyperparameter grid search.\n  --hyp-search {GS,BMA,EM}\n                        The strategy for tuning the hyperparameters of the model. Options are EM (Expectation-Maximization), GS (Grid search), and BMA (Bayesian\n                        Model Averaging).\n  --grid-metric {ELBO,validation,pseudo_validation}\n                        The metric for selecting best performing model in grid search.\n  --grid-search-mode {pathwise,independent}\n                        The mode for grid search. Pathwise mode updates the hyperparameters sequentially and in a warm-start fashion, while independent mode\n                        updates each model separately starting from same initialization.\n  --prop-train PROP_TRAIN\n                        The proportion of the samples to use for training when performing cross validation using the PUMAS procedure.\n  --pi-grid PI_GRID     A comma-separated grid values for the hyperparameter pi (see also --pi-steps).\n  --pi-steps PI_STEPS   The number of steps for the (default) pi grid. This will create an equidistant grid between 10/M and 0.2 on a log10 scale, where M is\n                        the number of variants.\n  --sigma-epsilon-grid SIGMA_EPSILON_GRID\n                        A comma-separated grid values for the hyperparameter sigma_epsilon (see also --sigma-epsilon-steps).\n  --sigma-epsilon-steps SIGMA_EPSILON_STEPS\n                        The number of steps (unique values) for the sigma_epsilon grid.\n  --lambda-min-steps LAMBDA_MIN_STEPS\n                        The number of grid steps for the lambda_min grid. Lambda_min is used to compensate for noise in the LD matrix and acts as an extra\n                        regularizer for the effect sizes.\n  --genomewide          Fit all chromosomes jointly\n  --exclude-lrld        Exclude Long Range LD (LRLD) regions during inference. These regions can cause numerical instabilities in some cases.\n  --backend {plink,xarray}\n                        The backend software used for computations on the genotype matrix.\n  --n-jobs N_JOBS       The number of processes to launch for the hyperparameter search (default is 1, but we recommend increasing this depending on system\n                        capacity).\n  --threads THREADS     The number of threads to use in the E-Step of VIPRS.\n  --output-profiler-metrics\n                        Output the profiler metrics that measure runtime, memory usage, etc.\n  --log-level {DEBUG,WARNING,INFO,ERROR,CRITICAL}\n                        The logging level for the console output.\n  --seed SEED           The random seed to use for the random number generator.\n</code></pre>"},{"location":"commandline/viprs_score/","title":"viprs_score","text":""},{"location":"commandline/viprs_score/#compute-polygenic-scores-using-inferred-variant-effect-sizes-viprs_score","title":"Compute Polygenic Scores using inferred variant effect sizes (<code>viprs_score</code>)","text":"<p>The <code>viprs_score</code> script is used to compute the polygenic risk scores (PRS) for a set of individuals  using the estimated variant effect sizes from the <code>viprs_fit</code> script. This is the script that generates  the PRS per individual.</p> <p>A full listing of the options available for the <code>viprs_score</code> script can be found by running the  following command in your terminal:</p> <pre><code>viprs_score -h\n</code></pre> <p>Which outputs the following help message:</p> <pre><code>\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001**********************************************\n\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001   \u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001_____\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001    \n\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001   ___\u2001\u2001\u2001_____(_)________\u2001________________    \n\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001   __\u2001|\u2001/\u2001/__\u2001\u2001/\u2001___\u2001\u2001__\u2001\\__\u2001\u2001___/__\u2001\u2001___/    \n\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001   __\u2001|/\u2001/\u2001_\u2001\u2001/\u2001\u2001__\u2001\u2001/_/\u2001/_\u2001\u2001/\u2001\u2001\u2001\u2001_(__\u2001\u2001)\u2001    \n\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001   _____/\u2001\u2001/_/\u2001\u2001\u2001_\u2001\u2001.___/\u2001/_/\u2001\u2001\u2001\u2001\u2001/____/\u2001\u2001    \n\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001   \u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001/_/\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001    \n\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001                     \u2001\u2001\u2001\u2001                     \n\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001Variational Inference of Polygenic Risk Scores\n\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001  Version: 0.1.3 | Release date: April 2025   \n\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001    Author: Shadi Zabad, McGill University    \n\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001**********************************************\n\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001\u2001&lt; Compute Polygenic Scores for Test Samples &gt; \n\nusage: viprs_score [-h] -f FIT_FILES --bfile BED_FILES --output-file OUTPUT_FILE [--temp-dir TEMP_DIR] [--keep KEEP] [--extract EXTRACT]\n                   [--backend {xarray,plink}] [--threads THREADS] [--compress] [--log-level {WARNING,CRITICAL,DEBUG,INFO,ERROR}]\n\nCommandline arguments for computing polygenic scores\n\noptions:\n  -h, --help            show this help message and exit\n  -f FIT_FILES, --fit-files FIT_FILES\n                        The path to the file(s) with the output parameter estimates from VIPRS. You may use a wildcard here if fit files are stored per-\n                        chromosome (e.g. \"prs/chr_*.fit\")\n  --bfile BED_FILES     The BED files containing the genotype data. You may use a wildcard here (e.g. \"data/chr_*.bed\")\n  --output-file OUTPUT_FILE\n                        The output file where to store the polygenic scores (with no extension).\n  --temp-dir TEMP_DIR   The temporary directory where to store intermediate files.\n  --keep KEEP           A plink-style keep file to select a subset of individuals for the test set.\n  --extract EXTRACT     A plink-style extract file to select a subset of SNPs for scoring.\n  --backend {xarray,plink}\n                        The backend software used for computations with the genotype matrix.\n  --threads THREADS     The number of threads to use for computations.\n  --compress            Compress the output file\n  --log-level {WARNING,CRITICAL,DEBUG,INFO,ERROR}\n                        The logging level for the console output.\n</code></pre>"}]}